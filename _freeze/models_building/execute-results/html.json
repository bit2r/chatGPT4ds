{
  "hash": "697ebcf1f509faa20e46495e40453c09",
  "result": {
    "markdown": "---\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# 모형 개발\n\n기계학습은 다음 세가지 사항을 기반으로 하고 있다.\n패턴은 존재한다고 가정하는 부분에서 통계학의 회귀분석과 유사하나,\n수학적으로 명세를 할 수 없다는 점에서 차이가 난다. 기계학습이나 회귀모형이나 둘다 데이터를 기반으로 한다.\n\n1. 패턴이 존재한다.\n1. 수학적으로 명시적으로 명세할 수 없다.\n1. 데이터를 갖고 있다.\n\n기계학습을 구성하는 이론은 편향-분산(bias-variance), 복잡성, [Vapnik–Chervonenkis 이론](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory), 베이즈통계가 이론이 되고,\n선형회귀모형을 비롯한 다양한 모형이 존재하고, 모형의 성능과 신뢰성을 높이고자 데이터 전처리, \n교차타당성(cross validation), 정규화(regularization)등이 동원된다. [@abu2012learning]\n\n\n![기계학습 지도](images/ml-map.png)\n\n## 기계학습/회귀모형 구성요소 {#ml-basic-elements}\n\n일반 모형을 \"신호 + 잡음(signal + noise)\"로 가정하고 다음과 같은 수식으로 표현할 수 있다.\n\n$$y = f(x) + \\epsilon$$\n\n1. 출력 : $y$, 관심갖고 있는 결과변수\n1. 입력 : $x$, 설명/예측 변수\n    - $y$의 변동성을 설명하는 목적의 모형을 구축하는 경우 $x$는 설명변수\n    - $y$의 변동성을 예측하는 목적의 모형을 구축하는 경우 $x$는 예측변수\n1. 가설: : $g: x \\rightarrow y$, $x$는 $y$에 영향을 주는 인과관계가 존재한다.\n1. 목적함수 : $f: x \\rightarrow y$, $y$와 $x$를 연관시켜주는 함수\n1. 데이터: $(x_1 , y_1 ), (x_2 , y_2 ), \\dots, (x_n , y_n )$\n1. 오차: $\\epsilon$, $f: x \\rightarrow y$으로 설명되지 않는 부분\n\n결국, 잡음이 낀 데이터에서 잡음을 제거하고 신호만 뽑아내는 것이 회귀모형, 기계학습 모형이라고 볼 수 있다.\n회귀모형과 기계학습 모형은 회귀모형이 특정 함수형태를 가정하고 데이터에서 신호와 잡음을 구부하는데 초점이 과거 맞춰졌다면,\n기계학습모형은 $x$는 $y$의 인과관계를 가정으로 놓고 신호와 잡음을 가장 잘 발라낼 수 있는 함수를 찾아내는데 초점을 두고 있다.\n\n![기계학습 도해](images/ml-process-math.png)\n\n## 3대 기계학습 원리 \n\n\n기계학습 알고리즘 개발자가 데이터를 학습시켜 기계학습 알고리즘을 뽑아내는 과정에 3대 기계학습 원리가 적용된다. [^ml-caltech]\n\n[^ml-caltech]: [Caltech MOOC, Yaser Abu-Mostafa, Introductory Machine Learning, 2012](https://work.caltech.edu/telecourse.html)\n\n\n* [오컴의 면도날(Occam's Razor)](https://ko.wikipedia.org/wiki/오컴의_면도날): 사고 절약의 원리(Principle of Parsimony)라고도 불리며, 같은 현상을 설명하는 두가지 모형이 있다면, 단순한 모형을 선택한다.\n* 표집 편향(Sampling Bias): 모집단을 대표성의 원리에 따라 표본을 추출하지 못할 때, 기계학습 알고리즘도 편향된 표본을 학습하여 결과를 왜곡시킨다.\n* 데이터 염탐 편향(Data Snooping Bias): 데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 된다.\n\n![3대 기계학습 원리](images/ml-three-principle.png)\n\n- 오컴의 면도날\n\n동일한 조건이면 더 단순한 것을 선택하는 것으로, 가장 큰 이유는 갖고 있는 데이터를 벗어나 새로운 데이터를 갖게 될 경우 학습시킨 기계학습 알고리즘이 더 좋은 성능을 보인다는 것이다. 결국 수많은 가능한 모형중에서 하나를 선택하는 기준이 된다.\n\n> An explanation of the data should be made as simple as possible, but no simpler -- Albert Einstein\n\n- 표집 편향 \n\n[1948년 미국 대통령선거](https://ko.wikipedia.org/wiki/1948%EB%85%84_%EB%AF%B8%EA%B5%AD_%EB%8C%80%ED%86%B5%EB%A0%B9_%EC%84%A0%EA%B1%B0)에서 트루먼이 듀이 후보를 물리치고 대통령이 된 것은 알려진 사실이다. 하지만, 대부분의 여론조사에서 듀이의 승리를 예상했지만, 사실은 그 반대로 나타났다. 그 당시 여론조사를 전화기를 사용하였는데, 문제는 전화기가 부유층이 많이 소유하고 있어 미국 대통령선거 모집단을 대표하는 대표성에 문제가 있어 왜곡된 결과가 도출된 것이다.\n\n상업적으로 개인금융의 신용카드발급, 신용평가에도 동일한 문제가 발생한다. 사실 수익성은 저신용자가 높아 이를 살펴보면, 신용평가에 사용될 데이터는 저신용자는 카드를 발급받을 수 없어 데이터베이스에는 표집편향된 고객정보만 존재하는 것을 어렵지 않게 볼 수 있다.\n\n> If the data is sampled in a biased way, learning will produce a similarly biased outome.\n\n- 데이터 염탐 편향\n\n데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 하지만, 현실적으로 현업에서 작업하는 사람들이 흔히 범하는 실수다. 동일한 데이터에 대해 갖가지 기계학습 알고리즘을 적용해서 가장 좋은 성능이 나오는 알고리즘을 선정한다. 문제는 데이터가 바뀌면 어떨까? 아마 기대했던 성능이 나오지 못할 가능성이 크다.\n\n> If you torture the data long enough, it will confess\n\n\n\n\n## 모형 개발과정\n\n통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다. \n다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고, 지난한 과정이 될 수도 있다.\n\n1. 데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 팩키지와 소통이 될 수 있는 데이터형태)가 되도록 준비한다.\n1. 변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도 적용한다.\n1. 변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를 계산한다. 특히 변수간 상관관계가 $r > 0.9$ 혹은 근처인 경우 변수를 빼거나 다른 방법을 강구한다.\n1. 동일한 척도로 회귀계수를 추정하고 평가하려는 경우, `scale()` 함수로 척도로 표준화한다.\n1. 모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다. 만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로 개발한다.\n    1. `plot()` 함수를 사용해서 이상점이 있는지, 비선형관계를 잘 잡아냈는지 시각적으로 확인한다.\n    1. 다양한 모형을 적합시키고 `R^2` 와 `RMSE`, 정확도 등 모형평가 결과가 가장 좋은 것을 선정한다.\n    1. 절약성의 원리(principle of parsimony)를 필히 준수하여 가장 간결한 모형이 되도록 노력한다.[^parsimony]\n1. 최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를 넣어 마무리한다.    \n\n[^parsimony]: 간결성 원칙으로도 번역되며, 오캄의 면도날(\"Occam's razor\")라는 이름으로도 알려져 있으며, 연구나 문제 해결의 맥락에서 가장 단순한 설명이나 가설을 우선적으로 고려해야 한다는 의미로, 두 개 이상의 설명이 관찰된 현상을 동등하게 설명할 수 있을 때, 더 적은 가정이 필요한 또는 더 단순한 설명을 선호해야 한다는 것이다.\n\n\n:::{.callout-note}\n### 키보드로 통계모형 표현법\n \n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n \n1. 주효과에 대해 변수를 입력으로 넣을 `+`를 사용한다.\n1. 교호작용을 변수간에 표현할 때 `:`을 사용한다. 예를 들어 `x*y`는 `x+y+x:z`와 같다.\n1. 모든 변수를 표기할 때 `.`을 사용한다. \n1. 종속변수와 예측변수를 구분할 때 `~`을 사용한다. `y ~ .`은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n1. 특정변수를 제거할 때는 `-`를 사용한다. `y ~ . -x`는 모든 예측변수를 사용하고, 특정 변수 `x`를 제거한다는 의미다.\n1. 상수항을 제거할 때는 `-1`을 사용한다.\n \n \n| R 공식구문 | 수학적 표현 | 설명 |\n|------------|---------------|-----------------------------|\n|`y~x`       | $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ | `x`를 `y`에 적합시키는 1차 선형회귀식 |\n|`y~x -1`       | $y_i = \\beta_1 x_i + \\epsilon_i$ | `x`를 `y`에 적합시 절편 없는 1차 선형회귀식 |\n|`y~x+z`       | $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i$ | `x`와 `z`를 `y`에 적합시키는 1차 선형회귀식 |\n|`y~x:z`       | $y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i$ | `x`와 `z` 교호작용 항을 `y`에 적합시키는 1차 선형회귀식 |\n|`y~x*z`       | $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i$ | `x`와 `z`, 교호작용항을 `y`에 적합시키는 1차 선형회귀식 |\n\n:::\n\n## 과대적합 사례\n\n$y=x^2 + \\epsilon$ 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, \n이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n1. `tidyr`, `modelr`, `ggplot2` 팩키지를 불러와서 환경을 설정한다.\n1. $y=x^2 + \\epsilon$, 오차는 $N(0, 0.25)$을 따르는 모형을 생성하고, `df` 데이터프레임에 결과를 저장한다.\n1. `poly_fit_model` 함수를 통해 7차 다항식으로 적합시킨다.\n1. 적합결과를 `ggplot`을 통해 시각화한다.\n\n1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(modelr)\n\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model <- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx <- seq(-1, 1, length=20)\ny <- true_model(x)\ndf <- tibble(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model <- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\n# fitted_mod <- poly_fit_model(df, 10)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\nfitted_tbl <- tibble(차수 = 1:10) |> \n  mutate(데이터 = list(df)) |> \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |> \n  mutate(예측  = map(적합된모형, fitted)) |> \n  mutate(x = list(seq(-1, 1, length=20))) |>\n  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, \n                                         prediction = .y, \n                                         .name_repair = \"unique\"))) |> \n  select(차수, fitted_df) |> \n  mutate(차수 = as.factor(차수)) |>   \n  unnest(fitted_df) \n\n\ndf %>% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))\n```\n\n::: {.cell-output-display}\n![](models_building_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n다항식 차수를 달리하여 데이터에 1~10차 다항식을 적합시킨 후에 오차를 계산했다.\n1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그 이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록 낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가 필요한지는 의문이다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(차수 = 1:10) |> \n  mutate(데이터 = list(df)) |> \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |> \n  mutate(예측  = map(적합된모형, fitted)) |> \n  mutate(true_y = map(데이터, 2)) |>\n  # 참값과 모형예측값\n  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |> \n  # 오차\n  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |> \n  mutate(차수 = factor(차수)) |> \n  mutate(color = c(\"gray30\", \"blue\", rep(\"gray30\", 8))) |> \n  # 시각화\n  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs( x = \"차수\",\n          y = \"RMSE 오차\",\n          title = \"다항식 차수를 달리하여 계산한 RMSE 오차\") +\n    scale_fill_manual(values = c(\"blue\", \"gray30\", rep(\"blue\", 8))) +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](models_building_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "models_building_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
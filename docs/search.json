[
  {
    "objectID": "ingest_file.html",
    "href": "ingest_file.html",
    "title": "24  파일 데이터",
    "section": "",
    "text": "24.1 유니코드와 UTF-8\n사람 간의 의사소통은 다양한 기호 체계를 통해 이루어진다. 영어 알파벳, 한글, 한자 등 문자가 의사소통에 사용되는 좋은 예이다. 디지털 환경에서 이러한 의사소통을 가능하게 하는 기술적 장치가 바로 문자 집합과 문자 인코딩 및 디코딩이다.\n컴퓨터 시스템은 이진수 바이트를 기본 단위로 사용한다. 바이트는 파일 형태로 묶이거나 네트워크를 통해 전송되어 다른 시스템에 도달한다. 이 데이터가 사람에게 의미 있는 정보로 전달되기 위해서는 인코딩(부호화)과 디코딩(복호화) 과정을 거쳐야 한다.\n컴퓨터 시스템은 데이터를 바이트(Byte) 형태로 처리한다. 이 바이트 데이터는 이진수, 즉 010101과 같은 형태로 표현되고, 바이트 데이터를 사람이 읽을 수 있는 문자로 변환하는 최초의 표준이 ASCII(아스키)다. 하지만 ASCII는 256개 문자만을 지원하기 때문에, CJK(중국, 일본, 한국)와 같은 동아시아 문화권에서는 그 한계가 명확하다. 이러한 한계를 해결하기 위해 유니코드(Unicode)가 도입되었다. 유니코드는 영문자는 물론이고 지구상의 거의 모든 문자와 기호를 디지털로 표현할 수 있는 방법을 제공한다.\n유니코드(Unicode)는 글자와 코드가 1:1 매핑되어 있는 단순한 코드표에 불과하고 산업 표준으로 일종의 국가 당사자 간 약속이다. 한글이 표현된 유니코드 영역도 위키백과 유니코드 영역에서 찾을 수 있다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#유니코드와-utf-8",
    "href": "ingest_file.html#유니코드와-utf-8",
    "title": "24  파일 데이터",
    "section": "",
    "text": "유니코드와 UTF-8\n\n\n\n\n\n\n\n인코딩 (Encoding)\n\n\n\n문자 인코딩(character encoding), 줄여서 인코딩은 사용자가 입력한 문자나 기호들을 컴퓨터가 이용할 수 있는 신호로 만드는 것을 말한다. 넓은 의미의 컴퓨터는 이러한 신호를 입력받고 처리하는 기계를 뜻하며, 신호 처리 시스템을 통해 이렇게 처리된 정보를 사용자가 이해할 수 있게 된다.\n\nAll text has a character encoding.\n\n\n\n\n24.1.1 인코딩 문제\n문자 인코딩은 컴퓨터가 텍스트를 바이트로 변환하거나 바이트를 텍스트로 변환하는 방법이다. 인코딩 과정에서는 다양한 문제가 발생할 수 있고, 그중 세 가지 문제가 많이 알려져 있다. 첫 번째는 ’두부(Tofu)’라 불리는 상황으로, 컴퓨터가 어떤 문자를 표현해야 할지 알지만, 화면에 어떻게 출력해야 할지 모르기 때문에 빈 사각형 상자로 표시된다. 두 번째는 ’문자 깨짐(Mojibake, 文字化け)’이다. 특히 일본어에서 자주 발생하며, 한 인코딩 방식으로 작성된 텍스트가 다른 인코딩 방식으로 해석될 때 문자가 깨지는 현상을 의미한다. 세 번째는 ’의문부호(Question Marks)’로, 특정 문자가 다른 문자로 변환될 때 발생된다. 문자 집합과 인코딩 궁합이 맞지 않을 때 발생하며, 데이터 손실과 오류도 야기된다.\n\n\n세 가지 인코딩 문제\n\n\n24.1.2 문자 집합\n아스키 코드\n디지털 글쓰기는 내용과 상관없이 결국 텍스트로 표현되고, 텍스트는 단지 문자이다. 하지만, 컴퓨터가 문자 하나를 어떻게 표현할까?\n1960년대 미국식 영문자를 컴퓨터로 표현하는 해결책은 간단했다. 알파벳 26개(대문자, 소문자), 숫자 10개, 구두점 몇 개, 그리고 전신을 보내던 시절에 제어를 위해 사용된 몇 개의 특수 문자(“새줄로 이동”, “본문 시작”, “경고음” 등)가 전부였다. 모두 합쳐도 128개보다 적어서, 아스키(ASCII) 위원회가 문자마다 7비트( \\(2^7\\) = 128)를 사용하는 인코딩으로 표준화했다. 1\n\n\n\n\n\n그림 24.2: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n\n\n그림 24.2 는 아스키 문자표에서 제어문자 10개와 출력 가능한 아스키 문자표 중 영문 대문자 A-I까지 10개를 뽑아 사례로 보여준다. 즉, 문자표는 어떤 문자가 어떤 숫자에 해당하는지를 정의하고 있다.\n확장 아스키\n아스키(ASCII) 방식으로 숫자 2, 문자 q, 혹은 곡절 악센트 ^를 표현하는 데 충분하다. 하지만, 투르크어족 추바시어 ĕ, 그리스 문자 β, 러시아 키릴 문자 Я는 어떻게 저장하고 표현해야 할까? 7비트를 사용하면 0에서 127까지 숫자를 부여할 수 있지만, 8비트(즉, 1바이트)를 사용하게 되면 255까지 표현할 수 있다. 그렇다면, ASCII 표준을 확장해서 추가되는 128개 숫자에 대해 추가로 문자를 표현할 수 있게 된다.\n\n아스키: 0…127\n확장된 아스키: 128…255\n\n불행하게도, 영어 문자를 사용하지 않는 세계 곳곳에서 많은 사람들이 시도를 했지만, 방식도 다르고, 호환이 되지 않는 방식으로 작업이 되어, 결과는 엉망진창이 되었다. 예를 들어, 실제 텍스트가 불가리아어로 인코딩되었는데 스페인어 규칙을 사용해서 인코딩한 것으로 프로그램이 간주하고 처리될 경우 결과는 무의미한 횡설수설 값이 출력된다. 이와는 별도로 한중일(CJK) 동아시아 국가들을 비롯한 많은 국가들이 256개 이상의 기호를 사용한다. 왜냐하면 8비트로는 특히 동아시아 국가 문자를 표현하는 데 부족하기 때문이다.\n한글 완성형과 조합형\n1980년대부터 컴퓨터를 사용하신 분이라면 완성형과 조합형의 표준화 전쟁을 지켜봤을 것이고, 그 이면에는 한글 워드프로세서에 대한 주도권 쟁탈전이 있었던 것을 기억할 것이다. 결국 완성형과 조합형을 모두 포용하는 것으로 마무리되었지만, 여기서 끝난 게 아니다. 유닉스 계열에서 KSC5601을 표준으로 받아들인 EUC-KR과 90년대와 2000년대를 호령한 마이크로소프트 CP949가 있었다. 결국 대한민국 정부에서 주도한 표준화 전쟁은 유닉스/리눅스, 마이크로소프트 모두를 녹여내는 것으로 마무리되었고, 웹과 모바일 시대는 유니코드로 넘어가서 KSC5601이 유니코드의 원소로 들어가는 것으로 마무리되었다.\n이제 신경 쓸 것은 인코딩, 즉 utf-8만 신경 쓰면 된다. 그리고 남은 디지털 레거시 유산을 잘 처리하면 된다.\n\n\n\n\n\n\n유닉스/리눅스(EUC-KR), 윈도우(CP949)\n\n\n\nEUC-KR, CP949 모두 2바이트 한글을 표현하는 방식으로 동일점이 있지만, EUC-KR 방식은 KSC5601-87 완성형을 초기에 사용하였으나, KSC5601-92 조합형도 사용할 수 있도록 확장되었다. CP949는 확장 완성형으로도 불리며 EUC-KR에서 표현할 수 없는 한글 글자 8,822자를 추가한 것으로 마이크로소프트 코드페이지(Code Page) 949를 사용하면서 일반화되었다.\n\n\n유니코드\n1990년대에 나타나기 시작한 해결책을 유니코드(Unicode)라고 부른다. 예를 들어, 영어 A 대문자는 1바이트, 한글 가는 3바이트다. 유니코드는 정수값을 서로 다른 수만 개 문자와 기호를 표현하는 데 정의한다. ’A’는 U+0041, ’가’는 U+AC00과 같이 고유한 코드 포인트를 가진다. 하지만, 파일에 혹은 메모리에 문자열로 정수값을 저장하는 방식을 정의하지는 않는다.\n각 문자마다 8비트를 사용하던 방식에서 32비트 정수를 사용하는 방식으로 전환하면 되지만, 영어, 에스토니아어, 브라질 포르투갈어 같은 알파벳 언어권에는 상당한 공간 낭비가 발생된다. 접근 속도가 중요한 경우 메모리에 문자당 32비트를 종종 사용한다. 하지만, 파일에 데이터를 저장하거나 인터넷을 통해 전송하는 경우 대부분의 프로그램과 프로그래머는 이와는 다른 방식을 사용한다.\n다른 방식은 (거의) 항상 UTF-8으로 불리는 인코딩으로, 문자마다 가변 바이트를 사용한다. 하위 호환성을 위해, 첫 128개 문자(즉, 구 아스키 문자 집합)는 바이트 1개에 저장된다. 다음 1920개 문자는 바이트 2개를 사용해서 저장된다. 다음 61,000개는 바이트 3개를 사용해서 저장해 나간다.\n궁금하다면, 동작 방식이 다음 표에 나타나 있다. “전통적” 문자열은 문자마다 1바이트를 사용한다. 반대로, “유니코드” 문자열은 문자마다 충분한 메모리를 사용해서 어떤 텍스트 유형이든 저장한다. R, 파이썬 3.x에서 모든 문자열은 유니코드다. 엄청난 바이트를 읽어오거나 저장하여 내보내려고 할 때, 인코딩을 지정하는 것은 엄청난 고통이다.\n유니코드 문자열은 여는 인용부호 앞에 소문자 U를 붙여 표시한다. 유니코드 문자열을 바이트 문자열로 전환하려면, 인코딩을 명세해야만 된다. 항상 UTF-8을 사용해야 하고, 그 밖의 인코딩을 사용하는 경우 매우, 매우 특별히 좋은 사유가 있어야만 된다. 특별한 인코딩을 사용하는 경우 두 번 생각해 보라.\n\n\n아스키에서 유니코드로 진화과정\n\n컴퓨터가 처음 등장할 때 미국 영어권 중심 아스키가 아니고 4바이트로 전 세계 모든 글자를 표현할 수 있는 유니코드가 사용되었다면 한글을 컴퓨터에 표현하기 위한 지금과 같은 번거로움은 없었을 것이다. 돌이켜보면 초기 컴퓨터가 저장 용량 한계로 인해 유니코드가 표준으로 자리를 잡더라도 실용적인 이유로 인해서 한글을 컴퓨터에 표현하기 위한 다른 대안이 제시됐을 것도 분명해 보인다. 초창기 영어권을 중심으로 아스키 표준이 정립되어 현재까지 내려오고, 유니코드와 UTF-8 인코딩이 사실상 표준으로 자리 잡았으며, 그 사이 유닉스/리눅스 EUC-KR, 윈도우즈 CP949가 빈틈을 한동안 메우면서 역할을 담당했다.\n\n\n\n\n\n\n\n\n\n항목\nASCII (1963)\nEUC-KR (1980s)\nCP949 (1990s)\nUnicode (1991)\n\n\n\n범위\n128개의 문자\n2,350개의 한글 문자 등\n약 11,172개의 완성형 한글 문자 등\n143,859개의 문자 (버전 13.0 기준)\n\n\n비트 수\n7비트\n8~16비트\n8~16비트\n다양한 인코딩 방식 (UTF-8, UTF-16, UTF-32 등)\n\n\n표준\nANSI, ISO/IEC 646\nKS X 2901\n마이크로소프트\nISO/IEC 10646\n\n\n플랫폼\n다양한 시스템\nUNIX 계열, 일부 Windows\nWindows 계열\n다양한 플랫폼\n\n\n문자 집합\n영문 알파벳, 숫자, 특수 문자\n한글, 영문 알파벳, 숫자, 특수 문자\n한글, 한자, 영문 알파벳, 숫자, 특수 문자\n전 세계 언어, 특수 문자, 이모티콘 등\n\n\n확장성\n확장 불가능\n한정적\n더 많은 문자 지원\n높은 확장성\n\n\n국제성\n영어 중심\n한국어 중심\n한국어 중심\n다국어 지원\n\n\n유니코드 호환\n호환 가능 (U+0000 ~ U+007F)\n호환 불가, 변환 필요\n유니코드와 상호 변환 가능\n자체가 표준\n\n\nUTF-8\nUTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)은 유니코드 중에서 가장 널리 쓰이는 인코딩으로, 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로 켄 톰프슨과 롭 파이크가 제작했다.\nUTF-8 인코딩의 가장 큰 장점은 아스키(ASCII), 라틴-1(ISO-8859-1)과 호환되어, 문서를 처리하는 경우 아스키, 라틴-1 문서를 변환 없이 그대로 처리할 수 있고 영어를 비롯한 라틴계열 문서로 저장할 때 용량이 매우 작다. 이러한 이유로 많은 오픈소스 소프트웨어와 데이터를 생산하는 미국을 비롯한 유럽 언어권에서 UTF-8이 많이 사용되고 있지만, 한글은 한 글자당 3바이트 용량을 차지한다.\n웹 표준 인코딩\n스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 유니코드 + UTF-8 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.\n2010년 구글에서 발표한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. (unicode2010?) 웹 기반 플롯 디지털 도구를 활용하여 그래프(WebPlotDigitizer)에서 데이터를 추출하여 시각화하면 유사한 결과를 시각적으로 표현할 수 있다. 2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech 웹 기술 조사(Web Technology Surveys)를 통해 확인할 수 있다. 여기서 주목할 점은, 프랑스어, 독일어, 스페인어와 같은 서유럽 언어의 문자와 기호를 표현하는 ISO-8859-1 인코딩, 종종 “Latin-1”으로 불리는 8비트 문자 인코딩이 현저히 줄고 있다는 점이다.\n\n\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#문자-집합",
    "href": "ingest_file.html#문자-집합",
    "title": "24  파일 데이터",
    "section": "\n24.2 문자 집합",
    "text": "24.2 문자 집합\n\n24.2.1 아스키 코드\n디지털 글쓰기는 내용과 상관없이 결국 텍스트로 표현되고, 텍스트는 단지 문자이다. 하지만, 컴퓨터가 문자 하나를 어떻게 표현할까?\n1960년대 미국식 영문자를 컴퓨터로 표현하는 해결책은 간단했다. 알파벳 26개(대문자, 소문자), 숫자 10개, 구두점 몇 개, 그리고 전신을 보내던 시절에 제어를 위해 사용된 몇 개의 특수 문자(“새줄로 이동”, “본문 시작”, “경고음” 등)가 전부였다. 모두 합쳐도 128개보다 적어서, 아스키(ASCII) 위원회가 문자마다 7비트( \\(2^7\\) = 128)를 사용하는 인코딩으로 표준화했다. 1\n\n\n\n\n\n그림 24.2: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n\n\n그림 24.2 는 아스키 문자표에서 제어문자 10개와 출력 가능한 아스키 문자표 중 영문 대문자 A-I까지 10개를 뽑아 사례로 보여준다. 즉, 문자표는 어떤 문자가 어떤 숫자에 해당하는지를 정의하고 있다.\n\n24.2.2 확장 아스키\n아스키(ASCII) 방식으로 숫자 2, 문자 q, 혹은 곡절 악센트 ^를 표현하는 데 충분하다. 하지만, 투르크어족 추바시어 ĕ, 그리스 문자 β, 러시아 키릴 문자 Я는 어떻게 저장하고 표현해야 할까? 7비트를 사용하면 0에서 127까지 숫자를 부여할 수 있지만, 8비트(즉, 1바이트)를 사용하게 되면 255까지 표현할 수 있다. 그렇다면, ASCII 표준을 확장해서 추가되는 128개 숫자에 대해 추가로 문자를 표현할 수 있게 된다.\n\n아스키: 0…127\n확장된 아스키: 128…255\n\n불행하게도, 영어 문자를 사용하지 않는 세계 곳곳에서 많은 사람들이 시도를 했지만, 방식도 다르고, 호환이 되지 않는 방식으로 작업이 되어, 결과는 엉망진창이 되었다. 예를 들어, 실제 텍스트가 불가리아어로 인코딩되었는데 스페인어 규칙을 사용해서 인코딩한 것으로 프로그램이 간주하고 처리될 경우 결과는 무의미한 횡설수설 값이 출력된다. 이와는 별도로 한중일(CJK) 동아시아 국가들을 비롯한 많은 국가들이 256개 이상의 기호를 사용한다. 왜냐하면 8비트로는 특히 동아시아 국가 문자를 표현하는 데 부족하기 때문이다.\n\n24.2.3 한글 완성형과 조합형\n1980년대부터 컴퓨터를 사용하신 분이라면 완성형과 조합형의 표준화 전쟁을 지켜봤을 것이고, 그 이면에는 한글 워드프로세서에 대한 주도권 쟁탈전이 있었던 것을 기억할 것이다. 결국 완성형과 조합형을 모두 포용하는 것으로 마무리되었지만, 여기서 끝난 게 아니다. 유닉스 계열에서 KSC5601을 표준으로 받아들인 EUC-KR과 90년대와 2000년대를 호령한 마이크로소프트 CP949가 있었다. 결국 대한민국 정부에서 주도한 표준화 전쟁은 유닉스/리눅스, 마이크로소프트 모두를 녹여내는 것으로 마무리되었고, 웹과 모바일 시대는 유니코드로 넘어가서 KSC5601이 유니코드의 원소로 들어가는 것으로 마무리되었다.\n이제 신경 쓸 것은 인코딩, 즉 utf-8만 신경 쓰면 된다. 그리고 남은 디지털 레거시 유산을 잘 처리하면 된다.\n\n\n\n\n\n\n유닉스/리눅스(EUC-KR), 윈도우(CP949)\n\n\n\nEUC-KR, CP949 모두 2바이트 한글을 표현하는 방식으로 동일점이 있지만, EUC-KR 방식은 KSC5601-87 완성형을 초기에 사용하였으나, KSC5601-92 조합형도 사용할 수 있도록 확장되었다. CP949는 확장 완성형으로도 불리며 EUC-KR에서 표현할 수 없는 한글 글자 8,822자를 추가한 것으로 마이크로소프트 코드페이지(Code Page) 949를 사용하면서 일반화되었다.\n\n\n\n24.2.4 유니코드\n1990년대에 나타나기 시작한 해결책을 유니코드(Unicode)라고 부른다. 예를 들어, 영어 A 대문자는 1바이트, 한글 가는 3바이트다. 유니코드는 정수값을 서로 다른 수만 개 문자와 기호를 표현하는 데 정의한다. ’A’는 U+0041, ’가’는 U+AC00과 같이 고유한 코드 포인트를 가진다. 하지만, 파일에 혹은 메모리에 문자열로 정수값을 저장하는 방식을 정의하지는 않는다.\n각 문자마다 8비트를 사용하던 방식에서 32비트 정수를 사용하는 방식으로 전환하면 되지만, 영어, 에스토니아어, 브라질 포르투갈어 같은 알파벳 언어권에는 상당한 공간 낭비가 발생된다. 접근 속도가 중요한 경우 메모리에 문자당 32비트를 종종 사용한다. 하지만, 파일에 데이터를 저장하거나 인터넷을 통해 전송하는 경우 대부분의 프로그램과 프로그래머는 이와는 다른 방식을 사용한다.\n다른 방식은 (거의) 항상 UTF-8으로 불리는 인코딩으로, 문자마다 가변 바이트를 사용한다. 하위 호환성을 위해, 첫 128개 문자(즉, 구 아스키 문자 집합)는 바이트 1개에 저장된다. 다음 1920개 문자는 바이트 2개를 사용해서 저장된다. 다음 61,000개는 바이트 3개를 사용해서 저장해 나간다.\n궁금하다면, 동작 방식이 다음 표에 나타나 있다. “전통적” 문자열은 문자마다 1바이트를 사용한다. 반대로, “유니코드” 문자열은 문자마다 충분한 메모리를 사용해서 어떤 텍스트 유형이든 저장한다. R, 파이썬 3.x에서 모든 문자열은 유니코드다. 엄청난 바이트를 읽어오거나 저장하여 내보내려고 할 때, 인코딩을 지정하는 것은 엄청난 고통이다.\n유니코드 문자열은 여는 인용부호 앞에 소문자 U를 붙여 표시한다. 유니코드 문자열을 바이트 문자열로 전환하려면, 인코딩을 명세해야만 된다. 항상 UTF-8을 사용해야 하고, 그 밖의 인코딩을 사용하는 경우 매우, 매우 특별히 좋은 사유가 있어야만 된다. 특별한 인코딩을 사용하는 경우 두 번 생각해 보라.\n\n\n아스키에서 유니코드로 진화과정\n\n컴퓨터가 처음 등장할 때 미국 영어권 중심 아스키가 아니고 4바이트로 전 세계 모든 글자를 표현할 수 있는 유니코드가 사용되었다면 한글을 컴퓨터에 표현하기 위한 지금과 같은 번거로움은 없었을 것이다. 돌이켜보면 초기 컴퓨터가 저장 용량 한계로 인해 유니코드가 표준으로 자리를 잡더라도 실용적인 이유로 인해서 한글을 컴퓨터에 표현하기 위한 다른 대안이 제시됐을 것도 분명해 보인다. 초창기 영어권을 중심으로 아스키 표준이 정립되어 현재까지 내려오고, 유니코드와 UTF-8 인코딩이 사실상 표준으로 자리 잡았으며, 그 사이 유닉스/리눅스 EUC-KR, 윈도우즈 CP949가 빈틈을 한동안 메우면서 역할을 담당했다.\n\n\n\n\n\n\n\n\n\n항목\nASCII (1963)\nEUC-KR (1980s)\nCP949 (1990s)\nUnicode (1991)\n\n\n\n범위\n128개의 문자\n2,350개의 한글 문자 등\n약 11,172개의 완성형 한글 문자 등\n143,859개의 문자 (버전 13.0 기준)\n\n\n비트 수\n7비트\n8~16비트\n8~16비트\n다양한 인코딩 방식 (UTF-8, UTF-16, UTF-32 등)\n\n\n표준\nANSI, ISO/IEC 646\nKS X 2901\n마이크로소프트\nISO/IEC 10646\n\n\n플랫폼\n다양한 시스템\nUNIX 계열, 일부 Windows\nWindows 계열\n다양한 플랫폼\n\n\n문자 집합\n영문 알파벳, 숫자, 특수 문자\n한글, 영문 알파벳, 숫자, 특수 문자\n한글, 한자, 영문 알파벳, 숫자, 특수 문자\n전 세계 언어, 특수 문자, 이모티콘 등\n\n\n확장성\n확장 불가능\n한정적\n더 많은 문자 지원\n높은 확장성\n\n\n국제성\n영어 중심\n한국어 중심\n한국어 중심\n다국어 지원\n\n\n유니코드 호환\n호환 가능 (U+0000 ~ U+007F)\n호환 불가, 변환 필요\n유니코드와 상호 변환 가능\n자체가 표준\n\n\n\n24.2.5 UTF-8\nUTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)은 유니코드 중에서 가장 널리 쓰이는 인코딩으로, 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로 켄 톰프슨과 롭 파이크가 제작했다.\nUTF-8 인코딩의 가장 큰 장점은 아스키(ASCII), 라틴-1(ISO-8859-1)과 호환되어, 문서를 처리하는 경우 아스키, 라틴-1 문서를 변환 없이 그대로 처리할 수 있고 영어를 비롯한 라틴계열 문서로 저장할 때 용량이 매우 작다. 이러한 이유로 많은 오픈소스 소프트웨어와 데이터를 생산하는 미국을 비롯한 유럽 언어권에서 UTF-8이 많이 사용되고 있지만, 한글은 한 글자당 3바이트 용량을 차지한다.\n\n24.2.6 웹 표준 인코딩\n스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 유니코드 + UTF-8 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.\n2010년 구글에서 발표한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. (unicode2010?) 웹 기반 플롯 디지털 도구를 활용하여 그래프(WebPlotDigitizer)에서 데이터를 추출하여 시각화하면 유사한 결과를 시각적으로 표현할 수 있다. 2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech 웹 기술 조사(Web Technology Surveys)를 통해 확인할 수 있다. 여기서 주목할 점은, 프랑스어, 독일어, 스페인어와 같은 서유럽 언어의 문자와 기호를 표현하는 ISO-8859-1 인코딩, 종종 “Latin-1”으로 불리는 8비트 문자 인코딩이 현저히 줄고 있다는 점이다.\n\n\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#데이터-종류",
    "href": "ingest_file.html#데이터-종류",
    "title": "24  파일 데이터",
    "section": "\n24.4 데이터 종류",
    "text": "24.4 데이터 종류\n\n\n\n\n\ngraph TB\n\nsubgraph 가져오기[\"가져오기(Import)\"]\n\n    스프레드쉬트 --&gt; 핸들러\n    데이터베이스 --&gt; 핸들러\n    아스키 --&gt; 핸들러\n    웹데이터 --&gt; 핸들러\n    핸들러 --&gt; 데이터프레임\n\n    subgraph 아스키[\"&lt;strong&gt;아스키 파일&lt;/strong&gt;\"]\n        데이터입력[데이터 입력]\n        csv[CSV 파일]\n        tsv[TSV 파일]\n        고정길이파일[고정길이 파일]\n    end\n\nend\n\nclassDef modern fill:#f0f0f0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#d0d0d0,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n\nclass csv,데이터입력,tsv,고정길이파일,스프레드쉬트,데이터베이스,웹데이터,핸들러 modern\nclass 데이터프레임 emphasize\nclass 아스키,가져오기 subgraphStyle\n\n\n\n\n그림 24.3: 다양한 데이터 종류",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#아스키-파일",
    "href": "ingest_file.html#아스키-파일",
    "title": "24  파일 데이터",
    "section": "\n24.2 아스키 파일",
    "text": "24.2 아스키 파일\n아스키 파일은 텍스트 파일로, 데이터를 저장하는 가장 기본적인 형태이다. R에서 데이터프레임으로 다양한 데이터를 가져올 때, 아스키 파일은 CSV(Comma-Separated Values) 파일, TSV(Tab-Separated Values) 파일, 고정길이 파일 등 다양한 형식으로 존재한다. CSV 파일은 쉼표로 구분된 값들로 이루어진 텍스트 파일이며, TSV 파일은 탭으로 구분된 값들로 이루어진 텍스트 파일이다. 고정길이 파일은 각 필드가 고정된 길이를 가지는 텍스트 파일이다. 또한, R에서는 데이터를 직접 입력하여 데이터프레임을 생성할 수도 있다. 아스키 파일을 데이터프레임으로 가져올 때는 read.csv(), read.table(), read.fwf() 등의 함수를 사용하며, 데이터를 직접 입력할 때는 열중심 혹은 행중심에 따라 tibble(), tribble() 함수를 사용한다.\n\n\n\n\n\ngraph TB\n\nsubgraph 가져오기[\"가져오기(Import)\"]\n\n    스프레드쉬트 --&gt; 핸들러\n    데이터베이스 --&gt; 핸들러\n    아스키 --&gt; 핸들러\n    웹데이터 --&gt; 핸들러\n    핸들러 --&gt; 데이터프레임\n\n    subgraph 아스키[\"&lt;strong&gt;아스키 파일&lt;/strong&gt;\"]\n        데이터입력[데이터 입력]\n        csv[CSV 파일]\n        tsv[TSV 파일]\n        고정길이파일[고정길이 파일]\n    end\n\nend\n\nclassDef modern fill:#f0f0f0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#d0d0d0,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n\nclass csv,데이터입력,tsv,고정길이파일,스프레드쉬트,데이터베이스,웹데이터,핸들러 modern\nclass 데이터프레임 emphasize\nclass 아스키,가져오기 subgraphStyle\n\n\n\n\n그림 24.3: 다양한 데이터 종류",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#자료형",
    "href": "ingest_file.html#자료형",
    "title": "24  파일 데이터",
    "section": "\n24.4 자료형",
    "text": "24.4 자료형\n가장 많이 사용되는 콤마 구분자 아스키 파일(.csv) 파일로 불러오면서 함께 고민해야 하는 사항이 바로 자료형이다.\nR에서는 다양한 자료형을 지원한다. 가장 기본적인 자료형은 숫자형, 문자형, 범주형, 논리형이다. 숫자형은 정수형과 실수형으로 구분되며, 문자형은 문자열을 저장하는 자료형이다. 논리형은 참과 거짓을 나타내는 자료형이고, 범주형은 내부적으로 정수로 저장되지만 한정된 범주를 갖는 문자형으로 표현된다. 그외에도 날짜와 시간을 저장하는 자료형, 지도정보를 담고 있는 자료형, 이미지 정보를 담고 있는 자료형 등 다양한 자료형이 있다.\nreadr 패키지 spec() 함수를 사용하면 아스키 파일을 불러읽어 오면서 각 열의 자료형을 확인할 수 있다. spec() 함수에서 출력한 각 열 자료형이 정답은 아니지만 나름 최선의 추정으로 각 열의 자료형을 살펴본 후 최종 열별 자료형을 지정하는데 도움이 되는 것은 사실이다.\n\nspec( read_csv(\"data/file/nine_penguins.csv\") )\n#&gt; cols(\n#&gt;   species = col_character(),\n#&gt;   island = col_character(),\n#&gt;   bill_length_mm = col_double(),\n#&gt;   flipper_length_mm = col_double(),\n#&gt;   body_mass_g = col_double(),\n#&gt;   sex = col_character(),\n#&gt;   year = col_double()\n#&gt; )\n\nreadr 패키지 col_types 인자를 사용하여 각 열의 자료형을 지정할 수 있다. col_types 인자에는 cols() 함수를 사용하여 각 열의 자료형을 지정한다. cols() 함수에는 col_factor(), col_character(), col_double(), col_integer(), col_logical() 함수를 사용하여 각 열의 자료형을 지정한다. col_factor() 함수는 범주형 자료형을 지정할 때 사용하며, col_character() 함수는 문자형 자료형을 지정할 때 사용한다. col_double() 함수는 실수형 자료형을 지정할 때 사용하며, col_integer() 함수는 정수형 자료형을 지정할 때 사용한다. col_logical() 함수는 논리형 자료형을 지정할 때 사용한다.\nspec() 함수가 텍스트로 된 열은 모두 문자형(col_character())으로 인식하였지만, species, sex 열은 범주형 자료형으로 지정하는 것이 더 적절하다. bill_length_mm, flipper_length_mm, body_mass_g, year 열은 실수형, 정수형 자료형으로 지정하는 것이 적절하다고 판단되어 다음과 같이 .csv 파일을 불러오면서 각 열의 자료형도 함께 지정한다.\n\npenguins_tbl &lt;- read_csv(\"data/file/nine_penguins.csv\",\n         col_types = cols(\n            species = col_factor(level = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n            island = col_character(),\n            bill_length_mm = col_double(),\n            flipper_length_mm = col_double(),\n            body_mass_g = col_double(),\n            sex = col_factor(levels = c(\"female\", \"male\")),\n            year = col_integer()\n          )\n)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island    bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 Adelie    Torgersen           33.5               190        3600 female  2008\n#&gt; 2 Adelie    Biscoe              45.6               191        4600 male    2009\n#&gt; 3 Adelie    Biscoe              37.6               194        3750 male    2008\n#&gt; 4 Gentoo    Biscoe              45.2               212        5200 female  2009\n#&gt; 5 Gentoo    Biscoe              43.8               208        4300 female  2008\n#&gt; 6 Gentoo    Biscoe              51.1               225        5250 male    2009\n#&gt; 7 Chinstrap Dream               47.6               195        3850 female  2008\n#&gt; 8 Chinstrap Dream               49                 210        3950 male    2008\n#&gt; 9 Chinstrap Dream               51.3               193        3650 male    2007",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#공공-데이터",
    "href": "ingest_file.html#공공-데이터",
    "title": "24  파일 데이터",
    "section": "\n24.6 공공 데이터",
    "text": "24.6 공공 데이터\n공공데이터포털을 비롯한 많은 정부기관에서 제공하는 데이터는 대부분 EUC-KR로 인코딩되어 있다. 이유는 여러가지가 있겠지만 가장 큰 이유는 아마도 엑셀에서 .csv 파일을 열었을 때 한글이 깨지는 민원을 처리하기 위함이 아닐까 싶다. 정형 .csv 파일 형태로 데이터를 받게 되면 먼저 인코딩을 확인해야 한다. readr 패키지의 guess_encoding() 함수를 사용하면 파일의 인코딩을 확인할 수 있다.\n공공데이터포털 인천광역시_정류장별 이용승객 현황 데이터를 다운로드 받아 로컬 파일로 저장한 후 인코딩을 확인한다.\n\nlibrary(readr)\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\nguess_encoding(file_path)\n#&gt; # A tibble: 4 × 2\n#&gt;   encoding confidence\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 EUC-KR         1   \n#&gt; 2 GB18030        0.81\n#&gt; 3 Big5           0.48\n#&gt; 4 EUC-JP         0.3\n\n따라서, 이를 바로 read_csv() 함수로 읽을 경우 오류가 발생된다. 왜냐하면 read_csv() 함수는 인코딩을 UTF-8을 기본으로 가정하고 있기 때문이다.\n\nread_csv(file_path)\n#&gt; Error in nchar(x, \"width\"): invalid multibyte string, element 1\n\n따라서, read_csv() 함수를 사용할 때는 locale 인수를 사용하여 인코딩을 지정해주어야 한다. “EUC-KR”로 인코딩을 지정하면 파일을 오류없이 읽을 수 있다.\n\nincheon_bus &lt;- spec(read_csv(file_path, locale = locale(encoding = \"EUC-KR\")))\nincheon_bus |&gt; names() |&gt; dput()\n#&gt; c(\"cols\", \"default\", \"delim\")\n\n데이터 가져오기는 데이터 분석의 첫 단계로, 외부 데이터를 R로 불러오는 과정으로 첫단추가 이후 이어질 분석단계에서 중요한 역할을 한다.\n먼저, 파일 형식에 따라 적절한 함수를 선택해야 한다. 텍스트 파일은 read.csv, read.table 등의 함수를 사용하고, 엑셀 파일은 readxl 패키지의 read_excel 함수를 사용한다. 특히, 인코딩도 이 단계에서 반듯이 확인해야 한다.\n데이터 전처리 단계에서는 구분자와 헤더 유무를 확인하고, 자료형과 칼럼명을 결정해야 한다. 결측값 처리를 위해 na = 옵션을 사용할 수 있고, 필요에 따라 특정 행/열을 선택하는 등의 추가 옵션을 설정할 수 있다.\n전처리 과정을 거쳐 최종적으로 데이터프레임을 생성하게 된다. 다소 번거럽더라도 데이터를 가져오는 단계에서 전처리 과정을 충실히 수행하게 되면 이후 dplyr, tidyr 패키지 등을 활용해 다양한 데이터 조작 및 시각화를 수월하게 할 수 있다.\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\n\nincheon_bus &lt;- read_csv(file_path, locale = locale(encoding = \"EUC-KR\"),\n                        skip = 1,\n                        na = c(\"---\", \"\"),\n                        col_names = c(\"정류소명\", \"정류소_id\", \"승차건수_총합계\", \n                                     \"하차건수_총합계\",\"승차건수_카드\", \"하차건수_카드\",\n                                     \"승차건수_현금\", \"일평균_승하차건수\"),\n                       col_types = cols(\n                         정류소명 = col_character(),\n                         정류소_id = col_double(),\n                         승차건수_총합계 = col_double(),\n                         하차건수_총합계 = col_double(),\n                         승차건수_카드 = col_double(),\n                         하차건수_카드 = col_double(),\n                         승차건수_현금 = col_double(),\n                         일평균_승하차건수 = col_double()\n                       ))  \nincheon_bus\n#&gt; # A tibble: 6,386 × 8\n#&gt;    정류소명             정류소_id 승차건수_총합계 하차건수_총합계 승차건수_카드\n#&gt;    &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 (구)국제여객터미널       35051              95            1923            21\n#&gt;  2 (구)국제여객터미널          NA            2512              43          2465\n#&gt;  3 (주)경동세라믹스         89146             341              26           335\n#&gt;  4 (주)경인양행앞           42096             945             923           938\n#&gt;  5 (주)경인양행앞           42097            1322            3536          1294\n#&gt;  6 (주)대한특수금속         39050            1243              89          1238\n#&gt;  7 (주)두남                 39135             147              29           147\n#&gt;  8 (주)세모입구(린나이…     37585            1410            1517          1404\n#&gt;  9 (주)세모입구(린나이…     40893             307             556           304\n#&gt; 10 (주)스킨이데아           89388             147             148           147\n#&gt; # ℹ 6,376 more rows\n#&gt; # ℹ 3 more variables: 하차건수_카드 &lt;dbl&gt;, 승차건수_현금 &lt;dbl&gt;,\n#&gt; #   일평균_승하차건수 &lt;dbl&gt;\n\n지금까지 작업한 전반적인 작업흐름은 그림 24.4 에 대략적으로 나와있다. 공공데이터포털에서 다운로드 받은 인천광역시_정류장별 이용승객 현황_20220630.csv은 EUC-KR로 인코딩 되어 있고 헤더를 갖고 있으며 쉼표로 구분되어 있다. 결측치는 없으나 임의로 --- 으로 정류장 한 곳을 달리 표현하여 na = c(\"---\", \"\")로 결측값 처리를 하였다.\n\n\n\n\n\ngraph LR\n    subgraph \"&lt;strong&gt;파일 형식 결정&lt;/strong&gt;\"\n    A[파일 형식 결정] --&gt; |\"read.csv, read.table 등\"| B[인코딩 확인]\n    A --&gt; |\"readxl::read_excel 등\"| B\n    end\n\n    subgraph \"&lt;strong&gt;데이터 전처리&lt;/strong&gt;\"\n    C[구분자 확인] --&gt; |\"쉼표, 탭 등\"| B\n    D[헤더 유무] --&gt; |\"header = TRUE/FALSE\"| B\n    B --&gt; E[자료형 및&lt;br&gt; 칼럼명 결정]\n    F[결측값 처리] --&gt; |\"na 옵션\"| E\n    G[추가 옵션] --&gt; |\"특정 행/열 선택 등\"| E\n    end\n\nE --&gt; H[데이터프레임&lt;br&gt;생성]\n\nstyle A fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle B fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle C fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle D fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle E fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle F fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle G fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle H fill:#c0c0c0,stroke:#333,stroke-width:2px\n\n\n\n\n그림 24.4: 인천광역시 정류장별 이용승객 현황 데이터 데이터프레임 가져오는 과정",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#텍스트-바로-읽기",
    "href": "ingest_file.html#텍스트-바로-읽기",
    "title": "24  파일 데이터",
    "section": "\n24.7 텍스트 바로 읽기",
    "text": "24.7 텍스트 바로 읽기\n파일 크기가 적은 경우 즉, 눈으로 식별가능한 크기를 갖는 아스키 파일을 .csv, .txt 등의 형식으로 저장한 후 readr 패키지 read_csv(), read_table(), read_delim() 등의 함수로 불러오는 것이 오히려 적절하지 못한 경우가 있다.\ntibble() 혹은 tribble() 함수를 사용해서 인라인 데이터를 생성하는 것이 더 효율적일 수 있다. 다음과 같이 쇼핑몰 초창기 고객 주문 데이터를 R로 불러와서 분석하는 경우를 살펴보자.\n주문일자,주문번호,고객번호,상품명,상품범주,주문금액\n\"2023-05-19 13:45:32\",203451,A20193,\"슬림핏 반팔 티셔츠\",의류,21800\n2023/05/19 14:23:11,203452,B10582,\"여성용 스니커즈, 240mm\",,68000\n\"2023.05.19 16:05:49\",203453,\"C30281\",\"진공 보온병, 500ml\",주방용품,\"35,600\"\n2023-05-20 09:18:22, 203454,\"D18734\",\"\"귀걸이\"세트 (실버)\",액세서리,112000\n2023-05-20 11:36:58,,E42097,남성용 슬림 진 (32인치),의류,54900\n2023/05/21 08:02:44,\"203,456\",F61052,\"무선 게이밍 마우스\",전자기기,\"\"88,700\"\"\n2023.05.21 15:30:05,203457,,,,42300\n데이터가 크지 않기 때문에 칼럼 혹은 행 기준으로 데이터프레임으로 불러올 수 있다. 먼저 tibble() 함수를 사용해서 데이터프레임을 생성한다. 실무에서 결측값도 있고 주문금액에 천단위 , 구분자도 포함되어 있고 날짜 형식도 다양하게 표현되어 있을 수 있다.\n\nlibrary(tibble)\n\norders &lt;- tribble(\n  ~주문일자, ~주문번호, ~고객번호, ~상품명, ~상품범주, ~주문금액,\n  \"2023-05-19 13:45:32\", \"203451\", \"A20193\", \"슬림핏 반팔 티셔츠\", \"의류\", \"21800\",\n  \"2023/05/19 14:23:11\", \"203452\", \"B10582\", \"여성용 스니커즈, 240mm\", NA, \"68000\",\n  \"2023.05.19 16:05:49\", \"203453\", \"C30281\", \"진공 보온병, 500ml\", \"주방용품\", \"35,600\",\n  \"2023-05-20 09:18:22\", \"203454\", \"D18734\", \"귀걸이세트 (실버)\", \"액세서리\", \"112000\",\n  \"2023-05-20 11:36:58\", NA, \"E42097\", \"남성용 슬림 진 (32인치)\", \"의류\", \"54900\",\n  \"2023/05/21 08:02:44\", \"203,456\", \"F61052\", \"무선 게이밍 마우스\", \"전자기기\", \"88,700\",\n  \"2023.05.21 15:30:05\", \"203457\", NA, NA, NA, \"42300\"\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntibble() 함수는 벡터를 기준으로 칼럼을 생성하고 이를 tibble() 함수로 결합하여 데이터프레임을 생성한다.\n\nlibrary(tibble)\n\norders &lt;- tibble(\n 주문일자 = c(\"2023-05-19 13:45:32\", \"2023/05/19 14:23:11\", \"2023.05.19 16:05:49\", \n             \"2023-05-20 09:18:22\", \"2023-05-20 11:36:58\", \"2023/05/21 08:02:44\", \n             \"2023.05.21 15:30:05\"),\n 주문번호 = c(\"203451\", \"203452\", \"203453\", \"203454\", NA, \"203,456\", \"203457\"),\n 고객번호 = c(\"A20193\", \"B10582\", \"C30281\", \"D18734\", \"E42097\", \"F61052\", NA),\n 상품명 = c(\"슬림핏 반팔 티셔츠\", \"여성용 스니커즈, 240mm\", \"진공 보온병, 500ml\", \n           \"귀걸이세트 (실버)\", \"남성용 슬림 진 (32인치)\", \"무선 게이밍 마우스\", NA),\n 상품범주 = c(\"의류\", NA, \"주방용품\", \"액세서리\", \"의류\", \"전자기기\", NA),\n 주문금액 = c(\"21800\", \"68000\", \"35,600\", \"112000\", \"54900\", \"88,700\", \"42300\")\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#footnotes",
    "href": "ingest_file.html#footnotes",
    "title": "24  파일 데이터",
    "section": "",
    "text": "미국정보교환표준부호(American Standard Code for Information Interchange, ASCII)는 영문 알파벳을 사용하는 대표적인 문자 인코딩으로 컴퓨터와 통신 장비를 비롯한 문자를 사용하는 많은 장치에서 사용되며, 대부분의 문자 인코딩이 아스키에 기초하고 있다.↩︎",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n30  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n31 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "31  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "32  챗GPT 자연어",
    "section": "",
    "text": "33 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n34 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35 Notable.ai\n\n\n36 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>챗GPT 자연어</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data\nAnalysis.” Journal of Statistical Software 40: 1–29.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "ingest_file.html#데이터-입력방식",
    "href": "ingest_file.html#데이터-입력방식",
    "title": "24  파일 데이터",
    "section": "\n24.3 데이터 입력방식",
    "text": "24.3 데이터 입력방식\n파일 크기가 적은 경우 즉, 눈으로 식별가능한 크기를 갖는 아스키 파일을 .csv, .txt 등의 형식으로 저장한 후 readr 패키지 read_csv(), read_table(), read_delim() 등의 함수로 불러오는 것이 오히려 적절하지 못한 경우가 있다.\ntibble() 혹은 tribble() 함수를 사용해서 인라인 데이터를 생성하는 것이 더 효율적일 수 있다. 다음과 같이 쇼핑몰 초창기 고객 주문 데이터를 입력하여 R로 불러와서 분석하는 방법를 살펴보자.\n주문일자,주문번호,고객번호,상품명,상품범주,주문금액\n\"2023-05-19 13:45:32\",203451,A20193,\"슬림핏 반팔 티셔츠\",의류,21800\n2023/05/19 14:23:11,203452,B10582,\"여성용 스니커즈, 240mm\",,68000\n\"2023.05.19 16:05:49\",203453,\"C30281\",\"진공 보온병, 500ml\",주방용품,\"35,600\"\n2023-05-20 09:18:22, 203454,\"D18734\",\"\"귀걸이\"세트 (실버)\",액세서리,112000\n2023-05-20 11:36:58,,E42097,남성용 슬림 진 (32인치),의류,54900\n2023/05/21 08:02:44,\"203,456\",F61052,\"무선 게이밍 마우스\",전자기기,\"\"88,700\"\"\n2023.05.21 15:30:05,203457,,,,42300\n데이터가 크지 않기 때문에 칼럼 혹은 행 기준으로 데이터프레임으로 불러올 수 있다. 먼저 tibble() 함수를 사용해서 데이터프레임을 생성한다. 실무에서 결측값도 있고 주문금액에 천단위 , 구분자도 포함되어 있고 날짜 형식도 다양하게 표현되어 있을 수 있다.\n\nlibrary(tibble)\n\norders &lt;- tribble(\n  ~주문일자, ~주문번호, ~고객번호, ~상품명, ~상품범주, ~주문금액,\n  \"2023-05-19 13:45:32\", \"203451\", \"A20193\", \"슬림핏 반팔 티셔츠\", \"의류\", \"21800\",\n  \"2023/05/19 14:23:11\", \"203452\", \"B10582\", \"여성용 스니커즈, 240mm\", NA, \"68000\",\n  \"2023.05.19 16:05:49\", \"203453\", \"C30281\", \"진공 보온병, 500ml\", \"주방용품\", \"35,600\",\n  \"2023-05-20 09:18:22\", \"203454\", \"D18734\", \"귀걸이세트 (실버)\", \"액세서리\", \"112000\",\n  \"2023-05-20 11:36:58\", NA, \"E42097\", \"남성용 슬림 진 (32인치)\", \"의류\", \"54900\",\n  \"2023/05/21 08:02:44\", \"203,456\", \"F61052\", \"무선 게이밍 마우스\", \"전자기기\", \"88,700\",\n  \"2023.05.21 15:30:05\", \"203457\", NA, NA, NA, \"42300\"\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntibble() 함수는 벡터를 기준으로 칼럼을 생성하고 이를 tibble() 함수로 결합하여 데이터프레임을 생성한다.\n\nlibrary(tibble)\n\norders &lt;- tibble(\n 주문일자 = c(\"2023-05-19 13:45:32\", \"2023/05/19 14:23:11\", \"2023.05.19 16:05:49\", \n             \"2023-05-20 09:18:22\", \"2023-05-20 11:36:58\", \"2023/05/21 08:02:44\", \n             \"2023.05.21 15:30:05\"),\n 주문번호 = c(\"203451\", \"203452\", \"203453\", \"203454\", NA, \"203,456\", \"203457\"),\n 고객번호 = c(\"A20193\", \"B10582\", \"C30281\", \"D18734\", \"E42097\", \"F61052\", NA),\n 상품명 = c(\"슬림핏 반팔 티셔츠\", \"여성용 스니커즈, 240mm\", \"진공 보온병, 500ml\", \n           \"귀걸이세트 (실버)\", \"남성용 슬림 진 (32인치)\", \"무선 게이밍 마우스\", NA),\n 상품범주 = c(\"의류\", NA, \"주방용품\", \"액세서리\", \"의류\", \"전자기기\", NA),\n 주문금액 = c(\"21800\", \"68000\", \"35,600\", \"112000\", \"54900\", \"88,700\", \"42300\")\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntribble(), tibble() 함수 모두 데이터 입력을 통해 orders 데이터프레임을 생성하였으나 결측값에 대한 처리와 자료형이 모두 문자형(&lt;chr&gt;)으로 되어 있어 후속작업을 위해 추가 데이터 정제 작업이 필수적이다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다양한-파일형태",
    "href": "ingest_file.html#다양한-파일형태",
    "title": "24  파일 데이터",
    "section": "\n24.5 다양한 파일형태",
    "text": "24.5 다양한 파일형태\n\n24.5.1 파일 저장\n펭귄 데이터에서 종별로 3마리를 무작위로 추출해서 nine_penguins 데이터프레임을 만든 후에 다양한 형식 아스키 파일로 저장한다. 펭귄 9마리 데이터프레임으로 아스크 파일 형식으로 저장된 다양한 형태(탭 구분자, 콤마 구분자, 고정길이) 데이터를 불러오는 방법을 살펴본다. 구분자로 탭과 콤마가 가장 많이 사용되지만 경우에 따라서는 “;”, “:”, “|” 등 다양한 구분자를 사용할 수 있다.\n\nlibrary(palmerpenguins)\n\nnine_penguins &lt;- palmerpenguins::penguins |&gt; \n    drop_na() |&gt; \n    slice_sample(n = 3, replace = FALSE, by = species) |&gt; \n    select(-bill_depth_mm)\n\n탭 구분자\nwrite_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일로 저장하는 방법과 write_tsv() 함수를 사용하는 방법이 있다. 탭 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    # write_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    write_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nspecies\tisland\tbill_length_mm\tflipper_length_mm\tbody_mass_g\tsex\tyear\nAdelie\tDream\t40.6\t187\t3475\tmale\t2009\nAdelie\tTorgersen\t38.6\t191\t3800\tmale\t2007\nAdelie\tBiscoe\t41.1\t188\t4100\tmale\t2008\nGentoo\tBiscoe\t43.3\t209\t4400\tfemale\t2007\nGentoo\tBiscoe\t51.1\t225\t5250\tmale\t2009\nGentoo\tBiscoe\t45\t220\t5050\tmale\t2008\nChinstrap\tDream\t49.7\t195\t3600\tmale\t2008\nChinstrap\tDream\t55.8\t207\t4000\tmale\t2009\nChinstrap\tDream\t51.7\t194\t3775\tmale\t2007\nCSV 구분자\nCSV(Comma-Separated Values) 파일은 콤마 구분자를 사용하여 데이터를 저장하는 형식으로 모든 운영체제에서 특별한 별도 프로그램없이 열어볼 수 있다는 장점이 있어 호환성에서 큰 장점이 있지만 파일에 많은 정보가 담기게 되면 파일크기가 커져서 저장공간을 많이 차지한다는 단점이 있다. write_csv() 함수를 사용하여 콤마 구분자 아스키 파일로 저장하는 방법과 write_delim() 함수를 사용하는 방법이 있다. 콤마 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    write_csv(\"data/file/nine_penguins.csv\")\n\nspecies,island,bill_length_mm,flipper_length_mm,body_mass_g,sex,year\nAdelie,Dream,40.6,187,3475,male,2009\nAdelie,Torgersen,38.6,191,3800,male,2007\nAdelie,Biscoe,41.1,188,4100,male,2008\nGentoo,Biscoe,43.3,209,4400,female,2007\nGentoo,Biscoe,51.1,225,5250,male,2009\nGentoo,Biscoe,45,220,5050,male,2008\nChinstrap,Dream,49.7,195,3600,male,2008\nChinstrap,Dream,55.8,207,4000,male,2009\nChinstrap,Dream,51.7,194,3775,male,2007\n고정길이 파일\n고정길이 아스키 파일(Fixed-width ASCII file, FWF)은 데이터 저장 및 교환을 위해 초기 컴퓨팅 시대에 개발되었다. 당시에는 데이터 저장 공간이 제한적이었기 때문에 고정길이 파일은 구분자를 사용하지 않고 데이터를 더 촘촘하게 저장할 수 있었고, 하드웨어와 소프트웨어도 고정 길이 레코드 처리에 최적화되어 있었다.\n현재까지도 고정길이 파일은 레거시 시스템과의 호환성, 데이터 무결성 유지, 데이터 밀도 향상, 대용량 데이터 처리 성능 개선 등의 이유로 명맥을 유지하고 있으며, 의료 및 금융 분야에서 고정길이 파일을 데이터 교환 표준으로 활용하기도 한다.\n하지만, 고정길이 파일은 파일 구조를 이해하기 위해 별도 문서나 스키마 정의가 필요하고, 데이터 추가나 수정 시 레코드 길이 조정이 요구되는 단점이 크고, 구분자로 구분되는 구조화된 데이터 형식과 비교하면 사용 편의성이 크게 떨어진다.\nAdelie    Dream     37.6          181            3300     female   2007\nAdelie    Biscoe    35.3          187            3800     female   2007\nAdelie    Biscoe    37.8          174            3400     female   2007\nGentoo    Biscoe    47.4          212            4725     female   2009\nGentoo    Biscoe    49.1          220            5150     female   2008\nGentoo    Biscoe    47.5          209            4600     female   2008\nChinstrap Dream     40.9          187            3200     female   2008\nChinstrap Dream     47.6          195            3850     female   2008\nChinstrap Dream     46            195            4150     female   2007\n\n24.5.2 불러오기\n탭 구분자\nread_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일을 불러오는 방법과 read_tsv()` 함수를 사용하는 방법이 있다. 탭 구분자 파일을 불러오는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins &lt;- \n    # read_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    read_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nCSV 구분자\n고정길이 파일\nreadr 패키지 read_fwf() 함수를 사용하여 고정길이 파일을 불러읽어오는 방식에서 fwf_widths 인자로 각 열의 길이를 지정하고 col_names 인자로 열 이름을 지정한다.\n\nnine_penguins_fwf &lt;-read_fwf(\"data/file/nine_penguins.fwf\",\n                             skip = 0,\n         col_positions = fwf_widths(c(10, 10, 14, 15, 9, 9, 5),\n           col_names = c(\"species\", \"island\", \"bill_length_mm\",\n                         \"flipper_length_mm\", \"body_mass_g\", \"sex\", \"year\")))\n\nnine_penguins_fwf\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Dream            37.6               181        3300 female  2007\n#&gt; 2 Adelie    Biscoe           35.3               187        3800 female  2007\n#&gt; 3 Adelie    Biscoe           37.8               174        3400 female  2007\n#&gt; 4 Gentoo    Biscoe           47.4               212        4725 female  2009\n#&gt; 5 Gentoo    Biscoe           49.1               220        5150 female  2008\n#&gt; 6 Gentoo    Biscoe           47.5               209        4600 female  2008\n#&gt; 7 Chinstrap Dream            40.9               187        3200 female  2008\n#&gt; 8 Chinstrap Dream            47.6               195        3850 female  2008\n#&gt; 9 Chinstrap Dream            46                 195        4150 female  2007",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다수-파일",
    "href": "ingest_file.html#다수-파일",
    "title": "24  파일 데이터",
    "section": "\n24.7 다수 파일",
    "text": "24.7 다수 파일\n다수 파일을 불러오는 경우를 상정하기 위해서 먼저 앞서 준비한 nine_penguins 데이터프레임을 재사용한다. split() 함수로 species 열을 기준으로 nine_penguins를 분할하여 분할된 데이터를 리스트 형태로 penguins_split에 저장한다. here() 함수로 data/file/ 폴더의 경로를 data_folder 변수에 저장한다. walk2() 함수로 penguins_split 리스트의 각 요소와 해당 요소의 이름을 순회하면서 write_csv() 함수를 사용해 각 분할된 데이터프레임을 CSV 파일로 저장한다. 코드 실행 결과, 프로젝트 디렉토리 내의 data/file/ 폴더에 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일이 생성되며, 각 파일에는 해당 펭귄 종 3마리 관측점 데이터가 저장된다.\n\n# species로 데이터 분할\npenguins_split &lt;- split(nine_penguins, nine_penguins$species)\n\n# 분할된 데이터를 CSV 파일로 저장\ndata_folder &lt;- here::here(\"data\", \"file\")\nwalk2(penguins_split, str_glue(\"penguin_{names(penguins_split)}\"), ~ write_csv(.x, here::here(data_folder, str_c(.y, \".csv\"))))\n\nlist.files() 함수로 data/file 폴더에 penguin으로 시작하는 .csv 파일 3개를 확인할 수 있다.\n\nlist.files(data_folder, pattern = \"^penguin\")\n#&gt; [1] \"penguin_Adelie.csv\"    \"penguin_Chinstrap.csv\" \"penguin_Gentoo.csv\"\n\n이제 데이터가 준비되었으니 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 만들어보자. 동일한 자료구조를 갖는 아스키 파일은 시도, 시군구 데이터처럼 공간적으로 관리를 위해 구분되거나 일, 월, 분기, 년 처럼 시점을 달리하는 경우 관리 목적으로 구분되어 흔히 접하게 되는 데이터다.\ndata_folder 변수에 CSV 파일들이 저장된 폴더 경로를 지정한다. list.files() 함수로 data_folder 내의 모든 CSV 파일 경로를 csv_files 변수에 저장한다. map_df() 함수로 csv_files의 각 파일 경로에 대해 read_csv() 함수를 적용하여 CSV 파일을 읽어와 읽어온 데이터프레임으로 결합한다. 코드 실행 결과, data/file/ 폴더에 있는 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 결합한 penguins_tbl이 생성되며, 총 9 마리 펭귄 3종의 데이터가 포함되어 있다.\n\n# CSV 파일 경로 지정\ndata_folder &lt;- \"data/file/\"\ncsv_files &lt;- list.files(data_folder, pattern = \"^penguin\", full.names = TRUE)\n\n# CSV 파일들을 읽어와 데이터프레임 결합\npenguins_tbl &lt;- purrr::map_df(csv_files, read_csv)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island    bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Dream               40.6               187        3475 male    2009\n#&gt; 2 Adelie    Torgersen           38.6               191        3800 male    2007\n#&gt; 3 Adelie    Biscoe              41.1               188        4100 male    2008\n#&gt; 4 Chinstrap Dream               49.7               195        3600 male    2008\n#&gt; 5 Chinstrap Dream               55.8               207        4000 male    2009\n#&gt; 6 Chinstrap Dream               51.7               194        3775 male    2007\n#&gt; 7 Gentoo    Biscoe              43.3               209        4400 female  2007\n#&gt; 8 Gentoo    Biscoe              51.1               225        5250 male    2009\n#&gt; 9 Gentoo    Biscoe              45                 220        5050 male    2008",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html",
    "href": "ingest_spreadsheet.html",
    "title": "26  상용 데이터",
    "section": "",
    "text": "26.1 통계패키지\nSPSS, SAS, STATA는 널리 사용되는 통계 분석 소프트웨어 패키지로, 각각 고유한 파일 형식을 사용한다. 고유한 파일 형식을 갖게 되면 데이터가 통계 패키지 내부에서 원활히 동작할 수 있는 메타 정보를 담을 수 있고 속도 향상도 기대할 수 있다. 그러나 이러한 독점적인 파일 형식은 다른 통계 패키지와의 상호 운용성을 제한할 수 있고, 장기적으로 데이터 보존 및 이식성에 문제를 일으킬 수 있다.\nSPSS는 .sav 확장자를 사용하는 이진 파일 형식을 사용한다. .sav 파일은 데이터, 변수 레이블, 값 레이블 등의 메타데이터를 포함하고 있다. SPSS .por 확장자를 가진 파일은 다른 시스템으로 이식도 가능하다.\nSAS는 .sas7bdat 확장자를 사용하는 이진 파일 형식을 사용한다. .sas7bdat 파일은 데이터와 메타데이터를 모두 포함하며, SAS에서만 읽을 수 있다. SAS도 SPSS .por처럼 .xpt 확장자를 가진 다른 시스템에 이식 가능한 파일 형식도 지원한다.\nSTATA는 .dta 확장자를 사용하는 이진 파일 형식을 사용한다. .dta 파일에는 데이터, 변수 레이블, 값 레이블 등 메타데이터가 포함되어 있다. .dta 파일은 STATA에서만 읽을 수 있고 SAS, SPSS에서 읽을 수는 없다 하지만, ‘SAS STATA Transfer’ 프로시저를 ’SPSS Data Access Pack’을 구매하여 STATA 파일을 불러읽을 수 있으며, STATA에서 CSV 파일 형태로 내보낸 후 별도 프로시저나 팩없이 SPSS, SAS에서 불러읽을 수 있는 방법이 있다.\n하지만, 통계 패키지 간에 데이터를 교환하려면 일반적으로 .csv(쉼표로 분리된 값) 또는 .txt(탭으로 분리된 값) 형식과 같은 중간 파일 형식을 사용하는 과정에서 변수 레이블과 값 레이블과 같은 일부 메타데이터가 손실될 수 있다.\n따라서, 단기적으로 SAS/SPSS/STATA와 같은 독점 파일 형식이 제공하는 장점보다 개방형 파일 형식이 장기적으로 데이터 접근성과 재사용성을 높일 수 있다는 면에서 장점이 크다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#통계패키지",
    "href": "ingest_spreadsheet.html#통계패키지",
    "title": "26  상용 데이터",
    "section": "",
    "text": "26.1.1 SPSS\n세종시에 위치한 한국보건사회연구원에서 조사하여 발표하는 한국복지패널데이터는 특이하게도 오픈 파일 형식만 제외하고 상용 통계 패키지가 있어야 열어볼 수 있는 SPSS, STATA, SAS 파일 형식으로 제공되고 있다. 총4가지 종류 파일을 제공하고 있지만 여기서는 다양한 파일 데이터를 불러오는 방법을 중심으로 살펴보기 때문에 가장 단순한 파일만 R 환경으로 불러오는 방법을 살펴보자.\n\n가구용데이터(SAS, SPSS, STATA):koweps_h17_2022_Beta1\n가구원용데이터(SAS, SPSS, STATA):koweps_p17_2022_Beta1\n복지인식설문용데이터(SAS, SPSS, STATA):koweps_wc17_2022_Beta1\n가구용, 가구원용, 복지인식설문용 머지데이터(SAS, SPSS, STATA):koweps_hpwc17_2022_Beta1\n\nSPSS 로 작성된 .sav 파일으로 R 환경으로 불러오기 위해서는 haven 패키지를 로드하여 SPSS (.sav) 데이터 파일을 R로 읽어온다. read_spss() 함수를 사용하여 “koweps_hpwc17_2022_Beta1.sav” 파일을 welfare_raw 데이터 프레임으로 저장한 후, map_chr() 함수를 사용하여 welfare_raw의 각 변수에 대해 attributes(.x)$label을 적용하여 변수의 레이블을 추출하고 후속 작업을 위해서 문자형 벡터로 변환시킨다.\nenframe() 함수를 사용하여 추출된 레이블을 데이터 프레임으로 변환하고, filter() 함수와 str_detect() 함수를 사용하여 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”라는 키워드가 포함된 변수만 선택한다. pull() 함수를 사용하여 선택된 변수의 이름을 추출하고, setdiff() 함수를 사용하여 정규표현식 작성과정에서 함께 추출된”h1707_6aq6” 변수를 제외시킨 후 demo_vars 변수로 저장한다.\nwelfare_raw 데이터 프레임에서 select() 함수와 all_of() 함수를 사용하여 demo_vars에 해당하는 변수만 선택한 후 set_names() 함수를 사용하여 선택된 변수명을 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”로 변경한다. str_split()과 dput()을 사용하여 변수 이름을 파이프(|)로 연산으로 한 명령어로 처리한다. janitor 패키지 clean_names() 함수를 사용하여 변수 이름을 깔끔하게 정리하는데, ascii = FALSE 옵션을 사용하여 한글 변수명을 유지한다.\n한국보건사회연구원에서 한국복지패널 데이터가 SPSS로 제공되고 있지만 상용 SPSS 패키지가 없더라도 R 환경에서 haven 패키지와 janitor 패키지를 활용하여 SPSS 데이터를 불러와서 본격적인 분석을 오픈 데이터 분석 및 통계 언어 R로 수행할 준비가 되었다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\n# Read the .sav file\nwelfare_raw &lt;- read_spss(\"data/file/SPSS/koweps_hpwc17_2022_Beta1.sav\")\n\n## 관심 변수 추출\ndemo_vars &lt;- welfare_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt; \n    enframe() |&gt; \n    filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt; \n    pull(name) |&gt; \n    setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nwelfare_raw %&gt;%\n  select(all_of(demo_vars)) |&gt; \n    set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt; \n    janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n26.1.2 SAS\nSAS 통계패키지 koweps_hpwc17_2022_beta1.sas7bdat 파일을 작성된 동일한 한국보건사회연구원에서 한국복지패널 데이터도 haven 패키지를 사용하여 read_sas() 함수를 사용하여 SAS 데이터 파일(.sas7bdat)을 불러온다. 이후 코드는 앞서 SPSS 데이터를 R 인구통계 데이터프레임으로 변환시켜 가져온 것과 동일한 방법으로 진행된다. 즉, 코드를 재사용하게 된다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\nsas_raw &lt;- read_sas(\"data/file/SAS/koweps_hpwc17_2022_Beta1.sas7bdat\")\n\n## 관심 변수 추출\nsas_vars &lt;- sas_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nsas_raw %&gt;%\n  select(all_of(sas_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n26.1.3 STATA\nSTATA 통계패키지 koweps_hpwc17_2022_beta1.dta 파일은 SAS 버전과 동일한 한국복지패널 데이터다. R에서 haven 패키지 read_dta() 함수를 사용하여 STATA 데이터 파일(.dta)을 불러올 수 있다. 이후 코드는 앞서 SPSS, SAS 데이터를 R로 가져와 인구통계 데이터프레임으로 변환한 것과 동일한 방법으로 진행된다. 따라서 이전에 작성한 코드를 그대로 재사용할 수 있다.\n\nlibrary(haven) # install.packages(\"haven\")\n\n# STATA 파일 불러오기\nstata_raw &lt;- read_dta(\"data/file/STATA/Koweps_hpwc17_2022_beta1.dta\")\n\n## 관심 변수 추출\nstata_vars &lt;- stata_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nstata_raw %&gt;%\n  select(all_of(stata_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#엑셀",
    "href": "ingest_spreadsheet.html#엑셀",
    "title": "26  상용 데이터",
    "section": "\n26.2 엑셀",
    "text": "26.2 엑셀",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  }
]
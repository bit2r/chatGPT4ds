[
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n20  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n21 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "21  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "22  챗GPT 자연어",
    "section": "",
    "text": "23 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n24 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25 Notable.ai\n\n\n26 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 자연어</span>"
    ]
  },
  {
    "objectID": "simpson.html",
    "href": "simpson.html",
    "title": "23  심슨의 역설",
    "section": "",
    "text": "24 심슨의 역설 사례\n심슨의 역설 사례를 책페이지수와 책가격의 관계를 살펴보자. 데이터는 책 유형(하드커버, 페이퍼백)은 두가지가 있고, 페이지수와 책가격이 달러로 구성된 데이터프레임이다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA",
    "href": "simpson.html#simpson-paradox-case-study-EDA",
    "title": "23  심슨의 역설",
    "section": "\n24.1 데이터 시각화",
    "text": "24.1 데이터 시각화\n이를 시각적으로 표현하면 관계가 음의 상관관계를 갖는 것을 알 수 있다.\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(extrafont)\nloadfonts()\n\nsimp_df &lt;- tribble(\n    ~book_type, ~num_pages, ~book_price,\n    \"hardcover\", 150, 27.43, \n    \"hardcover\", 225, 48.76, \n    \"hardcover\", 342, 50.25, \n    \"hardcover\", 185, 32.01, \n    \"paperback\", 475, 10.00, \n    \"paperback\", 834, 15.73, \n    \"paperback\", 1020, 20.00, \n    \"paperback\", 790, 17.89)\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE)",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "title": "23  심슨의 역설",
    "section": "\n24.2 기술통계량",
    "text": "24.2 기술통계량\nnum_pages, book_price 두변수를 추출하여 상관계수를 도출한다. 그리고 나서, 책 유형에 따른 상관관계도 도출해 낸다. 먼저 책 유형에 관계없이 num_pages, book_price 상관관계는 -0.5949366으로 나름 강한 음의 상관계수가 관측된다.\n\nsimp_df %&gt;% \n    summarise(book_cor = cor(num_pages, book_price)) %&gt;% \n    pull()\n#&gt; [1] -0.5949366\n\n이번에는 책 유형에 따른 상관계수는 어떤지 계산해 보자. 이 경우, 하드커버는 0.848, 페이퍼백은 0.956로 강한 양의 상관관계가 존재함이 확인된다.\n\nsimp_df %&gt;% \n    group_by(book_type) %&gt;% \n    summarise(book_cor = cor(num_pages, book_price))\n#&gt; # A tibble: 2 × 2\n#&gt;   book_type book_cor\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 hardcover    0.848\n#&gt; 2 paperback    0.956",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "title": "23  심슨의 역설",
    "section": "\n24.3 상관관계 시각화",
    "text": "24.3 상관관계 시각화\n앞서 확인한 결과를 책 유형별로 나눠 상관계수를 시각화한다.\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price, color=book_type)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"책페이지 수\", y=\"책가격($)\", title=\"심슨의 역설 사례\", color=\"책유형\" )+\n      theme(legend.position = \"top\")",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-berkeley",
    "href": "simpson.html#simpson-paradox-berkeley",
    "title": "23  심슨의 역설",
    "section": "\n25.1 UC 버클리 입학 2\n",
    "text": "25.1 UC 버클리 입학 2\n\n심슨의 역설관련 가장 유명한 사례는 1973년 UC 버클리 대학 입학데이터로 입학에 성차별이 존재하는지에 관한 데이터다.\n\n\n성별에 따른 입학률 비교\n\nlibrary(datasets)\nadmin_df &lt;- UCBAdmissions %&gt;% tbl_df\n\nadmin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n복사하여 붙여넣기\n\nadmin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n기술통계량을 통해 살펴본 사항을 그래프로 시각화한다. 막대 그래프를 통해 남성 합격률이 여성보다 높은 것으로 나타나 성차별이 존재하는 것으로 파악되지만, 학과별로 놓고 보면 여성 합격률이 더 높거나 남성과 유사한 것으로 시각적으로 나타난다.\n\n# A barplot for overall admission percentage for each gender.\n\nadmit_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, width = 0.2, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"성별\", y=\"입학합격율\", title=\"버클리 전체 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_minimal(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") \n\nadmit_dept_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    facet_grid(. ~ Dept) +\n    labs(x=\"성별\", y=\"\", title=\"버클리 학과별 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          legend.position = \"none\") \n\nplot_grid(admit_g, admit_dept_g, labels = \"\")",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-news-article-game-case",
    "href": "simpson.html#simpson-paradox-news-article-game-case",
    "title": "23  심슨의 역설",
    "section": "\n26.1 게임 업데이터 사례 3\n",
    "text": "26.1 게임 업데이터 사례 3\n\n예전에 모 게임에서 큰 규모의 업데이트를 한 후 게임 고객 동향을 분석한 적이 있습니다. 이 게임은 전체 게임 고객을 약 십 여가지 유형으로 분류하고 있는데, 크게 보면 게임 활동이 왕성하고 충성도가 높은 ‘진성’ 유형, 게임 활동이 그리 활발하지 않은 ‘라이트’ 유형, 자동 사냥 유저로 의심되는 ‘봇’ 유형 등이 있죠.\n이 게임의 업데이트 전/후 일별접속자수(DAU)와 유저당 결제금액(ARPU) 지표를 확인해 보니 아래와 같이 나왔습니다.\n\n일별 접속자수(DAU)가 크게 늘었지만 유저당 결재금액(ARPU)가 하락하여 뭔가 특단의 조치가 필요한 것으로 파악되지만, 이를 고객 집단을 반영하여 분석을 하게 되면 진성유저는 큰 차이가 없고, 크게 늘어난 유저가 봇이거나 Non-PU 유저라 봇을 비용으로 간주하여 제거하거나 Non-PU유저를 PU로 바꾸거나 PU 유저의 결재금액을 높이는 방향으로 사업적인 조치를 취하는 것이 바람직스러워 보인다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#footnotes",
    "href": "simpson.html#footnotes",
    "title": "23  심슨의 역설",
    "section": "",
    "text": "나무위키, “심슨의 역설”↩︎\nJohnny Hong(January 30, 2016), “A (very) brief introduction to ggplot2”↩︎\nNC소프트 (2017-07-05) “데이터 분석을 이용한 게임 고객 모델링 #4”↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "basic_stat.html",
    "href": "basic_stat.html",
    "title": "\n24  통계\n",
    "section": "",
    "text": "24.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#기술통계",
    "href": "basic_stat.html#기술통계",
    "title": "\n24  통계\n",
    "section": "\n24.2 기술통계",
    "text": "24.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n\n\n\n\n24.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n#&gt; # A tibble: 1 × 2\n#&gt;   평균_체중 최빈종\n#&gt;       &lt;dbl&gt; &lt;fct&gt; \n#&gt; 1     4202. Adelie\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\nprint(f'펭귄 최빈종: {mode_species}')\n\n\n\n\n\n24.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n#&gt; # A tibble: 1 × 2\n#&gt;   분산_체중 표준편차_체중\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1   643131.          802.\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n#&gt; # A tibble: 3 × 2\n#&gt;   species   빈도수\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 Adelie       152\n#&gt; 2 Gentoo       124\n#&gt; 3 Chinstrap     68\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#가능성",
    "href": "basic_stat.html#가능성",
    "title": "\n24  통계\n",
    "section": "\n24.3 가능성",
    "text": "24.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n24.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n#&gt; [1] \"부산\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n#&gt;  [1] \"울산\" \"충북\" \"강원\" \"강원\" \"제주\" \"광주\" \"전남\" \"전남\" \"울산\" \"충북\"\n#&gt; [11] \"서울\" \"충북\" \"인천\" \"인천\" \"경기\" \"충남\" \"울산\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n#&gt; [1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n#&gt; [1] 0.0564\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n#&gt; 대전\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n#&gt; 경기\n#&gt; 제주\n#&gt; 대전\n#&gt; 경남\n#&gt; 서울\n#&gt; 울산\n#&gt; 충북\n#&gt; 대구\n#&gt; 강원\n#&gt; 울산\n#&gt; 강원\n#&gt; 경기\n#&gt; 전북\n#&gt; 서울\n#&gt; 서울\n#&gt; 제주\n#&gt; 충북\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n#&gt; 0.058823529411764705\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n#&gt; 0.0604\n\n\n\n\n\n24.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}\nPr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\\n                   &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)\n\\end{aligned}\\)\n넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n#&gt; [1] 0.1367713\n\nmean(서건창 | 이정후)\n#&gt; [1] 0.5852018\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n#&gt; [1] 0.5852018\n\n두 선수가 동시에 안타를 칠 확률은 0.14이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.59이 된다.\n200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n#&gt; [1] 0.11626\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n#&gt; [1] 0.1189903\n\n\n24.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\nR 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n#&gt; [1] 5.0053\nmean(binom_df$y)\n#&gt; [1] 9.9718\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n#&gt; # A tibble: 1 × 1\n#&gt;   mean_z\n#&gt;    &lt;dbl&gt;\n#&gt; 1   15.0",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#분포",
    "href": "basic_stat.html#분포",
    "title": "\n24  통계\n",
    "section": "\n24.4 분포",
    "text": "24.4 분포",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#footnotes",
    "href": "basic_stat.html#footnotes",
    "title": "\n24  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "25.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n#&gt; # A tibble: 1 × 2\n#&gt;   amount_est amount_var\n#&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1       40.9       35.7\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#밀린_병원비_추정",
    "href": "sampling.html#밀린_병원비_추정",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\n\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept",
    "href": "sampling.html#basic-concept",
    "title": "\n25  표본 추출\n",
    "section": "\n25.2 표본추출",
    "text": "25.2 표본추출\n\n25.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n#&gt; \n#&gt;  Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n#&gt; Rows: 1,338\n#&gt; Columns: 8\n#&gt; $ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n#&gt; $ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n#&gt; $ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n#&gt; $ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n#&gt; $ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n#&gt; $ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n#&gt; $ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n#&gt; $ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n25.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n#&gt; # A tibble: 10 × 8\n#&gt;    total_cup_points species coo          farm_name aroma  body balance sweetness\n#&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1             83.6 Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.67        10\n#&gt;  2             82.4 Arabica China        menglian…  7.67  7.42    7.42        10\n#&gt;  3             82.8 Arabica Guatemala    linda vi…  7.58  7.5     7.42        10\n#&gt;  4             85   Arabica Uganda       mount el…  8.17  7.67    7.75        10\n#&gt;  5             83   Arabica Colombia     &lt;NA&gt;       7.5   7.58    7.58        10\n#&gt;  6             84.5 Arabica Guatemala    nueva gr…  7.67  7.5     7.83        10\n#&gt;  7             81   Arabica Guatemala    chapulte…  7.42  7.25    7.17        10\n#&gt;  8             81.3 Arabica Brazil       sertao f…  7.17  7.42    7.33        10\n#&gt;  9             81.7 Arabica Tanzania, U… multiple   7.42  7.5     7.42        10\n#&gt; 10             80.1 Arabica Mexico       &lt;NA&gt;       7.25  7.33    7.17        10\n\n\n25.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n#&gt; # A tibble: 10 × 9\n#&gt;    rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n#&gt;    &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n#&gt;  2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n#&gt;  3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n#&gt;  4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n#&gt;  5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n#&gt;  6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n#&gt;  7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n#&gt;  8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n#&gt;  9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n#&gt; 10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n#&gt; # A tibble: 3 × 9\n#&gt;   rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n#&gt;   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1   446             82.5 Arabica Colomb… &lt;NA&gt;       7.67  7.25    7.67        10\n#&gt; 2   892             83.7 Arabica Colomb… various    7.83  7.58    7.67        10\n#&gt; 3  1338             85.9 Arabica Guatem… la esper…  7.92  8.08    7.83        10\n\n\n25.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n#&gt; # A tibble: 96 × 8\n#&gt; # Groups:   coo [37]\n#&gt;    total_cup_points species coo      farm_name     aroma  body balance sweetness\n#&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1             81   Arabica Brazil   cachoeira da…  7.33  7.33    7.17        10\n#&gt;  2             82.3 Arabica Brazil   rio verde      8.25  8.17    8.08        10\n#&gt;  3             82.8 Arabica Brazil   capoeirinha    7.67  7.5     7.42        10\n#&gt;  4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n#&gt;  5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n#&gt;  6             85.3 Arabica China    pu'er city l…  8     7.83    7.75        10\n#&gt;  7             82.4 Arabica China    menglian gao…  7.67  7.42    7.42        10\n#&gt;  8             82.3 Arabica China    yun lan coff…  7.5   7.42    7.42        10\n#&gt;  9             83   Arabica Colombia &lt;NA&gt;           7.75  7.58    7.17        10\n#&gt; 10             82.8 Arabica Colombia &lt;NA&gt;           7.67  7.5     7.42        10\n#&gt; # ℹ 86 more rows\n\n\n25.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n#&gt; # A tibble: 5 × 8\n#&gt;   total_cup_points species coo           farm_name aroma  body balance sweetness\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1             79.7 Arabica El Salvador   la monta…  7.25  7.33    7.17        10\n#&gt; 2             82.8 Arabica El Salvador   several …  7.75  7.25    7.5         10\n#&gt; 3             83.1 Arabica United State… &lt;NA&gt;       7.33  7.83    7.67        10\n#&gt; 4             79.9 Arabica United State… &lt;NA&gt;       7.5   7.33    7.17        10\n#&gt; 5             83.1 Arabica United State… &lt;NA&gt;       7.33  7.67    7.58        10",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept-comparison",
    "href": "sampling.html#basic-concept-comparison",
    "title": "\n25  표본 추출\n",
    "section": "\n25.3 표본추출 비교",
    "text": "25.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n25.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n#&gt; [1] 82.1512\n\n\n25.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n#&gt; [1] 81.87774\n\n\n25.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n#&gt; [1] 82.12846\n\n\n25.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n#&gt; [1] 80.94727",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-errors",
    "href": "sampling.html#calculate-errors",
    "title": "\n25  표본 추출\n",
    "section": "\n25.4 오차 측정",
    "text": "25.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n#&gt; # A tibble: 1 × 4\n#&gt;   population   srs stratifed cluster\n#&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1       82.2  81.9      82.1    80.9\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n#&gt; # A tibble: 4 × 3\n#&gt;   method     estimation relative_error\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 population       82.2         0     \n#&gt; 2 srs              81.9         0.333 \n#&gt; 3 stratifed        82.1         0.0277\n#&gt; 4 cluster          80.9         1.47\n\n\n25.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n#&gt; [1] 82.32256\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n#&gt;   [1] 82.03361 82.14602 82.11233 81.83947 82.07414 81.94902 81.80143 81.90098\n#&gt;   [9] 81.99541 82.24323 82.43143 82.41759 81.69571 82.00797 82.00526 81.90113\n#&gt;  [17] 82.24707 81.68549 82.28677 82.23579 81.96248 82.52338 81.90534 82.49910\n#&gt;  [25] 81.82579 82.08203 81.97203 82.25195 82.22038 81.97699 82.15985 81.78368\n#&gt;  [33] 82.15789 82.17278 82.16962 82.47256 82.11534 81.97414 82.28586 82.21617\n#&gt;  [41] 82.10850 82.54887 82.42053 82.18692 82.09436 82.38504 82.11684 82.18120\n#&gt;  [49] 82.57195 82.44391 82.59729 82.10820 82.23030 82.40271 81.97857 82.14241\n#&gt;  [57] 82.21609 82.25406 82.30812 82.00030 82.22602 82.15564 82.29271 82.34549\n#&gt;  [65] 82.01308 82.48278 82.38406 82.48519 82.39865 82.18729 81.91211 81.73023\n#&gt;  [73] 82.14977 82.28135 82.00143 82.23504 82.35925 82.44692 82.30684 82.56346\n#&gt;  [81] 82.20579 82.49729 81.72429 82.12211 82.05647 81.91316 82.26256 81.98496\n#&gt;  [89] 81.92970 82.53451 82.07677 81.90647 81.94060 82.39353 82.15090 82.39474\n#&gt;  [97] 82.32789 82.35714 82.54474 81.95391\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n#&gt;   [1] 82.09045 82.01361 82.38414 82.51338 82.12481 82.58940 82.32534 81.82218\n#&gt;   [9] 82.11263 82.01098 82.16654 81.96639 82.23925 81.58594 82.19120 82.12256\n#&gt;  [17] 82.44511 81.85504 82.34820 81.66624 82.16902 82.07692 82.38391 81.99406\n#&gt;  [25] 82.27669 82.28865 82.06308 81.81887 82.06180 82.06143 82.20338 82.45632\n#&gt;  [33] 81.93301 82.08624 82.31977 81.89647 81.93992 82.35820 82.04323 82.22669\n#&gt;  [41] 82.37391 82.07248 82.36248 81.63023 81.90624 82.07173 82.56195 82.52293\n#&gt;  [49] 82.27774 81.94481 82.22218 81.81556 82.13609 82.24947 81.99714 81.98444\n#&gt;  [57] 82.18331 81.60278 82.27361 82.09353 82.16436 82.28323 81.50504 81.91308\n#&gt;  [65] 82.43887 82.11609 82.31233 81.85932 82.47692 82.07489 82.26737 82.20188\n#&gt;  [73] 81.76316 82.08150 81.70398 82.05451 82.03985 82.42947 82.31060 82.08729\n#&gt;  [81] 82.11188 82.21323 82.00180 82.37714 82.51361 81.81910 82.41511 81.99812\n#&gt;  [89] 82.02872 82.00669 82.30880 82.47556 81.36549 82.21316 82.47241 81.86195\n#&gt;  [97] 81.71647 82.22053 82.17120 82.22489\n\n\n25.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n25.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n#&gt; # A tibble: 5 × 3\n#&gt;   samp_prop mean_cup_points sd_cup_points\n#&gt;   &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 10 %                 82.1        0.239 \n#&gt; 2 33 %                 82.2        0.0997\n#&gt; 3 50 %                 82.2        0.0677\n#&gt; 4 75 %                 82.2        0.0473\n#&gt; 5 90 %                 82.2        0.0263",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-bootstrap",
    "href": "sampling.html#calculate-bootstrap",
    "title": "\n25  표본 추출\n",
    "section": "\n25.5 신뢰구간",
    "text": "25.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n25.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n#&gt; [1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n#&gt; [1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n#&gt; [1] 2.864368\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n#&gt; [1] 2.488636\n\n\n25.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n#&gt; # A tibble: 1 × 3\n#&gt;   lower  mean upper\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  82.0  82.4  82.8",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#footnotes",
    "href": "sampling.html#footnotes",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "26.1 통계적 가설검정\n통계적 가설 검정(統計的假說檢定, statistical hypothesis test)은 통계적 추측의 하나로서, 모집단 실제의 값이 얼마가 된다는 주장과 관련해, 표본의 정보를 사용해서 가설의 합당성 여부를 판정하는 과정을 의미하는데 이를 위해서 프로세스(Process)와 함께 검정 통계량을 수식으로 나타낼 수 있어야 하고 이를 해석하는 별도의 훈련도 받아야 했고 이 과정에서 상당량의 수학 및 통계학적 지식이 요구된다. 2\n통계적 가설검정(Statistical Testing)은 기존 통계학 전공자의 전유물이었으나, 컴퓨터의 일반화와 누구나 코딩을 할 수 있는 현재(2024-03-17)는 더 이상 기존 통념이 통용되지는 않게 되었다. 특히 파이썬 진영에서 이런 움직임이 활발하다. 그렇다고 R 진영에서도 기존의 방식을 고수하는 것은 아니다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference",
    "href": "hypothesis.html#computer-age-statistical-inference",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "유의수준의 결정, 귀무가설과 대립가설 설정\n검정통계량의 설정 (예를 들어, t-검정)\n\n\\(t_{검정통계량} \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\)\n자유도: \\(\\nu \\quad  \\approx \\quad {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\\)\n\n\n\n기각역의 설정\n검정통계량 계산\n통계적인 의사결정",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-tidyverse-inference",
    "href": "hypothesis.html#computer-age-tidyverse-inference",
    "title": "26  코딩 가설검정",
    "section": "\n26.2 tidyverse 가설검정",
    "text": "26.2 tidyverse 가설검정\n데이터 과학을 이끌어 나가는 있는 R과 파이썬 진영의 현재 주도적인 흐름을 살펴보자. 우선 다소 차이가 있지만, for 반복루프를 이해하고 이를 코드로 구현할 수만 있다면 컴퓨터를 활용한 가설검정이 가능한 것은 사실이다. 하지만, 2011년 Allen Downey 교수가 주장했던 것처럼 오랜동안 검정된 해석학적 방법(Analytic Method)에 대한 교차검정하는 방식으로 활용하는 것이 추천된다. 3\n\n\nR과 파이썬 검정 가설검정 프레임워크 비교\n\n코딩기반 가설검정은 우선 데이터로부터 시작된다. 데이터를 컴퓨터의 기능을 활용하여 모의실험 표본을 생성하고 나서 귀무가설(\\(H_0\\)) 모형에서 검정통계량을 추출하여 이를 바탕으로 \\(p-값\\)을 계산하여 의사결정을 추진한다.\n통계검정에도 tidyverse를 반영하고 Allen Downey 교수가 주창한 통계검정 프레임워크를 도입하여 극단적으로 말하며 딥러닝 모형이 거의 모든 통계, 기계학습 모형을 통일해 나가듯이 다양한 통계검정에 대해서도 비숫한 위치를 점할 것으로 예측된다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "href": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "title": "26  코딩 가설검정",
    "section": "\n26.3 가설검정과 신뢰구간",
    "text": "26.3 가설검정과 신뢰구간\ninfer 팩키지는 tidyverse 철학(?)에 따라 가설검정과 신뢰구간을 추정하는 목적으로 개발되었다. 크게 통계적 추론은 가설검정과 신뢰구간 추정이 주된 작업이다. 이를 위해서 5가지 동사(verb)를 새로 익혀야 한다.\n\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nvisualize()\n\n\n\n가설검정과 신뢰구간",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "href": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "title": "26  코딩 가설검정",
    "section": "\n26.4 사례: 맥주와 모기",
    "text": "26.4 사례: 맥주와 모기\n맥주를 마시는 사람이 말라리아 모기에게 매력적으로 보여 더 잘 물리는가? 라는 흥미로운 논문이 발표되었다. (Lefèvre 기타 2010) 이 연구는 맥주를 마신 후 사람의 냄새 (호흡 및 피부 방출 냄새)가 아노펠레스 감비아(Anopheles gambiae, 아프리카 주요 말라리아 매개체)에게 어떤 영향을 미치는지 조사하였다. 맥주를 마신 사람들의 몸 냄새는 모기의 활성화 (이륙 및 상향 풍속 비행에 참여하는 모기의 비율)와 방향성 (사람의 냄새를 향해 비행하는 모기의 비율)을 증가시켰다. 물을 마신 경우에는 사람이 모기에게 끌리는 것에 영향을 미치지 않았다.\n\n26.4.1 가설검정 환경설정\n데이터 전처리와 시각화, 한글설정을 위한 팩키지를 준비한다. 특히 infer 코딩기반 가설검정을 위해 필수적인 팩키지로 활용하는데 기본적인 사용방법은 mtcars, flights 데이터를 활용한 사례를 살펴본다.\n\nflights 데이터 소품문\nmtcars 데이터 소품문\n\n\n# 0. 환경설정 ----------\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(skimr)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(extrafont)\nloadfonts()\n\n# 1. 데이터 가져오기 -----\n\n# beer_dat &lt;- read_csv(\"https://raw.githubusercontent.com/aloy/m107/master/data/mosquitos.csv\")\nbeer_dat &lt;- read_csv(\"data/mosquitos.csv\")\n\nbeer_df &lt;- beer_dat %&gt;% \n    mutate(treatment = factor(treatment, levels = c(\"beer\", \"water\"), labels=c(\"맥주\", \"맹물\"))) \n\n\n26.4.2 탐색적 데이터 분석\n탐색적 데이터 분석을 통해서 말라리아 모기가 맥주를 마신 사람과 맹물을 마신 사람 어디에 더 많이 접근을 하는지 개체수 차이를 살펴본다. 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 우연에 의한 것인지 아니면 맥주가 더 모기에게 섹시하게 반응하는 역할을 하기 때문인지 살펴본다.\n\n# 2. 탐색적 데이터 분석 -----\n## 2.1. 전체 데이터 \nskim(beer_df)\n\n\nData summary\n\n\nName\nbeer_df\n\n\nNumber of rows\n43\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\n맥주: 25, 맹물: 18\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ncount\n0\n1\n21.77\n4.47\n12\n19\n21\n24\n31\n▂▃▇▅▂\n\n\n\n\n## 2.2. 맥주와 맹물 투여 집단 비교\nbeer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(최소 = min(count),\n                분위수_25 = quantile(count, 0.25),\n                평균 = mean(count),\n                중위수 = median(count),\n                분위수_75 = quantile(count, 0.75),\n                표준편차 = sd(count),\n                중위절대편차 = mad(count)) %&gt;% \n    mutate(맥주맹물차이 = max(평균) - min(평균))\n#&gt; # A tibble: 2 × 9\n#&gt;   treatment  최소 분위수_25  평균 중위수 분위수_75 표준편차 중위절대편차\n#&gt;   &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 맥주         17      20    23.6     24        27     4.13         5.93\n#&gt; 2 맹물         12      16.5  19.2     20        22     3.67         2.97\n#&gt; # ℹ 1 more variable: 맥주맹물차이 &lt;dbl&gt;\n\n## 2.3. 맥주와 맹물 투여 집단 비교 시각화\n\nbeer_density_g &lt;- ggplot(data = beer_df, mapping = aes(x = count, fill=treatment)) +\n    geom_density(aes(y = ..count..), alpha = 0.7) +\n    scale_x_continuous(limits=c(5,40)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    labs(title=\"맥주를 마시면 모기에게 섹시하게 보일까라고 쓰고 잘 물릴까라고 읽는다.\",\n        x=\"채집된 모기 개체수\", y=\"빈도수\", fill=\"실험처리(treatment): \")\n\nbeer_boxplot_g &lt;- ggplot(data = beer_df, mapping = aes(x = treatment, y = count, fill=treatment)) +\n    geom_boxplot(alpha = 0.5) +\n    geom_jitter(width = 0.2) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    coord_flip() +\n    theme(legend.position = \"none\") +\n    labs(title=\"\",\n         y=\"채집된 모기 개체수\", x=\"실험처리\", fill=\"실험처리(treatment): \")\n\ngrid.arrange(beer_density_g, beer_boxplot_g, nrow=2)\n\n\n\n\n\n\n\n\n26.4.3 의사결정\n맥주를 마신 집단과 맹물을 마신 집단간에 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 유의적인지 전통적인 t-검정과 코딩기반 모의실험을 통해서 살펴보자.\n전통 t-검정\nt-검정 결과 유의적인 차이가 나타나느 것으로 나타난다. p-값이 무척이나 작게 나온다.\n\n# 3. 통계 검정 -----\n## 3.1. 전통적인 해석적인 방법 (t-검정)\nt.test(count ~ treatment, beer_df, null = 0, var.equal = TRUE, alternative=\"greater\")\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  count by treatment\n#&gt; t = 3.587, df = 41, p-value = 0.0004416\n#&gt; alternative hypothesis: true difference in means between group 맥주 and group 맹물 is greater than 0\n#&gt; 95 percent confidence interval:\n#&gt;  2.323889      Inf\n#&gt; sample estimates:\n#&gt; mean in group 맥주 mean in group 맹물 \n#&gt;           23.60000           19.22222\n\n코딩기반 t-검정\n코딩기반 t-검정은 다음 절차를 통해 준비한다.\n\n코딩기반 t-검정을 수행할 경우 beer_df가 \\(\\delta^*\\)에 해당되어 데이터에서 사전에 계산해 놓는다.\n가설검정 공식을 specify 함수에 명세한다.\n귀무가설을 hypothesize 함수에서 적시한다.\n컴퓨터에서 모의실험 난수를 generate에서 생성시킨다.\n검정 통계량을 calculate 함수에 명시한다.\n\n그리고 나서 p-값, 95% 신뢰구간을 모의실험결과에서 단순히 세어서 정리하면 된다.\n마지막으로 시각적으로 한번 더 확인한다. 즉, 4.38번 더 물리는 것은 극히 드물게 일어나는 사례로 맥주를 마시면 모기에 더 잘 물리게 된다고 볼 수 있다.\n\n## 3.2. `infer` 팩키지 -----\n\n### 3.2.1. 데이터에서 두 집단 간 차이 산출\nbeer_diff &lt;- beer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(mean = mean(count)) %&gt;% \n    summarise(abs(diff(mean))) %&gt;% \n    pull\n\n### 3.2.2. 귀무가설 모형에서 모의실험을 통해서 통계량 산출\nnull_model &lt;- beer_df %&gt;%\n    specify(count ~ treatment) %&gt;%\n    hypothesize(null = \"independence\") %&gt;% \n    generate(reps = 1000, type = \"permute\") %&gt;% \n    calculate(stat = \"diff in means\", order = c(\"맥주\", \"맹물\"))\n\n### 3.2.3. p-갑과 95% 신뢰구간: 백분위수(Percentile) 방법\nnull_model %&gt;%\n    summarize(p_value = mean(stat &gt; beer_diff))\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1       0\n\nnull_model %&gt;%\n    summarize(l = quantile(stat, 0.025),\n              u = quantile(stat, 0.975))\n#&gt; # A tibble: 1 × 2\n#&gt;       l     u\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 -2.88  2.56\n\n### 3.2.4. 시각화\nggplot(null_model, aes(x = stat, fill=\"gray75\")) +\n    geom_density(aes(y=..count..), alpha=0.7) +\n    geom_vline(xintercept = beer_diff, color = \"red\", size=1.5) +\n    scale_x_continuous(limits=c(-5,5)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(title=\"맥주를 마시고 4.38번 더 모기에 물린다면...  \",\n         x=\"맥주와 맹물 개체수 차이\", y=\"빈도수\")\n\n\n\n\n\n\n\n\n\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric Elguero, Didier Fontenille, François Renaud, Carlo Costantini, 와/과 Frédéric Thomas. 2010. “Beer consumption increases human attractiveness to malaria mosquitoes”. PloS one 5 (3): e9546.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#footnotes",
    "href": "hypothesis.html#footnotes",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "Allen Downey (2016), There is still only one test↩︎\n위키 백과 - 가설 검정↩︎\nHadley Wickham(2017-11-13), The tidy tools manifesto↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "NHST.html",
    "href": "NHST.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "R.A. Fisher는 NHST의 토대를 만들었으며 분산분석의 개념과 제한된 표본을 이용해 실험을 설계하는 실험계획법에 큰 기여를 했다. 1925년에 그가 발표한 ’Statistical Methods for Research Workers’라는 책에서 유의성 검정(significance test) 개념이 소개된 것을 확인할 수 있다. (Fisher 1970)\n귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 바탕으로 한 가설검정(hypothesis testing) 개념을 Neyman과 Pearson이 정립했고, 이를 적용한 최초의 사례는 1940년에 발표한 “Statistical Analysis in Educational Research”라는 책에서 NHST(Null Hypothesis Significance Testing) 개념을 처음으로 사용한 것으로 알려져 있다. (Lindquist 1940)\n\n\n\n\n\nflowchart TD\n    A[귀무가설 & 대립가설 설정]:::process\n    B[\"유의수준 선택 (예, 0.05)\"]:::process\n    C[데이터 수집 및 분석]:::process\n    D[\"검정 통계량 계산 (예, t-점수, z-점수)\"]:::process\n    E[검정 통계량을 임계값과 비교]:::decision\n    F[귀무가설 기각 또는 기각하지 않음]:::decision\n    G[\"결과 보고\"]:::report\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    classDef process fill:#efefef,stroke:#333,stroke-width:1px;\n    classDef decision fill:#ffefef,stroke:#333,stroke-width:1px;\n    classDef report fill:#efefff,stroke:#333,stroke-width:1px;\n\n\n\n\n\n\n\n\n\n\nFisher, Ronald Aylmer. 1970. “Statistical methods for research workers”. In Breakthroughs in statistics: Methodology and distribution, 66–70. Springer.\n\n\nLindquist, Everet Franklin. 1940. “Statistical analysis in educational research.”",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NHST.html</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFisher, Ronald Aylmer. 1970. “Statistical Methods for Research\nWorkers.” In Breakthroughs in Statistics: Methodology and\nDistribution, 66–70. Springer.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric\nElguero, Didier Fontenille, François Renaud, Carlo Costantini, and\nFrédéric Thomas. 2010. “Beer Consumption Increases Human\nAttractiveness to Malaria Mosquitoes.” PloS One 5 (3):\ne9546.\n\n\nLindquist, Everet Franklin. 1940. “Statistical Analysis in\nEducational Research.”\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#책의-구성",
    "href": "index.html#책의-구성",
    "title": "챗GPT 데이터 과학",
    "section": "책의 구성",
    "text": "책의 구성",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 데이터 과학",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 4월 속초 영금정\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "tidyr.html",
    "href": "tidyr.html",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "8.1 시작하기\n먼저 설치하지 않았다면 tidyr 패키지를 설치한다(아마도 앞에서 dplyr 패키지는 설치했을 것이다):\n#install.packages(\"tidyr\")\n#install.packages(\"dplyr\")\n패키지를 로드한다\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\n먼저 원래 gapminder 데이터프레임의 구조를 살펴보자:\nstr(gapminder)\n#&gt; 'data.frame':    1704 obs. of  6 variables:\n#&gt;  $ country  : chr  \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n#&gt;  $ year     : int  1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n#&gt;  $ pop      : num  8425333 9240934 10267083 11537966 13079460 ...\n#&gt;  $ continent: chr  \"Asia\" \"Asia\" \"Asia\" \"Asia\" ...\n#&gt;  $ lifeExp  : num  28.8 30.3 32 34 36.1 ...\n#&gt;  $ gdpPercap: num  779 821 853 836 740 ...\ngapminder 데이터셋처럼, 관측된 데이터에는 다양한 자료 형식이 있다. 대부분 순도 100% ‘long’ 혹은 순도 100% ‘wide’ 자료 형식 사이 어딘가에 위치하게 된다. gapminder 데이터셋에는 “ID” 변수가 3개(continent, country, year), “관측변수”가 3개(pop, lifeExp, gdpPercap)가 있다. 저자는 일반적으로 대부분의 경우에 중간단계 형식 데이터를 선호한다. 칼럼 1곳에 모든 관측점이 3가지 서로 다른 단위를 갖는 일은 거의 없다(예를 들어, ID변수 4개, 관측변수 1개).\n흔히 벡터 기반인 다수의 R 함수를 사용할 때, 서로 다른 단위를 갖는 값에 수학적 연산작업을 수행하지는 않는다. 예를 들어, 순수 ‘long’ 형식을 사용할 때, 인구, 기대수명, GDP의 모든 값에 대한 평균은 의미가 없는데, 이는 상호 호환되지 않는 3가지 단위를 갖는 평균값을 계산하여 반환하기 때문이다. 해법은 먼저 집단으로 그룹지어서 데이터를 솜씨 있게 다루거나(dplyr 학습교재 참조), 데이터프레임 구조를 변경시키는 것이다. 주의: R에서 일부 도식화 함수는 ‘wide’ 형식 데이터에 더 잘 작동한다.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#시작하기",
    "href": "tidyr.html#시작하기",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "도전과제\n\n\n\ngapminder는 순수한 ‘long’ 형식인가, ‘wide’ 형식인가, 혹은 두 가지 특징을 갖는 중간 형식인가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n원 gapminder 데이터프레임은 두 가지 특징을 갖는 중간 형식이다. 데이터프레임에 다수의 관측변수(pop, lifeExp, gdpPercap)가 있다는 점에서, 순수한 long 형식이라고 보기는 어렵다.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "href": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.2 pivot_longer(): wide에서 long 형식 전환",
    "text": "8.2 pivot_longer(): wide에서 long 형식 전환\n지금까지 깔끔한 형식을 갖는 원본 gapminder 데이터셋으로 작업을 했다. 하지만 ‘실제’ 데이터(즉, 자체 연구 데이터)는 절대로 잘 구성되어 있지 못하다. gapminder 데이터셋에 대한 wide 형식 버전을 가지고 시작해보자.\n\n이곳에서 ‘wide’ 형태를 갖는 gapminder 데이터를 다운로드 받아서, 로컬 data 폴더에 저장시킨다.\n\n데이터 파일을 불러와서 살펴보자. 주의: continent, country 칼럼이 요인형 자료형이 될 필요가 없으므로 read.csv() 함수 인자로 stringsAsFactors을 거짓(FALSE)으로 설정한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nwide 형식 데이터프레임\n\n깔끔한 중간 데이터 형식을 얻는 첫 단추는 먼저 ‘wide’ 형식에서 ‘long’ 형식으로 변환하는 것이다. tidyr 패키지의 pivot_longer() 함수는 관측 변수를 모아서(gather) long 형식 단일 변수로 변환한다. wide에서 long 형식으로 변환하기 위해 pivot_longer() 함수를 사용한다. pivot_longer()는 행의 수를 늘리고 열의 수를 줄임으로써 데이터셋을 더 길게 만들거나 관측 변수를 단일 변수로 ’연장’한다.\n\n\n\n\n\n그림 8.2: wide 형식에서 long 형식 전환과정 도식화\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n위에서 파이프 구문을 사용했는데, 이는 앞서 dplyr로 작업한 것과 유사하다. 사실, dplyr과 tidyr은 상호 호환되어 파이프 구문으로 dplyr과 tidyr 팩키지 함수를 파이핑하여 혼합하여 사용할 수 있다.\n먼저 pivot_longer()에 longer 형식으로 피벗될 열 이름 벡터를 제공한다. 모든 관측 변수를 입력할 수도 있지만 dplyr 레슨의 select() 함수처럼 starts_with() 인수를 사용하여 원하는 문자열로 시작하는 모든 변수를 선택할 수 있다. pivot_longer()는 피벗하지 않을 변수(즉, ID 변수)를 식별하기 위해 - 기호를 사용하는 대체 구문도 허용한다. pivot_longer()에 대한 다음 인수는 새 ID 변수(obstype_year)를 포함할 열의 이름을 지정하는 names_to와 새로 합쳐진 관측 변수(obs_value)의 이름을 지정하는 values_to이다. 새 열 이름을 문자열로 제공하여 후속 작업 가독성을 높인다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n특정 데이터프레임에서는 사소해 보일 수 있지만, 때로는 ID 변수 1개와 불규칙한 변수 이름을 가진 관측 변수 40개를 가질 수 있다. 이런 유연성은 시간을 상당히 절약해 준다!\n이제 obstype_year은 정보가 두 조각으로 나뉜다. 관측 유형(pop, lifeExp, gdpPercap)과 연도(year). separate() 함수를 사용하여 문자열을 여러 변수로 분할할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\ngap_long을 사용해서 각 대륙별로 평균 기대수명, 인구, 1인당 GDP를 계산한다. 힌트: dplyr에서 학습한 group_by()와 summarize() 함수를 사용한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "href": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.3 pivot_wider(): ’long’에서 중간 형식으로",
    "text": "8.3 pivot_wider(): ’long’에서 중간 형식으로\n작업을 항상 확인하는 것이 좋다. pivot_wider()는 pivot_longer()의 반대로, 열의 수를 늘리고 행의 수를 줄여 데이터셋을 더 넓게 만든다. pivot_wider()를 사용하여 gap_long을 원래의 중간 형식 또는 가장 넓은 형식으로 피벗하거나 재구성할 수 있다. 중간 형식에서부터 시작해보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 최초 데이터프레임 gapminder와 동일한 차원을 갖는 중간 데이터프레임 gap_normal이 있다. 하지만 변수 순서가 다르다. 순서를 수정하기 전에 all.equal() 함수를 사용해서 동일한지 확인한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n거의 다 왔다. 최초 데이터프레임은 country로 정렬된 다음 year로 정렬되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n훌륭하다! ‘long’ 형식에서 다시 중간 형식으로 돌아왔지만, 코드에 어떤 오류도 스며들지 않았다.\n이제 long에서 wide로 완전히 변환해 보자. wide 형식에서는 country와 continent를 ID 변수로 유지하고 관측치를 3개의 측정 기준(pop, lifeExp, gdpPercap)과 시간(year)에 걸쳐 피벗할 것이다. 먼저 모든 새 변수(시간*측정 기준 조합)에 대한 적절한 레이블을 만들어야 하며 gap_wide를 정의하는 과정을 단순화하기 위해 ID 변수를 통합해야 한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nunite()를 사용하여 이제 continent와 country의 조합인 단일 ID 변수를 가지고 있고 변수 이름을 정의했다. 이제 pivot_wider()로 파이핑할 준비가 되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n국가, 연도 및 3개의 측정 기준에 대해 피벗하여 gap_ludicrously_wide 형식 데이터를 만드시오.\n힌트 이 새로운 데이터 프레임은 행이 5개만 있어야 한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n이제 훌륭한 ‘wide’ 형식 데이터프레임을 가지고 있지만 ID_var가 더 사용하기 편할 수 있다. separate()를 사용하여 2개의 변수로 분리해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 되돌아왔다!",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "9  논문 품질 그래프 생성",
    "section": "",
    "text": "9.1 계층\n산점도는 시간에 따른 정보를 시각화하는 데 가장 적합한 방법은 아닐 수 있다. 대신 ggplot에게 선 그래프로 데이터를 표현하도록 지시할 수 있다.\ngeom_point 계층 대신 geom_line 계층을 추가했다. aes에 by를 추가하여 ggplot이 각 국가를 선으로 연결하여 표현하도록 했다.\n그런데 선과 점을 모두 표시하려면 어떻게 해야 할까? 간단히 그래프에 또 다른 계층을 추가하면 된다.\n각 계층이 이전 계층 위에 그려진다는 점에 주목하자. 이번 예제에서는 점이 선 위에 표시되었다. 다음에 표시된 그래프를 보자.\n이 예제에서 aesthetic color 매핑이 ggplot의 전역 설정에서 geom_line 계층으로 이동했다. 따라서 더 이상 점에는 적용되지 않는다. 이제 선이 점 위에 그려진 것을 명확히 볼 수 있다.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#계층",
    "href": "ggplot2.html#계층",
    "title": "9  논문 품질 그래프 생성",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\n지금까지 (색상같은) aesthetic 를 데이터의 변수로 매핑(mapping)해서 사용하는 법을 살펴봤다. 예를 들어, geom_line(aes(color=continent))을 사용하면, ggplot에서 자동으로 각 대륙별로 다른 색상을 입힌다. 그런데, 모든 선을 파란색으로 바꾸고자 하면 어떨까? geom_line(aes(color=\"blue\")) 명령어가 동작해야 된다고 생각하지만, 사실은 그렇지 않다.특정 변수에 대한 매핑을 생성하지 않았기 대문에, aes() 함수 밖으로 색상을 명세하는 부분을 예를 들어, geom_line(color=\"blue\")와 같이 빼내기만 하면 된다.\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n선이 점 위에 그려진다!",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#변환과-통계",
    "href": "ggplot2.html#변환과-통계",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.2 변환과 통계",
    "text": "9.2 변환과 통계\nggplot을 사용하면 데이터에 통계 모델을 쉽게 겹칠 수 있다. 이를 시연하기 위해서, 첫번째 예제로 돌아가보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n현재 1인당 GDP에 몇 가지 극단적인 이상치가 있어서 점 사이의 내재된 관계를 보기 어렵다. scale 함수를 사용하여 y 축 스케일을 조정할 수 있다. 이를 통해 데이터 값과 aesthetic 시각적 표현 사이의 매핑을 제어할 수 있다. alpha 함수를 사용하여 투명도도 조정할 수 있는데, 이는 특히 많은 데이터가 한 곳에 밀집되어 있을 때 유용하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래프를 렌더링하기 전에 log10 함수가 gdpPercap 열 값을 변환했다. 따라서 변환된 스케일에서 10의 거듭제곱 단위마다 1씩 증가한다. 예를 들어, 1인당 GDP 1,000은 y-축에 3, 10,000은 y-축에 4에 대응된다. 로그 변환은 x 축에 넓게 퍼져 있는 데이터를 시각화하는 데 도움이 된다.\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\ngeom_point(alpha = 0.5)를 사용한 것에 주목하자. 앞서 언급했듯이, aes() 함수 외부에서 설정한 것은 모든 점에 동일한 값이 적용된다. 이 경우 투명도를 지정하는 것은 원하는 바로 문제가 없다. 그러나 다른 aesthetic 설정과 마찬가지로 alpha 투명도를 데이터의 변수에 매핑할 수도 있다. 예를 들어, 각 대륙마다 다른 투명도를 적용하려면 geom_point(aes(alpha = continent))와 같이 코딩하면 된다.\n\n\n또 다른 계층(geom_smooth)을 추가하여 관계를 단순하게 모델링할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n굵은 선의 두께는 geom_smooth 계층의 aesthetic size를 설정하여 조정할 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\naesthetic을 지정하는 방법에는 두 가지가 있다. 방금 전에는 geom_smooth 함수에 인수로 전달하여 size에 대한 aesthetic을 설정했다. 앞에서는 aes 함수를 사용하여 데이터 변수와 시각적 표현 사이의 매핑을 정의했다.\n\n\n\n\n\n\n도전과제\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자.\n힌트: aes 함수를 사용하지 않는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자. 힌트: aes 함수를 사용하지 않는다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#다중-패널-그래프",
    "href": "ggplot2.html#다중-패널-그래프",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.3 다중-패널 그래프",
    "text": "9.3 다중-패널 그래프\n앞에서는 하나의 그래프에 모든 국가에 대한 시간의 흐름에 따른 기대수명 변화를 시각화했다. 대안으로 facet 계층을 추가하여 그래프를 여러 개의 패널로 나눌 수 있다. 이번에는 국가명이 “A” 또는 “Z”로 시작하는 국가에만 초점을 맞춰보자.\n\n\n\n\n\n\n유용한 팁\n\n\n\n데이터의 부분 집합을 추출하는 것으로 시작해보자. substr 함수를 사용하여 문자열의 일부를 추출할 수 있다. 이 경우에는 gapminder$country 벡터의 시작과 끝 위치 문자를 추출한다. %in% 연산자를 사용하면 여러 개의 비교를 간단하게 수행할 수 있다. (이 경우 starts.with %in% c(\"A\", \"Z\")는 starts.with == \"A\" | starts.with == \"Z\"와 동일하다)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfacet_wrap 계층은 틸드(~)로 표시되는 “공식”을 인수로 받는다. gapminder 데이터셋의 country 열에 있는 각 고유값에 대해 별도의 패널로 그래프를 생성한다.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#텍스트-수정하기",
    "href": "ggplot2.html#텍스트-수정하기",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.4 텍스트 수정하기",
    "text": "9.4 텍스트 수정하기\n출판용 그래프를 만들기 위해서는 텍스트 요소를 일부 변경해야 할 필요가 있다. x 축이 너무 복잡하고, y 축은 데이터프레임 열 이름이 아닌 “Life expectancy”로 표시되어야 한다.\n몇 가지 다른 계층을 추가하여 텍스트를 수정할 수 있다. theme 계층은 각 축의 텍스트와 전반적인 텍스트 크기를 제어한다. 축, 그래프 제목, 범례는 labs() 함수를 사용하여 설정할 수 있다. 범례 제목은 aes() 함수에서 지정한 것과 동일한 이름을 사용한다. 따라서 색상 범례 제목은 color = \"Continent\"가 되고, 채우기(fill) 범례는 fill = \"MyTitle\"이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#그래프-저장하기",
    "href": "ggplot2.html#그래프-저장하기",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.5 그래프 저장하기",
    "text": "9.5 그래프 저장하기\nggsave() 함수를 사용하여 ggplot으로 생성한 그래프를 로컬 컴퓨터에 저장할 수 있다. 출판을 위한 고품질 그래픽을 생성하기 위해 그래프의 크기와 해상도를 ggsave() 함수의 인수(width, height, dpi)로 전달할 수 있다. 앞에서 생성한 그래프를 저장하려면 먼저 lifeExp_plot 변수에 그래프를 할당한 다음 ggsave() 함수를 사용하여 png 형식으로 results 디렉터리에 저장하도록 지정한다. (작업 디렉터리에 results/ 폴더가 생성되어 있어야 한다.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nggsave() 함수에는 두 가지 좋은 점이 있다. 첫째, 기본값으로 가장 최근에 생성한 그래프가 저장되므로 plot 인수를 생략하면 ggplot으로 생성한 마지막 그래프가 자동으로 저장된다. 둘째, 저장되는 그래프 이미지 형식이 파일 확장자(예: .png 또는 .pdf)에 따라 결정된다. 필요한 경우 device 인수를 사용하여 명시적으로 파일 형식을 지정할 수도 있다.\n지금까지 ggplot2의 기본을 살펴보았다. RStudio는 다른 계층 사용법에 대한 참고자료로 유용한 치트 시트를 제공하고 있으며, ggplot2 웹사이트에는 추가 기능에 대한 자세한 정보가 있다. 마지막으로, 어떻게 수정해야 할지 모르겠다면 구글 검색을 통해 Stack Overflow에서 재사용 가능한 코드와 함께 관련 질문과 답변을 쉽게 찾을 수 있다.\n\n\n\n\n\n\n도전과제 (심화)\n\n\n\n가용한 연도 기간동안 각 대륙 간 기대수명을 비교하는 상자 그림(boxplot)을 생성한다.\n\ny축의 이름을 “기대수명”으로 변경한다.\nx축 레이블은 제거한다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n가능한 해법 중 하나는 다음과 같다. xlab()과 ylab()은 각각 x축과 y축 레이블을 설정한다. 축의 제목, 텍스트, 눈금은 테마의 속성이며 theme() 호출 내에서 수정되어야 한다.\n\n9.5.1 R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "dplyr_plyr.html",
    "href": "dplyr_plyr.html",
    "title": "\n16  분할-적용-병합 전략\n",
    "section": "",
    "text": "16.1 dplyr 패키지\ndplyr 패키지는 데이터 조작을 위한 문법을 제공하며, 이를 통해 “분할-적용-병합” 문제를 해결할 수 있다. 먼저 dplyr 패키지를 로드하고, dplyr을 사용하여 대륙별 평균 GDP를 빠르게 계산한다.\n방금 코드에서 일어난 일을 복기해 보자.\n그룹화 변수를 추가하면 어떻게 될까?\ngroup_by()에 year를 추가하여 대륙과 연도별로 그룹화했다.\n이렇게 dplyr 패키지를 사용하면 데이터 조작과 분석을 보다 간결하고 직관적으로 수행할 수 있다. 파이프 연산자(%&gt;%)를 사용하여 연산을 연결하면 코드의 가독성도 향상된다. group_by(), summarise(), mutate() 등의 함수를 조합하여 다양한 분석 작업을 수행할 수 있다.알겠습니다. 계속해서 교정 작업을 진행하겠습니다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "dplyr_plyr.html#dplyr-패키지",
    "href": "dplyr_plyr.html#dplyr-패키지",
    "title": "\n16  분할-적용-병합 전략\n",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n%&gt;% 파이프 연산자를 사용하여 calcGDP(gapminder) 함수의 결과를 다음 단계로 전달했다.\n\ngroup_by(continent)를 사용하여 데이터를 대륙별로 그룹화했다.\n\nsummarise()를 사용하여 각 그룹에 대해 mean(gdp)를 계산하고, 결과를 mean_gdp라는 새로운 열에 저장했다.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n도전과제\n\n\n\n대륙별로 평균 기대수명을 계산해보자. 어느 대륙의 기대수명이 가장 길고, 어느 대륙이 가장 짧은가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\ndplyr의 함수를 사용하여 도전과제 2의 출력 결과로부터, 1952년에서 2007년 사이의 평균 기대수명 차이를 계산해보자.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n실제로 실행하지 말고, 다음 중 어떤 코드가 대륙별 평균 기대수명을 계산하는지 살펴보자.\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(gapminder$continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  filter(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n답안 3은 대륙별 평균 기대 수명을 계산한다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "functions_ds.html",
    "href": "functions_ds.html",
    "title": "\n16  데이터 과학 함수\n",
    "section": "",
    "text": "16.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n#&gt; # A tibble: 11 × 5\n#&gt;    species island    sex    year body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n#&gt;  2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n#&gt;  3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n#&gt;  4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n#&gt;  5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n#&gt;  6 Adelie  Dream     &lt;NA&gt;   2007        2975\n#&gt;  7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n#&gt;  8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n#&gt;  9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n#&gt; 10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n#&gt; 11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#역사",
    "href": "functions_ds.html#역사",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.2 역사",
    "text": "16.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다.\n\n데이터 분석에서 빈도수가 높은 작업을 Base R과 dplyr 패키지를 사용한 사례를 다음과 같이 비교하면 S언어에서 현재까지 이뤄낸 발전이 가시적으로 다가온다.\n\n\n\n\n\n\n\n작업\nBase R\ndplyr\n\n\n\n행 필터링 (Filter)\nsubset(data, condition)\ndata %&gt;% filter(condition)\n\n\n특정 칼럼 선택 (Select)\ndata[, c(\"col1\", \"col2\")]\ndata %&gt;% select(col1, col2)\n\n\n그룹별 집계작업\naggregate(. ~ grouping_var, data, FUN = mean)\ndata %&gt;% group_by(grouping_var) %&gt;% summarize(new_col = mean(col_name))\n\n\n죠인(Join)\nmerge(data1, data2, by = \"key_column\")\ndata1 %&gt;% inner_join(data2, by = \"key_column\")\n\n\n칼럼 추가\ntransform(data, new_col = some_func(existing_col))\ndata %&gt;% mutate(new_col = some_func(existing_col))\n\n\n행 결합\nrbind(data1, data2)\nbind_rows(data1, data2)\n\n\n칼럼 결합\ncbind(data1, data2)\nbind_cols(data1, data2)\n\n\n정렬\ndata[order(data$col_name), ]\ndata %&gt;% arrange(col_name)\n\n\n\n\n16.2.1 attach() 함수\n데이터프레임에 attach() 함수를 사용하면 데이터프레임을 구성하는 칼럼이 벡터로 작업환경에서 바로 접근하여 작업을 수행할 수 있다. penguins 데이터프레임을 attach()한 결과 bill_depth_mm 벡터가 작업환경에서 바로 접근하여 평균값을 계산할 수 있게 되었다. 작업을 완료한 후에 detach() 를 사용해서 작업환경에서 제거한다.\n\nlibrary(palmerpenguins)\n\nbase::attach(penguins)\n\nls(pos = which(search() == \"penguins\")[1])\n#&gt; [1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n#&gt; [4] \"flipper_length_mm\" \"island\"            \"sex\"              \n#&gt; [7] \"species\"           \"year\"\n\nmean(bill_depth_mm, na.rm = TRUE)\n#&gt; [1] 17.15117\n\ndetach(penguins)\n\n\n16.2.2 with() 함수\nattach() 함수는 편리한 장점이 있지만, 데이터프레임 변수명과 함수명, 또 다른 작업에서 나온 객체명과 충돌이 발생할 경우 전혀 생각하지 못한 문제가 발생할 수 있다. 따라서, 격리를 통해 문제를 단순화하는 것이 필요하다. 이를 위해서 with() 함수를 사용하게 되면 데이터프레임에 속한 칼럼명을 명시하지 않더라도 간결하게 데이터 분석 작업을 이어나갈 수 있다.\n\nlibrary(palmerpenguins)\n\nwith(data = penguins,\n     expr = mean(bill_depth_mm, na.rm = TRUE))\n#&gt; [1] 17.15117\n\n\n16.2.3 aggregate() 함수\nBase R 에서 지원되는 aggregate() 함수를 사용해서 동일한 결과를 얻을 수 있다. aggregate() 함수는 with()와 지향점은 유사하지만 구현방식에서 다소 차이가 난다.\n\naggregate(bill_depth_mm ~ 1, \n          data = penguins, \n          FUN = mean, \n          na.rm = TRUE)\n#&gt;   bill_depth_mm\n#&gt; 1      17.15117",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#데이터-마스킹",
    "href": "functions_ds.html#데이터-마스킹",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.3 데이터 마스킹",
    "text": "16.3 데이터 마스킹\nR에서의 데이터 마스킹(Data Masking)은 tidyverse 생태계에서 데이터 문법을 담당하는 dplyr 패키지에서 핵심적인 개념이다. 데이터 마스킹을 사용하면 데이터프레임 칼럼을 $, [[ ]]를 사용하지 않고도 칼럼명으로 직접 참조할 수 있어, 데이터를 조작하고 변환할 때 훨씬 직관적이고 가독성 높은 코드를 작성할 수 있다.\npenguins 데이터프레임의 species를 데이터 마스킹 없이 조작하려면 penguins$species 혹은 penguins[['species']]와 같이 구문을 작성해야 하지만 데이터 마스킹을 사용하면 species 만으로 충분한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# 데이터 마스킹을 사용하여 펭귄종(species)이 \"Adelie\"만 추출한다.\npenguins %&gt;% filter(species == \"Adelie\")\n#&gt; # A tibble: 152 × 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ℹ 142 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\ndplyr 함수의 데이터 마스킹은 비표준 평가 (Non-standard evaluation, NSE)라는 개념에 기반을 두는데, 표현식을 캡처하고 난 후 바로 바로 실행되지 않고 제공된 데이터의 맥락 내에서 평가가 이루어진다. 데이터 마스킹은 강력하며 깔끔한 구문을 제공하지만, 칼럼명과 충돌할 수 있는 환경의 변수 이름이 있을 때 예기치 않은 방식으로 동작한다. 모호한 상황이 발생할 때 항상 다음과 같은 방식으로 .data$column_name 함으로써 데이터 마스킹 재정의(Overriding)를 통해 명확히 한다.\n\nspecies &lt;- \"Chinstrap\"\n\npenguins %&gt;% \n  filter(.data$species == \"Adelie\")\n#&gt; # A tibble: 152 × 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ℹ 142 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n항목\n데이터 마스킹\nTidy Evaluation\n\n\n\n정의\n- 데이터프레임 칼럼명을 직접적인 변수처럼 다룰 수 있는 능력.  - $나 [[ ]] 없이 칼럼 참조를 단순화.\n- R 메타프로그래밍을 위한 프레임워크, 특히 tidyverse 에서 사용.  - 다양한 맥락에서 표현식을 캡쳐하고 평가하는 도구 제공.\n\n\n사용 사례\n- dplyr 함수에서 직접 데이터 조작.  - 코드 가독성 향상.\n- 따옴표 없는 표현식으로 사용자 정의 함수 생성.  - 표현식을 프로그래밍 방식으로 구성 및 평가.  - 표현식 평가 맥락 제어.\n\n\n구현\n- 기본적으로 tidy evaluation 메커니즘을 사용하여 구현됨.\n- rlang 패키지 enquo(), quo(), !! 등을 사용.  - 표현식과 그 환경을 캡쳐하기 위해 쿼저(Quosure) 의존.\n\n\n복잡성\n- 최종 사용자를 위해 간소화.  - 기본적인 복잡성을 추상화.\n- R 메타프로그래밍 이해 필요.  - 고급 사용자에게 더 많은 유연성 제공.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#깔끔한-평가",
    "href": "functions_ds.html#깔끔한-평가",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.4 깔끔한 평가",
    "text": "16.4 깔끔한 평가\n깔끔한 평가(Tidy evaluation)은 R tidyverse 프레임워크로, 특히 비표준 평가 (Non-Standard Evaluation, NSE)와 관련하여 tidyverse 함수로 프로그래밍하는 방법을 표준화했다. NSE는 R 함수가 표준과는 다른 맥락에서 표현식을 평가할 때 발생한다.\n\n준인용(Quasiquotation): enquo() 함수를 사용하여 표현식을 캡처하고 !!를 사용하여 표현식의 인용제거(Unquoting)을 가능케 한다.\nPronouns (대명사): .data 대명사는 데이터프레임의 칼럼명을 명시적으로 참조하는데 사용되어 모호성을 제거한다.\n함수: enquo()는 표현식을 캡처하고, quo_name()은 표현식을 문자열로 변환하며, !!는 표현식 인용제거 또는 주입작업을 수행한다.\n\n예를 들어, dplyr 패키지 filter 및 select와 같은 동사를 사용하지만 함수에 칼럼명을 작성하려는 경우, 인수로 전달될 때 이러한 동사가 어떤 칼럼을 참조하는지 명확히 하기 위해 깔끔한 평가(tidy evaluation)가 사용된다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_and_select &lt;- function(data, col_name, threshold) {\n  \n  # 칼럼명 문자열을 기호로 변환\n  col_sym &lt;- sym(col_name)\n  \n  # 준인용(quasiquotation)을 사용해서 칼럼 표현식을 캡쳐\n  col_expr &lt;- enquo(col_sym)\n  \n  # !! 연산자를 이용하여 인용제거(unquote)하고 표현식을 주입\n  data %&gt;% \n    filter(!!col_expr &gt; threshold) %&gt;% \n    select(!!col_expr)\n}\n\nfilter_and_select(penguins, \"bill_length_mm\", 55)\n#&gt; # A tibble: 5 × 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           59.6\n#&gt; 2           55.9\n#&gt; 3           55.1\n#&gt; 4           58  \n#&gt; 5           55.8\n\nsym() → enquo() → !!(뱅뱅) 구현방식이 Defuse-and-Inject 패턴으로 내부 동작방식은 동일하지만 사용자 구문은 { 칼럼명 }으로 깔끔해졌다.\n\nfilter_and_select_latest &lt;- function(data, col_name, threshold) {\n  \n  data %&gt;% \n    filter({{ col_name }} &gt; threshold) %&gt;% \n    select({{ col_name }})\n  \n}\n\nfilter_and_select_latest(penguins, bill_length_mm, 55)\n#&gt; # A tibble: 5 × 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           59.6\n#&gt; 2           55.9\n#&gt; 3           55.1\n#&gt; 4           58  \n#&gt; 5           55.8\n\n\n데이터 마스킹: 변수를 사용하여 계산하는 arrange(), filter(), summarize() 등 함수에 사용.\nTidy-selection: 변수를 선택하는 select(), relocate(), rename()과 같은 함수에 사용.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그룹별-평균",
    "href": "functions_ds.html#그룹별-평균",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.5 그룹별 평균",
    "text": "16.5 그룹별 평균\n\n데이터프레임 요약통계량을 계산하는 코드를 작성해보자. 펭귄종(species) 별로 부리길이를 계산하는 코드를 작성해보자.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(부리길이_평균 = mean(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 3 × 2\n#&gt;   species   부리길이_평균\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie             38.8\n#&gt; 2 Chinstrap          48.8\n#&gt; 3 Gentoo             47.5\n\n이번에는 그룹변수와 데이터프레임 칼럼명을 달리하여 그룹별로 평균을 계사하는 함수를 작성하여 코드로 작성해보자.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by(group_varname) |&gt; \n    summarise(부리길이_평균 = mean(varname, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group_varname` is not found.\n\n상기 코드가 동작하지 않는 이유는 함수에 tidyverse 코드를 함수에 단순히 전달해서 넣기 때문에 발생했다. 다음과 같이 칼러명을 포용(embracing)하는 방식으로 { 칼럼명 }과 같이 함수에 사용되는 데이터프레임 변수명을 명시적으로 작성할 경우 문제가 해결된다.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by( {{ group_varname }} ) |&gt; \n    summarise(부리길이_평균 = mean( {{ varname }}, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n#&gt; # A tibble: 3 × 2\n#&gt;   species   부리길이_평균\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie             38.8\n#&gt; 2 Chinstrap          48.8\n#&gt; 3 Gentoo             47.5",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그래프",
    "href": "functions_ds.html#그래프",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.6 그래프",
    "text": "16.6 그래프\n\n데이터프레임에서 범주형 변수를 하나 선택하여 빈도수를 시각화하는 스크립트를 다음과 같이 작성한다.\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  count(island) |&gt; \n  ggplot(aes(x=island, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n범주형 변수를 막대그래프로 시각화하는 함수를 제작해보자.\n\ndraw_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    count( {{ varname }} ) |&gt; \n    ggplot(aes(x = {{ varname }}, y = n )) + \n      geom_col()\n}\n\npenguins |&gt; draw_bar_chart(year)\n\n\n\n\n\n\n\n함수내에서 새로운 변수 이름을 생성하는 경우 := 연산자를 사용해야 한다. 깔끔한 평가(tidy evaluation)에서 = 와 동일한 역할을 수행하는 것이 := 이기 때문이다. 예를 들어, 펭귄이 서식하고 있는 섬을 기준으로 빈도수를 내림차순 막대그래프를 작성할 경우, 함수 내부에서 범주형 변수를 다시 재정의해야 하는데 이 경우 := 연산자의 도입이 필요하다.\n\nlibrary(forcats)\n\norder_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    mutate({{ varname }} :=  fct_rev(fct_infreq( {{ varname }} ))) |&gt;\n    ggplot(aes(y = {{ varname }} )) + \n      geom_bar()\n}\n\npenguins |&gt; order_bar_chart(island)\n\n\n\n\n\n\n\n그래프를 작성할 때 거의 항상 등장하는 문제가 그래프 x-축, y-축 라벨을 붙이고 그래프 제목, 범례 등 텍스트를 넣어야 한다. 이런 경우 stringr, glue 패키지의 다양한 함수를 깔끔한 평가(tidy evaluation)에서 지원하는 기능이 rlang 패키지 englue() 함수다.\n\ndraw_bar_chart &lt;- function(dataframe, varname, penguin_species) {\n  \n  title_label &lt;- rlang::englue(\"남극에 서식하고 있는 펭귄 종 ({{ varname }}) {penguin_species} 빈도수\")\n  \n  dataframe |&gt; \n    filter({{ varname }} == penguin_species) |&gt; \n    count( island ) |&gt; \n    ggplot(aes(x = island, y = n )) + \n      geom_col() +\n      labs(title = title_label)\n}\n\npenguins |&gt; draw_bar_chart(species, \"Adelie\")\n\n\n\n\n\n\n\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#역사-1",
    "href": "functions_ds.html#역사-1",
    "title": "\n16  데이터 과학 함수\n",
    "section": "\n16.7 역사",
    "text": "16.7 역사\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "\n14  함수\n",
    "section": "",
    "text": "14.1 함수 기본 지식\n함수는 입력값(x)를 넣어 어떤 작업(f)을 수행한 결과를 반환(y) 과정으로 이해할 수 있는데, 인자로 다양한 값을 함수에 넣을 수 있고, 물론 함수가 뭔가 유용한 작업을 수행하기 위한 전제조건을 만족시키는지 확인하는 과정을 assert 개념을 넣어 확인하고 기술된 작업을 수행한 후에 출력값을 변환시키게 된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#all-about-function",
    "href": "functions.html#all-about-function",
    "title": "\n14  함수\n",
    "section": "",
    "text": "데이터 과학 함수 개념",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-use-function",
    "href": "functions.html#how-to-use-function",
    "title": "\n14  함수\n",
    "section": "\n14.2 함수 사용법",
    "text": "14.2 함수 사용법\n본격적으로 함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 과학 대표 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 자세히 살펴보자.\n\n14.2.1 R 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n#&gt; [[1]]\n#&gt; [1] 10\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 2.333333\n\n\n14.2.2 파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n#&gt; [10, 4, 21, 2.3333333333333335]\n\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 다른 사람이 작성한 함수를 사용하기 위해서 먼저 함수명을 알아야 하고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n#&gt; function (x, na.rm = FALSE) \n#&gt; NULL\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야만 표준편차를 계산할 수 있다. 예를 들어, 데이터프레임(penguins)의 변수 하나(bill_length_mm)를 지정하여 전달하고 na.rm = TRUE도 명세하여 인자로 전달한다. 인자값이 기본디폴트 값으로 설정된 경우 타이핑을 줄일 수 있고, 결측값이 포함된 경우에 따라서 다른 인자를 넣어 전달하는 방식으로 함수를 사용한다.\n\nlibrary(palmerpenguins)\n\nsd(penguins$bill_length_mm, na.rm = TRUE)\n#&gt; [1] 5.459584",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#convert-scripts-to-function",
    "href": "functions.html#convert-scripts-to-function",
    "title": "\n14  함수\n",
    "section": "\n14.3 스크립트 → 함수",
    "text": "14.3 스크립트 → 함수\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다.\n\nR 함수 템플릿을 제작한다.\n\n함수명 &lt;- function() { }\n\n\n스크립트를 함수 몸통에 복사하여 붙인다.\n반복작업되는 인자를 찾아내 이를 인자로 넣어둔다.\n인자값과 연동되는 부분을 찾아 맞춰준다.\n함수명을 적절한 동사를 갖춘 이름으로 작명한다.\n\nreturn이 불필요하기 때문에 R 언어 특성을 반영하여 필요한 경우 제거한다.\n\n\n14.3.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해 본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n#&gt; [1] 2\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1, 2, 3, 4, 5, 6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n#&gt; [1] 1\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  face &lt;- sample(dice, size=num_try)\n  return(face) # 불필요함.\n}\n\ndraw_dice(3)\n#&gt; [1] 1 3 4",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#why-write-function",
    "href": "functions.html#why-write-function",
    "title": "\n14  함수\n",
    "section": "\n14.4 왜 함수가 필요한가?",
    "text": "14.4 왜 함수가 필요한가?\n왜 함수가 필요한지를 데이터를 분석할 때 자주 나오는 변수 정규화 사례를 바탕으로 살펴보자. 데이터프레임에 담긴 변수의 측도가 상이하여 변수를 상대적으로 비교하기 위해 측도를 재조정하여 표준화할 필요가 있다. 변수에서 평균을 빼고 표준편차로 나누는 정규화도 있지만, 최대값에서 최소값을 빼서 분모에 두고 분자에 최소값을 빼서 나누면 모든 변수가 0–1 사이 값으로 척도가 조정된다.\n\\[ f(x)_{\\text{척도조정}} = \\frac{x-min(x)}{max(x)-min(x)} \\]\n\ndf &lt;- data.frame(a=c(1,2,3,4,5),\n                         b=c(10,20,30,40,50),\n                         c=c(7,8,6,1,3),\n                         d=c(5,4,6,5,2))\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) /\n        (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) /\n        (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\ndf        \n#&gt;      a         b         c    d\n#&gt; 1 0.00  0.000000 0.8571429 0.75\n#&gt; 2 0.25 -1.111111 1.0000000 0.50\n#&gt; 3 0.50 -2.222222 0.7142857 1.00\n#&gt; 4 0.75 -3.333333 0.0000000 0.75\n#&gt; 5 1.00 -4.444444 0.2857143 0.00\n\n상기 R 코드는 측도를 모두 맞춰서 변수 4개(a, b, c, d)를 비교하거나 향후 분석을 위한 것이다. 하지만, 읽어야 하는 코드중복이 심하고 길어 코드를 작성한 개발자의 의도 가 본의 아니게 숨겨져 있다. 작성한 R 코드에 실수한 것이 있는 경우, 다음 프로그램 실행에서 버그(특히, 구문론이 아닌 의미론적 버그)가 숨겨지게 된다. 상기 코드가 작성되는 과정을 살펴보면 본의 아니게 의도가 숨겨진다는 의미가 어떤 것인지 명확해진다.\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) 코드를 작성한 후, 정상적으로 돌아가는지 확인한다.\n1번 코드가 잘 동작하게 되면 다음 복사하여 붙여넣기 신공을 사용하여 다른 칼럼 작업을 확장해 나간다. df$b, df$c, df$d를 생성하게 된다.\n즉, 복사해서 붙여넣은 것을 변수명을 편집해서 df$b, df$c, df$d 변수를 순차적으로 생성해 낸다.\n\n\n\n\n\n\n\n해들리 위캠 어록\n\n\n\n\n중복은 의도를 숨기게 되고, 복사하여 붙여넣기 두번하면 함수를 작성할 시점이 되었다. (Duplication hides the intent. If you have copied-and-pasted twice, it is time to write a function.)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#funciton-component",
    "href": "functions.html#funciton-component",
    "title": "\n14  함수\n",
    "section": "\n14.5 함수 구성요소",
    "text": "14.5 함수 구성요소\n\n14.5.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n\n14.5.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다.\n다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n#&gt; [1] 15\nsum_numbers(na_vector)\n#&gt; Error in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n#&gt; Error in if (assert_any_are_na(vec)) {: the condition has length &gt; 1\n\n\n14.5.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\n\ncat(\"절편: \", inter, \"\\n기울기: \", slope)\n#&gt; 절편:  37.88458 \n#&gt; 기울기:  -2.87579\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 나누어 할당하는 것도 가능하다. 각 붓꽃마다 0~1 사이 난수를 생성하여 할당한다. 그리고 난수값이 0.2 이상이면 훈련, 그렇지 않으면 시험 데이터로 구분한다.\n\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\ncat(\"총 관측점: \", nrow(iris), \"\\n훈련: \", nrow(train), \"\\n시험: \", nrow(test))\n#&gt; 총 관측점:  150 \n#&gt; 훈련:  121 \n#&gt; 시험:  29\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n#&gt; $intercept\n#&gt; (Intercept) \n#&gt;    37.88458 \n#&gt; \n#&gt; $beta\n#&gt;      cyl \n#&gt; -2.87579",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#time-to-write-function",
    "href": "functions.html#time-to-write-function",
    "title": "\n14  함수\n",
    "section": "\n14.6 함수를 작성하는 시점",
    "text": "14.6 함수를 작성하는 시점\n복사해서 붙여넣는 것을 두번 하게 되면, 함수를 작성할 시점이다. 중복을 제거하는 한 방법이 함수를 작성하는 것이고, 함수를 작성하게 되면 의도가 명확해진다. 함수명을 rescale로 붙이고 이를 실행하게 되면, 의도가 명확하게 드러나게 되고, 복사해서 붙여넣게 되면서 생겨나는 중복과 반복에 의한 실수를 줄일 수 있게 되고, 향후 코드를 갱신할 때도 도움이 된다.\n\nrescale &lt;- function(x){\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a &lt;- rescale(df$a)\ndf$b &lt;- rescale(df$b)\ndf$c &lt;- rescale(df$c)\ndf$d &lt;- rescale(df$d)\n\nrescale() 함수를 사용해서 복사하여 붙여넣는 중복을 크게 줄였으나, 여전히 함수명을 반복해서 복사하여 붙여넣기를 통해 코드를 작성했다. 함수형 프로그래밍을 사용하는 것으로 함수명을 반복적으로 사용하는 것조차도 피할 수 있다.\n\nlibrary(purrr)\ndf &lt;- map_df(df, rescale)\ndf\n#&gt; # A tibble: 5 × 4\n#&gt;       a     b     c     d\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0     1    0.857  0.75\n#&gt; 2  0.25  0.75 1      0.5 \n#&gt; 3  0.5   0.5  0.714  1   \n#&gt; 4  0.75  0.25 0      0.75\n#&gt; 5  1     0    0.286  0\n\n함수를 사용하지 않고 복사하여 붙여넣기 방식으로 코드를 작성한 경우 의도하지 않은 실수가 있어 함수를 도입하여 작성한 코드와 결과가 다른 것이 존재한다. 코드를 읽어 찾아보거나 실행한 후 결과를 통해 버그를 찾아보는 것도 함수의 의미와 중요성을 파악하는데 도움이 된다.\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것.\n\n한걸음 더 들어가 구체적으로 좋은 함수는 다음과 같은 특성을 지니고 있다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 일관된 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-write-function",
    "href": "functions.html#how-to-write-function",
    "title": "\n14  함수\n",
    "section": "\n14.7 사례: rescale 함수",
    "text": "14.7 사례: rescale 함수\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만드는 과정을 통해 앞서 학습한 사례를 실습해 보자.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#function-is-argument",
    "href": "functions.html#function-is-argument",
    "title": "\n14  함수\n",
    "section": "\n14.8 사례: 요약통계 함수",
    "text": "14.8 사례: 요약통계 함수\n데이터를 분석할 때 가장 먼저 수행하는 작업이 요약통계를 통해 데이터를 이해하는 것이다. 이미 훌륭한 요약통계 패키지와 함수가 있지만, 친숙한 개념을 함수로 다시 제작함으로써 함수에 대한 이해를 높일 수 있다.\n요약통계 기능을 먼저 구현한 다음에 중복 제거하여 요약통계 기능 함수를 제작해보자. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성하는 경우, 숫자 1,2,3 만 차이날 뿐 다른 부분은 동일하기 때문에 함수 코드에 중복이 심하게 관찰된다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n여기서 요약통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘겨 요약통계함수를 간략하고 가독성 높게 작성할 수 있다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 가정해보자.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n#&gt; [1] 0.5000000 0.5000000 0.7142857 0.7500000\ncol_summary(df, fun = mean)\n#&gt; [1] 0.5000000 0.5000000 0.5714286 0.6000000\ncol_summary(df, fun = sd)\n#&gt; [1] 0.3952847 0.3952847 0.4164966 0.3791438",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html",
    "href": "functions_purrr.html",
    "title": "17  함수형 프로그래밍",
    "section": "",
    "text": "17.1 왜 함수형 프로그래밍인가?\n데이터 분석을 아주 추상화해서 간략하게 얘기한다면 데이터프레임을 함수에 넣어 새로운 데이터프레임으로 만들어 내는 것이다.\n데이터 분석, 데이터 전처리, 변수 선택, 모형 개발이 한번에 해결되는 것이 아니라서, 데이터프레임을 함수에 넣어 상태가 변경된 데이터프레임이 생성되고, 이를 다시 함수에 넣어 또다른 변경된 상태 데이터프레임을 얻게 되는 과정을 쭉 반복해 나간다.\n따라서… 데이터 분석에는 함수형 프로그래밍 패러다임을 활용하고, 툴/패키지 개발에는 객체지향 프로그래밍 패러다임 사용이 권장된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#why-functional-programming",
    "href": "functions_purrr.html#why-functional-programming",
    "title": "17  함수형 프로그래밍",
    "section": "",
    "text": "함수로 이해하는 데이터 분석\n\n\n\n\n데이터 분석 작업흐름\n\n\n\n\n데이터 분석과 툴/패키지 도구 개발",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton",
    "href": "functions_purrr.html#functional-programming-newton",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.2 뉴튼 방법(Newton’s Method)\n",
    "text": "17.2 뉴튼 방법(Newton’s Method)\n\n뉴튼-랩슨 알고리즘으로도 알려진 뉴튼(Newton Method) 방법은 컴퓨터를 사용해서 수치해석 방법으로 실함수의 근을 찾아내는 방법이다.\n특정 함수 \\(f\\) 의 근을 찾을 경우, 함수 미분값 \\(f'\\), 초기값 \\(x_0\\)가 주어지면 근사적 근에 가까운 값은 다음과 같이 정의된다.\n\\[x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)}\\]\n이 과정을 반복하게 되면 오차가 매우 적게 근의 값에 도달하게 된다.\n\\[x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\]\n기하적으로 보면, 파란 선은 함수 \\(f\\) 이고, \\(f\\)를 미분한 \\(f'\\) 빨간 선은 뉴턴방법을 활용하여 근을 구해가는 과정을 시각적으로 보여주고 있다. \\(x_{n-1}\\) 보다 \\(x_n\\)이, \\(x_n\\) 보다 \\(x_{n+1}\\)이 함수 \\(f\\) 근에 더 가깝게 접근해 나가는 것이 확인된다.\n\n\n뉴튼 방법 도식화",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton-method",
    "href": "functions_purrr.html#functional-programming-newton-method",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.3 뉴튼 방법 R 코드 1\n",
    "text": "17.3 뉴튼 방법 R 코드 1\n\n뉴튼 방법을 R코들 구현하면 다음과 같이 612의 제곱근 값을 수치적으로 컴퓨터를 활용하여 구할 수 있다. while같은 루프를 활용하여 반복적으로 해를 구하는 것도 가능하지만 재귀를 활용하여 해를 구하는 방법이 코드를 작성하고 읽는 개발자 관점에서는 훨씬 더 편리하고 권장된다.\n하지만, 속도는 while 루프를 사용하는 것이 R에서는 득이 많다. 이유는 오랜 세월에 걸쳐 최적화 과정을 거쳐 진화했기 때문이다.\n\n\nwhile 루프를 사용한 방법\n\nfind_root &lt;- function(guess, init, eps = 10^(-10)){\n    while(abs(init**2 - guess) &gt; eps){\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"현재 값: \", init, \"\\n\")\n    }\n    return(init)\n}\n\nfind_root(612, 10)\n#&gt; 현재 값:  35.6 \n#&gt; 현재 값:  26.39551 \n#&gt; 현재 값:  24.79064 \n#&gt; 현재 값:  24.73869 \n#&gt; 현재 값:  24.73863 \n#&gt; 현재 값:  24.73863\n#&gt; [1] 24.73863\n\n\n\n재귀를 사용한 방법\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"재귀방법 현재 값: \", init, \"\\n\")\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\nfind_root_recur(612, 10)\n#&gt; 재귀방법 현재 값:  35.6 \n#&gt; 재귀방법 현재 값:  26.39551 \n#&gt; 재귀방법 현재 값:  24.79064 \n#&gt; 재귀방법 현재 값:  24.73869 \n#&gt; 재귀방법 현재 값:  24.73863 \n#&gt; 재귀방법 현재 값:  24.73863\n#&gt; [1] 24.73863",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#map-reduce-apply",
    "href": "functions_purrr.html#map-reduce-apply",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.4 Map(), Reduce()와 *apply() 함수",
    "text": "17.4 Map(), Reduce()와 *apply() 함수\n함수를 인자로 받는 함수를 고차함수(High-order function)라고 부른다. 대표적으로 Map(), Reduce()가 있다. 숫자 하나가 아닌 벡터에 대한 제곱근을 구하기 위해서 Map 함수를 사용한다. 2 3\n\n# 제곱근 함수 -------------------------------------------\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\n# 벡터에 대한 제곱근 계산 \n\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nMap(find_root_recur, numbers, init=1, eps = 10^-10)\n#&gt; [[1]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 6\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 8\n#&gt; \n#&gt; [[6]]\n#&gt; [1] 9\n\n숫자 하나를 받는 함수가 아니라, 벡터를 인자로 받아 제곱근을 계산하는 함수를 작성할 경우 함수 내부에서 함수를 인자로 받을 수 있도록 Map 함수를 활용한다.\n\n# `Map` 벡터 제곱근 계산\n\nfind_vec_root_recur &lt;- function(numbers, init, eps = 10^(-10)){\n    return(Map(find_root_recur, numbers, init, eps))\n}\n\nnumbers_z &lt;- c(9, 16, 25, 49, 121)\nfind_vec_root_recur(numbers_z, init=1, eps=10^(-10))\n#&gt; [[1]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 11\n\n이러한 패턴이 많이 활용되어 *apply 함수가 있어, 이전에 많이 사용했을 것이다. 벡터를 인자로 먼저 넣고, 함수명을 두번째 인자로 넣고, 함수에 들어갈 매개변수를 순서대로 쭉 나열하여 lapply, sapply 함수에 넣는다.\n\n# `lapply` 활용 제급근 계산\n\nlapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n#&gt; [[1]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 11\nsapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n#&gt; [1]  3  4  5  7 11\n\nReduce 함수도 삶을 편안하게 할 수 있는, 루프를 회피하는 또다른 방법이다. 이름에서 알 수 있듯이 numbers_z 벡터 원소 각각에 대해 해당 연산작업 +, %%을 수행시킨다. %%는 나머지 연산자로 기본디폴트 설정으로 \\(\\frac{10}{7}\\)로 몫 대신에 나머지 3을 우선 계산하고, 그 다음으로 \\(\\frac{3}{5}\\)로 최종 나머지 3을 순차적으로 계산하여 결과를 도출한다.\n\n# Reduce ----------------------------------------------\nnumbers_z\n#&gt; [1]   9  16  25  49 121\nReduce(`+`, numbers_z)\n#&gt; [1] 220\n\nnumbers_z &lt;- c(10,7,5)\nReduce(`%%`, numbers_z)\n#&gt; [1] 3",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr",
    "href": "functions_purrr.html#functional-programming-purrr",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.5 purrr 팩키지",
    "text": "17.5 purrr 팩키지\n*apply 계열 함수는 각각의 자료형에 맞춰 기억하기가 쉽지 않아, 매번 도움말을 찾아 확인하고 코딩을 해야하는 번거러움이 많다. 데이터 분석을 함수형 프로그래밍 패러다임으로 실행하도록 purrr 팩키지가 개발되었다. 이를 통해 데이터 분석 작업이 수월하게 되어 저녁이 있는 삶이 길어질 것으로 기대된다.\n\n17.5.1 purrr 헬로월드\npurrr 팩키지를 불러와서 map_dbl() 함수에 구문에 맞게 작성하면 동일한 결과를 깔끔하게 얻을 수 있다. 즉,\n\n\nmap_dbl(): 벡터, 데이터프레임, 리스트에 대해 함수를 원소별로 적용시켜 결과를 double 숫자형으로 출력시킨다.\n\nnumbers: 함수를 각 원소별로 적용시킬 벡터 입력값\n\nfind_root_recur: 앞서 작성한 뉴톤 방법으로 제곱근을 수치적으로 구하는 사용자 정의함수.\n\ninit=1, eps = 10^-10: 뉴톤 방법을 구현한 사용자 정의함수에 필요한 초기값.\n\n\nlibrary(purrr)\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nmap_dbl(numbers, find_root_recur, init=1, eps = 10^-10)\n#&gt; [1] 4 5 6 7 8 9",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-read-iris",
    "href": "functions_purrr.html#functional-programming-purrr-read-iris",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.6 병렬 데이터 분석",
    "text": "17.6 병렬 데이터 분석\n구글 검색을 통해서 쉽게 iris(붓꽃) 데이터를 구할 수 있다. 이를 불러와서 각 종별로 setosa versicolor, virginica로 나눠 로컬 .csv 파일로 저장하고 나서 이를 다시 불러오는 사례를 함수형 프로그래밍으로 구현해본다.\n\n\n붓꽃 데이터 불러오기\n\n먼저 iris.csv 파일을 R로 불러와서 각 종별로 나눠서 iris_종명.csv 파일형식으로 저장시킨다.\n\nlibrary(tidyverse)\niris_df &lt;- read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\")\n\niris_species &lt;- iris_df %&gt;% \n  count(species) %&gt;% pull(species)\n\nfor(i in 1:nrow(iris_df)) {\n  tmp_df &lt;- iris_df %&gt;% \n    filter(species == iris_species[i])\n  species_name &lt;- iris_species[i]\n  tmp_df %&gt;% write_csv(paste0(\"data/iris_\", species_name, \".csv\"))\n}\n\nSys.glob(\"data/iris_*.csv\")\n#&gt; [1] \"data/iris_NA.csv\"         \"data/iris_setosa.csv\"    \n#&gt; [3] \"data/iris_versicolor.csv\" \"data/iris_virginica.csv\"\n\n로컬 파일 iris_종명.csv 형식으로 저장된 데이터를 함수형 프로그래밍을 통해 불러와서 분석작업을 수행해보자. map() 함수를 사용해서 각 종별로 데이터를 깔끔하게 불러왔다.\niris_filename 벡터에 iris_종명.csv과 경로명이 포함된 문자열을 저장시켜 놓고 read_csv() 함수를 각 벡터 원소에 적용시켜 출력값으로 리스트 iris_list 객체를 생성시켰다.\n\niris_filename &lt;- c(\"data/iris_setosa.csv\", \"data/iris_versicolor.csv\", \"data/iris_virginica.csv\")\n\niris_list &lt;- map(iris_filename, read_csv) %&gt;% \n  set_names(iris_species)\n\niris_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name       value              \n#&gt;   &lt;chr&gt;      &lt;list&gt;             \n#&gt; 1 setosa     &lt;spc_tbl_ [50 × 5]&gt;\n#&gt; 2 versicolor &lt;spc_tbl_ [50 × 5]&gt;\n#&gt; 3 virginica  &lt;spc_tbl_ [50 × 5]&gt;\n\niris_list 각 원소는 데이터프레임이라 summary 함수를 사용해서 기술 통계량을 구할 수도 있다. 물론 cor() 함수를 사용해서 iris_list의 각 원소를 지정하는 .x 여기서는 종별 데이터프레임에서 변수 두개를 추출하여 sepal_length, sepal_width 이 둘간의 스피커만 상관계수를 계산하는데 출력값이 double 연속형이라 map_dbl로 저정하여 작업시킨다.\n\nmap(iris_list, summary)\n#&gt; $setosa\n#&gt;   sepal_length    sepal_width     petal_length    petal_width   \n#&gt;  Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n#&gt;  1st Qu.:4.800   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.200  \n#&gt;  Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n#&gt;  Mean   :5.006   Mean   :3.418   Mean   :1.464   Mean   :0.244  \n#&gt;  3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n#&gt;  Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt;                    \n#&gt; \n#&gt; $versicolor\n#&gt;   sepal_length    sepal_width     petal_length   petal_width   \n#&gt;  Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000  \n#&gt;  1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200  \n#&gt;  Median :5.900   Median :2.800   Median :4.35   Median :1.300  \n#&gt;  Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326  \n#&gt;  3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500  \n#&gt;  Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt;                    \n#&gt; \n#&gt; $virginica\n#&gt;   sepal_length    sepal_width     petal_length    petal_width   \n#&gt;  Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n#&gt;  1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n#&gt;  Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n#&gt;  Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n#&gt;  3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n#&gt;  Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt; \n\nmap_dbl(iris_list, ~cor(.x$sepal_length, .x$sepal_width, method = \"spearman\"))\n#&gt;     setosa versicolor  virginica \n#&gt;  0.7686085  0.5176060  0.4265165",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-random-number",
    "href": "functions_purrr.html#functional-programming-purrr-random-number",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.7 표본추출",
    "text": "17.7 표본추출\n서로 다른 난수를 생성시키는 방법을 살펴보자. 정규분포를 가정하고 평균과 표준편차를 달리하는 모수를 지정하고 난수갯수도 숫자를 달리하여 난수를 생성시킨다.\n\n17.7.1 \\(\\mu\\) 평균 변화\n정규분포에서 난수를 10개 추출하는데 표준편차는 1로 고정시키고, 평균만 달리한다. 평균만 달리하기 때문에 map() 함수를 그대로 사용한다. 즉, 입력값으로 평균만 달리하는 리스트를 입력값으로 넣는다.\n\n## 평균을 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\n\nsim_mu_name &lt;- paste0(\"mu: \", normal_mean)\n\nsim_mu_list &lt;- map(normal_mean, ~ data.frame(mean = .x, \n                            random_number = rnorm(mean=.x, sd=1, n=10))) %&gt;% \n  set_names(sim_mu_name)\n\nmap_dbl(sim_mu_list, ~mean(.x$random_number))\n#&gt;     mu: 1     mu: 5    mu: 10 \n#&gt; 0.9774252 5.1067705 9.6557945\n\nsim_mu_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name   value        \n#&gt;   &lt;chr&gt;  &lt;list&gt;       \n#&gt; 1 mu: 1  &lt;df [10 × 2]&gt;\n#&gt; 2 mu: 5  &lt;df [10 × 2]&gt;\n#&gt; 3 mu: 10 &lt;df [10 × 2]&gt;\n\n\n17.7.2 \\(\\mu\\) 평균, \\(\\sigma\\) 표준편차\n난수갯수만 고정시키고 평균과 표준편차를 달리하여 난수를 정규분포에서 추출한다. 입력값으로 평균과 표준편차 두개가 되기 때문에 map2() 함수를 사용한다.\n\n## 평균과 표준편차를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\n\nsim_mu_sd_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd)\n\nsim_mu_sd_list &lt;- map2(normal_mean, normal_sd, \n                        ~ data.frame(mean = .x, sd = .y,\n                            random_number = rnorm(mean=.x, sd=.y, n=10))) %&gt;% \n  set_names(sim_mu_sd_name)\n\nmap_dbl(sim_mu_sd_list, ~sd(.x$random_number))\n#&gt; mu: 1,  sd: 10  mu: 5,  sd: 5 mu: 10,  sd: 1 \n#&gt;      7.4495526      6.2588362      0.7741449\n\nsim_mu_sd_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name           value        \n#&gt;   &lt;chr&gt;          &lt;list&gt;       \n#&gt; 1 mu: 1,  sd: 10 &lt;df [10 × 3]&gt;\n#&gt; 2 mu: 5,  sd: 5  &lt;df [10 × 3]&gt;\n#&gt; 3 mu: 10,  sd: 1 &lt;df [10 × 3]&gt;\n\n\n17.7.3 \\(\\mu\\), \\(\\sigma\\), 표본크기\n\\(\\mu\\) 평균, \\(\\sigma\\) 표준편차, 표본크기를 모두 다르게 지정하여 난수를 추출한다. 이런 경우 pmap() 함수를 사용하고 입력 리스트가 다수라 이를 normal_list로 한번더 감싸서 이름이 붙은 리스트(named list)형태로 넣어주고, 이를 function() 함수의 내부 인수로 사용한다.\n\n## 평균, 표준편차, 표본크기를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\nnormal_size &lt;- list(10,20,30)\n\nsim_mu_sd_size_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd,\n                              \"  size: \", normal_size)\n\nnormal_list &lt;- list(normal_mean=normal_mean, normal_sd=normal_sd, normal_size=normal_size)\n\nsim_mu_sd_size_list &lt;- pmap(normal_list,\n                            function(normal_mean, normal_sd, normal_size)\n                        data.frame(mean=normal_mean, sd = normal_sd, size = normal_size,\n                            random_number = rnorm(mean=normal_mean, sd=normal_sd, n=normal_size))) %&gt;% \n  set_names(sim_mu_sd_size_name)\n\nmap_dbl(sim_mu_sd_size_list, ~length(.x$random_number))\n#&gt; mu: 1,  sd: 10  size: 10  mu: 5,  sd: 5  size: 20 mu: 10,  sd: 1  size: 30 \n#&gt;                       10                       20                       30\n\nsim_mu_sd_size_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name                     value        \n#&gt;   &lt;chr&gt;                    &lt;list&gt;       \n#&gt; 1 mu: 1,  sd: 10  size: 10 &lt;df [10 × 4]&gt;\n#&gt; 2 mu: 5,  sd: 5  size: 20  &lt;df [20 × 4]&gt;\n#&gt; 3 mu: 10,  sd: 1  size: 30 &lt;df [30 × 4]&gt;",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-ggplot",
    "href": "functions_purrr.html#functional-programming-purrr-ggplot",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.8 ggplot 시각화",
    "text": "17.8 ggplot 시각화\nlist-column을 활용하여 티블(tibble) 데이터프레임에 담아서 시각화를 진행해도 되고, 다른 방법으로 리스트에 담아서 이를 한장에 찍는 것도 가능하다. 4\n\nlibrary(gapminder)\n\n## 데이터 -----\nthree_country &lt;-  c(\"Korea, Rep.\", \"Japan\", \"China\")\n\ngapminder_tbl &lt;- gapminder %&gt;% \n  filter(str_detect(continent, \"Asia\")) %&gt;% \n  group_by(continent, country) %&gt;% \n  nest() %&gt;% \n  select(-continent) %&gt;% \n  filter(country %in% three_country )\n\n## 티블 데이터 시각화 -----\ngapminder_plot_tbl &lt;- gapminder_tbl %&gt;% \n  mutate(graph = map2(data, country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y)))\n\ngapminder_plot_tbl\n#&gt; # A tibble: 3 × 4\n#&gt; # Groups:   continent, country [3]\n#&gt;   continent country     data              graph \n#&gt;   &lt;fct&gt;     &lt;fct&gt;       &lt;list&gt;            &lt;list&gt;\n#&gt; 1 Asia      China       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n#&gt; 2 Asia      Japan       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n#&gt; 3 Asia      Korea, Rep. &lt;tibble [12 × 4]&gt; &lt;gg&gt;\n\n## 리스트 데이터 시각화 -----\ngapminder_plot &lt;- map2(gapminder_tbl$data , three_country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y))\n\nwalk(gapminder_plot, print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 리스트 데이터 시각화 - 한장에 찍기 -----\ncowplot::plot_grid(plotlist = gapminder_plot)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-theory-practice",
    "href": "functions_purrr.html#fp-theory-practice",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.9 함수형 프로그래밍 이론과 실제",
    "text": "17.9 함수형 프로그래밍 이론과 실제\n함수는 다음과 같이 될 수도 있어 함수형 프로그래밍 언어가 된다. 5\n\n함수의 인자\n함수로 반환\n리스트에 저장\n변수에 저장\n무명함수\n조작할 수 있다.\n\nFirstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do anything with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.\n\n\n\n\n\n\nJohn Chambers 창시자가 말하는 R 계산의 기본원칙\n\n\n\n\n존재하는 모든 것은 객체다. (Everything that exists is an object.)\n일어나는 모든 것은 함수호출이다. (Everything that happens is a function call.)\n\n\nlibrary(tidyverse)\nclass(`%&gt;%`)\n#&gt; [1] \"function\"\nclass(`$`)\n#&gt; [1] \"function\"\nclass(`&lt;-`)\n#&gt; [1] \"function\"\nclass(`+`)\n#&gt; [1] \"function\"",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#pure-vs-impure-function",
    "href": "functions_purrr.html#pure-vs-impure-function",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.10 순수한 함수 vs 불순한 함수",
    "text": "17.10 순수한 함수 vs 불순한 함수\n순수한 함수(pure function)는 입력값에만 출력값이 의존하게 되는 특성과 부수효과(side-effect)를 갖지 않는 반면 순수하지 않은 함수(impure function)는 환경에 의존하며 부수효과도 갖는다.\n\n\n순수한 함수(pure function)\n\nmin(1:100)\n#&gt; [1] 1\n\nmean(1:100)\n#&gt; [1] 50.5\n\n\n순수하지 않은 함수(impure function)\n\nSys.time()\n#&gt; [1] \"2024-03-20 09:31:29 KST\"\n\nrnorm(10)\n#&gt;  [1] -1.4959541 -0.2486522 -0.4523397  1.1723539 -0.2992888 -1.0520736\n#&gt;  [7]  0.7389625  0.4706090 -0.7900775  0.8746797\n\n# write_csv(\"data/sample.csv\")\n\n\n\n\n17.10.1 무명함수와 매퍼\n\\(\\lambda\\) (람다) 함수는 무명(anonymous) 함수는 함수명을 갖는 일반적인 함수와 비교하여 함수의 좋은 점은 그대로 누리면서 함수가 많아 함수명으로 메모리가 난잡하게 지져분해지는 것을 막을 수 있다.\n무명함수로 기능르 구현한 후에 매퍼(mapper)를 사용해서 as_mapper() 명칭을 부여하여 함수처럼 사용하는 것도 가능하다. 매퍼(mapper)를 사용하는 이유를 다음과 같이 정리할 수 있다.\n\n간결함(Concise)\n가독성(Easy to read)\n재사용성(Reusable)\n\n정치인 페이스북 페이지에서 팬수를 추출한다. 그리고 이를 이름이 부은 리스트(named list)로 일자별 팬수 추이를 리스트로 준비한다. 그리고 나서 안철수, 문재인, 심상정 세 후보에 대한 최고 팬수증가를 무명함수로 계산한다.\n\nlibrary(tidyverse)\n## 데이터프레임을 리스트로 변환\nahn_df  &lt;- read_csv(\"data/fb_ahn.csv\")  %&gt;% rename(fans = ahn_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nmoon_df &lt;- read_csv(\"data/fb_moon.csv\") %&gt;% rename(fans = moon_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nsim_df  &lt;- read_csv(\"data/fb_sim.csv\")  %&gt;% rename(fans = sim_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\n\nconvert_to_list &lt;- function(df) {\n  df_fans_v &lt;- df$fans %&gt;% \n    set_names(df$fdate)\n  return(df_fans_v)\n}\n\nahn_v  &lt;- convert_to_list(ahn_df)\nmoon_v &lt;- convert_to_list(moon_df)\nsim_v  &lt;- convert_to_list(sim_df)\n\nfans_lst &lt;- list(ahn_fans  = ahn_v,\n                 moon_fans = moon_v,\n                 sim_fans  = sim_v)\n\nlistviewer::jsonedit(fans_lst)\n\n\n\n\n\n## 무명함수 테스트\nmap_dbl(fans_lst, ~max(.x))\n#&gt;  ahn_fans moon_fans  sim_fans \n#&gt;       796      1464      2029\n\nrlang_lambda_function 무명함수로 increase_1000_fans 작성해서 일별 팬수 증가가 1000명 이상인 경우 keep() 함수를 사용해서 각 후보별로 추출할 수 있다. discard() 함수를 사용해서 반대로 버려버릴 수도 있다.\n\nincrease_1000_fans &lt;- as_mapper( ~.x &gt; 1000)\n\nmap(fans_lst, ~keep(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; named numeric(0)\n#&gt; \n#&gt; $moon_fans\n#&gt; 2017-03-28 2017-04-18 2017-04-20 \n#&gt;       1464       1310       1093 \n#&gt; \n#&gt; $sim_fans\n#&gt; 2017-03-12 2017-03-13 2017-04-14 2017-04-19 2017-04-20 2017-04-21 2017-04-24 \n#&gt;       1301       1079       1070       1441       1190       1025       1948 \n#&gt; 2017-04-25 \n#&gt;       2029\n\n술어논리(predicate logic)은 조건을 테스트하여 참(TRUE), 거짓(FALSE)을 반환시킨다. every, some을 사용하여 팬수가 증가한 날이 매일 1,000명이 증가했는지, 전부는 아니고 일부 특정한 날에 1,000명이 증가했는지 파악할 수 있다.\n\n## 세후보 팬수가 매일 모두 1000명 이상 증가했나요?\nmap(fans_lst, ~every(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $moon_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $sim_fans\n#&gt; [1] FALSE\n## 세후보 팬수가 전체는 아니고 일부 특정한 날에 1000명 이상 증가했나요?\nmap(fans_lst, ~some(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $moon_fans\n#&gt; [1] TRUE\n#&gt; \n#&gt; $sim_fans\n#&gt; [1] TRUE\n\n\n17.10.2 고차 함수(High order function)\n고차 함수(High order function)는 함수의 인자로 함수를 받아 함수로 반환시키는 함수를 지칭한다. high_order_fun 함수는 함수를 인자(func)로 받아 함수를 반환시키는 고차함수다. 평균 함수(mean)를 인자로 넣어 출력값으로 mean_na() 함수를 새롭게 생성시킨다. NA가 포함된 벡터를 넣어 평균값을 계산하게 된다.\n\nhigh_order_fun &lt;- function(func){\n  function(...){\n    func(..., na.rm = TRUE)\n  }\n}\n\nmean_na &lt;- high_order_fun(mean)\nmean_na( c(NA, 1:10) )\n#&gt; [1] 5.5\n\n벡터가 입력값으로 들어가서 벡터가 출력값으로 나오는 보통 함수(Regular Function)외에 고차함수는 3가지 유형이 있다.\n\n벡터 → 함수: 함수공장(Function Factory)\n함수 → 벡터: Functional - for루프를 purrr 팩키지 map() 함수로 대체\n함수 → 함수: 함수연산자(Function Operator) - Functional과 함께 사용될 경우 adverbs로서 강력한 기능을 발휘\n\n\n\n고차함수 유형\n\n\n17.10.3 부사 - safely, possibly,…\npurrr 팩키지의 대표적인 부사(adverbs)에는 possibly()와 safely()가 있다. 그외에도 silently(), surely() 등 다른 부사도 있으니 필요한 경우 purrr 팩키지 문서를 참조한다.\nsafely(mean)은 동사 함수(mean())를 받아 부사 safely()로 “부사 + 동사”로 기능이 추가된 부사 동사를 반환시킨다. 따라서, NA가 추가된 벡터를 넣을 경우 $result와 $error를 원소로 갖는 리스트를 반환시킨다.\n\nmean_safe &lt;- safely(mean)\nclass(mean_safe)\n#&gt; [1] \"function\"\n\nmean_safe(c(NA, 1:10))\n#&gt; $result\n#&gt; [1] NA\n#&gt; \n#&gt; $error\n#&gt; NULL\n\n이를 활용하여 오류처리작업을 간결하게 수행시킬 수 있다. $result와 $error을 원소로 갖는 리스트를 반환시키기 때문에 오류와 결과값을 추출하여 후속작업을 수행하여 디버깅하는데 유용하게 활용할 수 있다.\n\ntest_lst &lt;- list(\"NA\", 1,2,3,4,5)\nlog_safe &lt;- safely(log)\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"result\")\n#&gt; [[1]]\n#&gt; NULL\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0.6931472\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 1.098612\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 1.386294\n#&gt; \n#&gt; [[6]]\n#&gt; [1] 1.609438\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"error\")\n#&gt; [[1]]\n#&gt; &lt;simpleError in .Primitive(\"log\")(x, base): non-numeric argument to mathematical function&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; NULL\n#&gt; \n#&gt; [[3]]\n#&gt; NULL\n#&gt; \n#&gt; [[4]]\n#&gt; NULL\n#&gt; \n#&gt; [[5]]\n#&gt; NULL\n#&gt; \n#&gt; [[6]]\n#&gt; NULL\n\n반면에 possibly()는 결과와 otherwise 값을 반환시켜서 오류가 발생되면 중단되는 것이 아니라 오류가 있다는 사실을 알고 예외처리시킨 후에 쭉 정상진행시킨다.\n\nmax_possibly &lt;- possibly(sum, otherwise = \"watch out\")\n\nmax_possibly(c(1:10))\n#&gt; [1] 55\n\nmax_possibly(c(NA, 1:10))\n#&gt; [1] NA\n\nmax_possibly(c(\"NA\", 1:10))\n#&gt; [1] \"watch out\"\n\npossibly()는 부울 논리값, NA, 문자열, 숫자를 반환시킬 수 있다.\ntranspose()와 결합하여 safely(), possibly() 결과를 변형시킬 수도 있다.\n\nmap(test_lst, log_safe) %&gt;% length()\n#&gt; [1] 6\n\nmap(test_lst, log_safe) %&gt;% transpose() %&gt;% length()\n#&gt; [1] 2\n\ncompact()를 사용해서 NULL을 제거하는데, 앞서 possibly()의 인자로 otherwise=를 지정하는 경우 otherwise=NULL와 같이 정의해서 예외처리로 NULL을 만들어 내고 compact()로 정상처리된 데이터만 얻는 작업흐름을 갖춘다.\n\nnull_lst &lt;- list(1, NULL, 3, 4, NULL, 6, 7, NA)\ncompact(null_lst)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 6\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[6]]\n#&gt; [1] NA\n\npossibly_log &lt;- possibly(log, otherwise = NULL)\nmap(null_lst, possibly_log) %&gt;% compact()\n#&gt; [[1]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1.098612\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 1.386294\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 1.791759\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 1.94591\n#&gt; \n#&gt; [[6]]\n#&gt; [1] NA",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-clean-code",
    "href": "functions_purrr.html#fp-clean-code",
    "title": "17  함수형 프로그래밍",
    "section": "\n17.11 깨끗한 코드",
    "text": "17.11 깨끗한 코드\nround_mean() 함수를 compose() 함수를 사용해서 mean() 함수로 평균을 구한 후에 round()함수로 반올림하는 코드를 다음과 같이 쉽게 작성할 수 있다. 6\n\nround_mean &lt;- compose(round, mean)\nround_mean(1:10)\n#&gt; [1] 6\n\n두번째 사례로 전형적인 데이터 분석 사례로 lm() → anova() → tidy()를 통해 한방에 선형회귀 모형 산출물을 깨끗한 코드로 작성하는 사례를 살펴보자.\nmtcars 데이터셋에서 연비 예측에 변수 두개를 넣고 일반적인 lm() 선형예측모형 제작방식과 동일하게 인자를 넣는다.\n\nclean_lm &lt;- compose(broom::tidy, anova, lm)\nclean_lm(mpg ~ hp + wt, data=mtcars)\n#&gt; # A tibble: 3 × 6\n#&gt;   term         df sumsq meansq statistic   p.value\n#&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 hp            1  678. 678.       101.   5.99e-11\n#&gt; 2 wt            1  253. 253.        37.6  1.12e- 6\n#&gt; 3 Residuals    29  195.   6.73      NA   NA\n\ncompose()를 통해 함수를 조합하는 경우 함수의 인자를 함께 전달해야될 경우가 있다. 이와 같은 경우 partial()을 사용해서 인자를 넘기는 함수를 제작하여 compose()에 넣어준다.\n\nrobust_round_mean &lt;- compose(\n  partial(round, digits=1),\n  partial(mean, na.rm=TRUE))\nrobust_round_mean(c(NA, 1:10))\n#&gt; [1] 5.5\n\n리스트 칼럼(list-column)과 결합하여 모형에서 나온 데이터 분석결과를 깔끔하게 코드로 제작해보자. 먼저 lm을 돌려 모형 요약하는 함수 summary를 통해 r.squared값을 추출하는 함수를 summary_lm으로 제작한다.\n그리고 나서 nest() 함수로 리스트 칼럼(list-column)을 만들고 두개의 집단 수동/자동을 나타내는 am 변수를 그룹으로 삼아 두 집단에 속한 수동/자동 데이터에 대한 선형 회귀모형을 적합시키고 나서 “r.squared”값을 추출하여 이를 티블 데이터프레임에 저장시킨다.\n\nsummary_lm &lt;- compose(summary, lm) \n\nmtcars %&gt;%\n  group_by(am) %&gt;%\n  nest() %&gt;%\n  mutate(lm_mod = map(data, ~ summary_lm(mpg ~ hp + wt, data = .x)),\n         r_squared = map(lm_mod, \"r.squared\")) %&gt;%\n  unnest(r_squared)\n#&gt; # A tibble: 2 × 4\n#&gt; # Groups:   am [2]\n#&gt;      am data               lm_mod     r_squared\n#&gt;   &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n#&gt; 1     1 &lt;tibble [13 × 10]&gt; &lt;smmry.lm&gt;     0.837\n#&gt; 2     0 &lt;tibble [19 × 10]&gt; &lt;smmry.lm&gt;     0.768",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#footnotes",
    "href": "functions_purrr.html#footnotes",
    "title": "17  함수형 프로그래밍",
    "section": "",
    "text": "Bruno Rodrigues(2016), “Functional programming and unit testing for data munging with R”, LeanPub, 2016-12-23↩︎\npurrr tutorial: Lessons and Examples↩︎\npurrr tutorial GitHub Webpage↩︎\nVery statisticious (August 20, 2018), “Automating exploratory plots with ggplot2 and purrr”↩︎\nAdvanced R, “Introduction”↩︎\nColin Fay, “A Crazy Little Thing Called {purrr} - Part 5: code optimization”↩︎",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "function_plyr.html",
    "href": "function_plyr.html",
    "title": "15  분할-적용-병합 전략",
    "section": "",
    "text": "15.1 gapminder 데이터셋\ngapminder 데이터 팩키지의 각 대륙, 국가, 연도별 인구와 중요 두가지 정보인 평균수명과 일인당GDP 정보를 바탕으로 각 대륙별 평균 GDP를 추출해보자. 이를 위해서 먼저 인당GDP(gdpPercap)와 인구수(pop)를 바탕으로 GDP를 계산하고 이를 평균낸다. 특정 연도를 지칭하지 않는 것이 다소 문제의 소지가 있을 수 있지만, 분할-적용-병합 전략을 살펴보는데 큰 무리는 없어 보인다.\nlibrary(tidyverse)\nlibrary(gapminder)\n\ncalc_GDP &lt;- function(dat, year=NULL, country=NULL) {\n  if(!is.null(year)) {\n    dat &lt;- dat[dat$year %in% year, ]\n  }\n  if (!is.null(country)) {\n    dat &lt;- dat[dat$country %in% country,]\n  }\n  gdp &lt;- dat$pop * dat$gdpPercap\n\n  new &lt;- cbind(dat, gdp=gdp)\n  return(new)\n}\n\ngapminder %&gt;% \n  mutate(GDP = pop * gdpPercap) %&gt;% \n  DT::datatable()",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#dplyr-패키지",
    "href": "function_plyr.html#dplyr-패키지",
    "title": "\n15  분할-적용-병합 전략\n",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n%&gt;% 파이프 연산자를 사용하여 calcGDP(gapminder) 함수의 결과를 다음 단계로 전달했다.\n\ngroup_by(continent)를 사용하여 데이터를 대륙별로 그룹화했다.\n\nsummarise()를 사용하여 각 그룹에 대해 mean(gdp)를 계산하고, 결과를 mean_gdp라는 새로운 열에 저장했다.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n도전과제\n\n\n\n대륙별로 평균 기대수명을 계산해보자. 어느 대륙의 기대수명이 가장 길고, 어느 대륙이 가장 짧은가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\ndplyr의 함수를 사용하여 도전과제 2의 출력 결과로부터, 1952년에서 2007년 사이의 평균 기대수명 차이를 계산해보자.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n실제로 실행하지 말고, 다음 중 어떤 코드가 대륙별 평균 기대수명을 계산하는지 살펴보자.\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(gapminder$continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  filter(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n답안 3은 대륙별 평균 기대 수명을 계산한다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#split-apply-combine-histroy",
    "href": "function_plyr.html#split-apply-combine-histroy",
    "title": "\n15  분할-적용-병합 전략\n",
    "section": "\n15.2 분할-적용-병합 전략 진화",
    "text": "15.2 분할-적용-병합 전략 진화\n데이터 분석에서 흔히 사용되는 방법 중 하나가 분할-적용-병합(Split-Apply-Combine) 전략(Wickham 2011)이다. 즉, 큰 문제를 작은 문제로 쪼개고 각 문제에 대해서 적절한 연산(예를 들어 요약통계량)을 취하고 이를 결합하는 방법이 많이 사용되는 방법이다. 예를 들어 재현가능한 과학적 분석을 위한 R - “분할-적용-병합 전략”에 나온 것처럼 각 그룹별로 쪼갠 후에 각 그룹별로 평균을 내고 이를 조합한 사례가 전반적인 큰 그림을 그리는데 도움이 될 수 있다. 여기서 평균을 사용했는데 요약(summarize) 뿐만 아니라, 윈도우 함수, 이를 일반화한 do() 함수도 포함된다.\n\n\n\n\n\n그림 15.2: 분할-적용-병합 작업과정 도식화\n\n\n\n선사시대 Split-Apply-Combine: split, lapply, do.call(rbind, …)\n석기시대(plyr) Split-Apply-Combine: plyr::ddply\n초기 tidyverse 시대 Split-Apply-Combine: group_by, do\n중기 tidyverse 시대 Split-Apply-Combine: group_by & by_slice\n현대 tidyverse 시대 Split-Apply-Combine: group_by, nest, mutate(map())\ntidyverse/base 하이브리드 조합 Split-Apply-Combine: split, map_dfr\n\n\n15.2.1 gapminder 데이터셋\ngapminder 데이터 팩키지의 각 대륙, 국가, 연도별 인구와 중요 두가지 정보인 평균수명과 일인당GDP 정보를 바탕으로 각 대륙별 평균 GDP를 추출해보자. 이를 위해서 먼저 인당GDP(gdpPercap)와 인구수(pop)를 바탕으로 GDP를 계산하고 이를 평균낸다. 특정 연도를 지칭하지 않는 것이 다소 문제의 소지가 있을 수 있지만, 분할-적용-병합 전략을 살펴보는데 큰 무리는 없어 보인다.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ncalc_GDP &lt;- function(dat, year=NULL, country=NULL) {\n  if(!is.null(year)) {\n    dat &lt;- dat[dat$year %in% year, ]\n  }\n  if (!is.null(country)) {\n    dat &lt;- dat[dat$country %in% country,]\n  }\n  gdp &lt;- dat$pop * dat$gdpPercap\n\n  new &lt;- cbind(dat, gdp=gdp)\n  return(new)\n}\n\ngapminder %&gt;% \n  mutate(GDP = pop * gdpPercap) %&gt;% \n  DT::datatable()\n\n\n\n\n\n\n15.2.2 선사시대 split, lapply, do.call\n\n선사시대에는 대륙별로 split 한 후에 lapply() 함수를 사용해서 앞서 정의한 calc_GDP 함수로 GDP를 계산한 후에 평균을 다시 계산한 뒤에 마지막으로 do.call() 함수로 병합(combine)하여 GDP 대륙별 평균을 구할 수 있다.\n\ncontinent_split_df  &lt;- split(gapminder, gapminder$continent)\nGDP_list_df         &lt;- lapply(continent_split_df, calc_GDP)\nGDP_list_df         &lt;- lapply(GDP_list_df, function(x) mean(x$gdp))\nmean_GDP_df         &lt;- do.call(rbind, GDP_list_df)\n\nmean_GDP_df\n#&gt;                  [,1]\n#&gt; Africa    20904782844\n#&gt; Americas 379262350210\n#&gt; Asia     227233738153\n#&gt; Europe   269442085301\n#&gt; Oceania  188187105354\n\n\n15.2.3 plyr::ddply\n\n석기시대 plyr 팩키지 ddply 함수를 사용해서 각 대륙별로 쪼갠 후에 각 대륙별 평균 GDP를 구할 수 있다.\n\nlibrary(gapminder)\n\nplyr::ddply(\n .data = calc_GDP(gapminder),\n .variables = \"continent\",\n .fun = function(x) mean(x$gdp)\n)\n#&gt;   continent           V1\n#&gt; 1    Africa  20904782844\n#&gt; 2  Americas 379262350210\n#&gt; 3      Asia 227233738153\n#&gt; 4    Europe 269442085301\n#&gt; 5   Oceania 188187105354\n\n\n15.2.4 초기 tidyverse 시대: group_by, do\n\ngroup_by() + do()를 결합하여 임의 연산작업을 각 그룹별로 수행시킬 수 있다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  do(calc_GDP(.)) %&gt;%\n  do(out = mean(.$gdp)) %&gt;% \n  unnest\n#&gt; # A tibble: 5 × 2\n#&gt;   continent           out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n15.2.5 중기 tidyverse 시대: group_by, by_slice\n\ngroup_by() + by_slice()를 결합하여 분할-적용-병합 전략을 적용시킬 수도 있으나 by_slice() 함수가 dplyr::do() 함수와 같은 작업을 수행했고, purrrlyr에 갔다가… 그후 행방이 묘연해졌다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~calc_GDP(.x), .collate = 'rows') %&gt;% \n  select(continent, gdp) %&gt;% \n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~mean(.$gdp), .collate = 'rows')\n#&gt; # A tibble: 5 × 2\n#&gt;   continent          .out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n15.2.6 현대 tidyverse 시대: group_by, nest, mutate(map())\n\n현대 분할-적용-병합 전략은 group_by + nest()로 그룹별 데이터프레임으로 만들고, mutate(map())을 사용해서 calc_GDP() 함수를 적용시켜 GDP를 계산하고, summarize() 함수를 적용시켜 각 대륙별 평균 GDP를 계산한다. 마지막으로 unnest를 적용시켜 원하는 산출물을 얻는다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  nest() %&gt;% \n  mutate(data = purrr::map(data, calc_GDP)) %&gt;% \n  mutate(mean_GDP = purrr::map(data, ~ summarize(., mean_GDP = mean(gdp)))) %&gt;% \n  unnest(mean_GDP)\n#&gt; # A tibble: 5 × 3\n#&gt; # Groups:   continent [5]\n#&gt;   continent data                mean_GDP\n#&gt;   &lt;fct&gt;     &lt;list&gt;                 &lt;dbl&gt;\n#&gt; 1 Asia      &lt;df [396 × 6]&gt; 227233738153.\n#&gt; 2 Europe    &lt;df [360 × 6]&gt; 269442085301.\n#&gt; 3 Africa    &lt;df [624 × 6]&gt;  20904782844.\n#&gt; 4 Americas  &lt;df [300 × 6]&gt; 379262350210.\n#&gt; 5 Oceania   &lt;df [24 × 6]&gt;  188187105354.\n\n\n15.2.7 tidyverse/base 하이브리드 조합: split, map_dfr\n\n마지막으로 base R split() 함수와 map_dfr() 함수를 조합해서 원하는 결과를 얻어낼 수도 있다.\n\ngapminder %&gt;%\n  split(.$continent) %&gt;%\n  purrr::map_dfr(calc_GDP) %&gt;% \n  split(.$continent) %&gt;%\n  purrr::map_dfr(~mean(.$gdp)) %&gt;% \n  gather(continent, mean_GDP)\n#&gt; # A tibble: 5 × 2\n#&gt;   continent      mean_GDP\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n\n\n\nWickham, Hadley. 2011. “The split-apply-combine strategy for data analysis”. Journal of statistical software 40: 1–29.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#진화의-역사",
    "href": "function_plyr.html#진화의-역사",
    "title": "15  분할-적용-병합 전략",
    "section": "\n15.2 진화의 역사",
    "text": "15.2 진화의 역사\n\n15.2.1 선사시대 split, lapply, do.call\n\n선사시대에는 대륙별로 split 한 후에 lapply() 함수를 사용해서 앞서 정의한 calc_GDP 함수로 GDP를 계산한 후에 평균을 다시 계산한 뒤에 마지막으로 do.call() 함수로 병합(combine)하여 GDP 대륙별 평균을 구할 수 있다.\n\ncontinent_split_df  &lt;- split(gapminder, gapminder$continent)\nGDP_list_df         &lt;- lapply(continent_split_df, calc_GDP)\nGDP_list_df         &lt;- lapply(GDP_list_df, function(x) mean(x$gdp))\nmean_GDP_df         &lt;- do.call(rbind, GDP_list_df)\n\nmean_GDP_df\n#&gt;                  [,1]\n#&gt; Africa    20904782844\n#&gt; Americas 379262350210\n#&gt; Asia     227233738153\n#&gt; Europe   269442085301\n#&gt; Oceania  188187105354\n\n\n15.2.2 plyr::ddply\n\n석기시대 plyr 팩키지 ddply 함수를 사용해서 각 대륙별로 쪼갠 후에 각 대륙별 평균 GDP를 구할 수 있다.\n\nlibrary(gapminder)\n\nplyr::ddply(\n .data = calc_GDP(gapminder),\n .variables = \"continent\",\n .fun = function(x) mean(x$gdp)\n)\n#&gt;   continent           V1\n#&gt; 1    Africa  20904782844\n#&gt; 2  Americas 379262350210\n#&gt; 3      Asia 227233738153\n#&gt; 4    Europe 269442085301\n#&gt; 5   Oceania 188187105354\n\n\n15.2.3 초기 tidyverse 시대: group_by, do\n\ngroup_by() + do()를 결합하여 임의 연산작업을 각 그룹별로 수행시킬 수 있다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  do(calc_GDP(.)) %&gt;%\n  do(out = mean(.$gdp)) %&gt;% \n  unnest\n#&gt; # A tibble: 5 × 2\n#&gt;   continent           out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n15.2.4 중기 tidyverse 시대: group_by, by_slice\n\ngroup_by() + by_slice()를 결합하여 분할-적용-병합 전략을 적용시킬 수도 있으나 by_slice() 함수가 dplyr::do() 함수와 같은 작업을 수행했고, purrrlyr에 갔다가… 그후 행방이 묘연해졌다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~calc_GDP(.x), .collate = 'rows') %&gt;% \n  select(continent, gdp) %&gt;% \n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~mean(.$gdp), .collate = 'rows')\n#&gt; # A tibble: 5 × 2\n#&gt;   continent          .out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n15.2.5 현대 tidyverse 시대: group_by, nest, mutate(map())\n\n현대 분할-적용-병합 전략은 group_by + nest()로 그룹별 데이터프레임으로 만들고, mutate(map())을 사용해서 calc_GDP() 함수를 적용시켜 GDP를 계산하고, summarize() 함수를 적용시켜 각 대륙별 평균 GDP를 계산한다. 마지막으로 unnest를 적용시켜 원하는 산출물을 얻는다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  nest() %&gt;% \n  mutate(data = purrr::map(data, calc_GDP)) %&gt;% \n  mutate(mean_GDP = purrr::map(data, ~ summarize(., mean_GDP = mean(gdp)))) %&gt;% \n  unnest(mean_GDP)\n#&gt; # A tibble: 5 × 3\n#&gt; # Groups:   continent [5]\n#&gt;   continent data                mean_GDP\n#&gt;   &lt;fct&gt;     &lt;list&gt;                 &lt;dbl&gt;\n#&gt; 1 Asia      &lt;df [396 × 6]&gt; 227233738153.\n#&gt; 2 Europe    &lt;df [360 × 6]&gt; 269442085301.\n#&gt; 3 Africa    &lt;df [624 × 6]&gt;  20904782844.\n#&gt; 4 Americas  &lt;df [300 × 6]&gt; 379262350210.\n#&gt; 5 Oceania   &lt;df [24 × 6]&gt;  188187105354.\n\n\n15.2.6 tidyverse/base 하이브리드 조합: split, map_dfr\n\n마지막으로 base R split() 함수와 map_dfr() 함수를 조합해서 원하는 결과를 얻어낼 수도 있다.\n\ngapminder %&gt;%\n  split(.$continent) %&gt;%\n  purrr::map_dfr(calc_GDP) %&gt;% \n  split(.$continent) %&gt;%\n  purrr::map_dfr(~mean(.$gdp)) %&gt;% \n  gather(continent, mean_GDP)\n#&gt; # A tibble: 5 × 2\n#&gt;   continent      mean_GDP\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#gapminder-실습",
    "href": "function_plyr.html#gapminder-실습",
    "title": "15  분할-적용-병합 전략",
    "section": "\n15.3 gapminder 실습",
    "text": "15.3 gapminder 실습\n앞서, 함수를 사용해서 코드를 단순화하는 방법을 살펴봤다. gapminder 데이터셋을 인자로 받아, pop와 gdpPercap를 곱해 GDP를 계산하는 calcGDP 함수를 정의했다. 추가적인 인자를 정의해서, year별, country별 필터를 적용할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터 작업을 할 때, 흔히 마주치는 작업이 집단별로 그룹을 묶어 계산하는 것이다. 위에서는 단순히 두 열을 곱해서 GDP를 계산했다. 하지만, 대륙별로 평균 GDP를 계산하고자 한다면 어떻게 해야 할까?\ncalcGDP를 실행하고 나서, 각 대륙별로 평균을 산출해보자. 데이터셋을 가져와서 인구 열과 1인당 GDP 열을 곱합니다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n하지만, 그다지 멋있지는 않다. 그렇다. 함수를 사용해서 반복되는 작업을 상당량 줄일 수 있었다. 함수 사용은 멋있었다. 하지만, 여전히 반복이 있다. 직접 반복하게 되면, 지금은 물론이고 나중에도 시간을 까먹게 되고, 잠재적으로 버그가 스며들 여지를 남기게 된다.\ncalcGDP처럼 유연성 있는 함수를 새로 작성할 수도 있지만, 제대로 동작하는 함수를 개발하기까지 상당한 노력과 테스트가 필요하다.\n여기서 맞닥뜨린 추상화 문제를 “분할-적용-병합(split-apply-combine)” 전략이라고 한다.\n\n\n\n\n\n그림 15.2: 분할-적용-병합 전략 도식화\n\n\n데이터를 집단으로 분할(split)하고, 이번 경우에는 대륙별로 분할한다. 그리고 분할된 집단에 연산을 적용(apply)한 후, 선택적으로 결과를 묶어 병합(combine)한다.\n\n15.3.1 dplyr 패키지\ndplyr 패키지는 데이터 조작을 위한 문법을 제공하며, 이를 통해 “분할-적용-병합” 문제를 해결할 수 있다. 먼저 dplyr 패키지를 로드하고, dplyr을 사용하여 대륙별 평균 GDP를 빠르게 계산한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n방금 코드에서 일어난 일을 복기해 보자.\n\n\n%&gt;% 파이프 연산자를 사용하여 calcGDP(gapminder) 함수의 결과를 다음 단계로 전달했다.\n\ngroup_by(continent)를 사용하여 데이터를 대륙별로 그룹화했다.\n\nsummarise()를 사용하여 각 그룹에 대해 mean(gdp)를 계산하고, 결과를 mean_gdp라는 새로운 열에 저장했다.\n\n그룹화 변수를 추가하면 어떻게 될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ngroup_by()에 year를 추가하여 대륙과 연도별로 그룹화했다.\n\n\n\n\n\n\n도전과제\n\n\n\n대륙별로 평균 기대수명을 계산해보자. 어느 대륙의 기대수명이 가장 길고, 어느 대륙이 가장 짧은가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\ndplyr의 함수를 사용하여 도전과제 2의 출력 결과로부터, 1952년에서 2007년 사이의 평균 기대수명 차이를 계산해보자.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n이렇게 dplyr 패키지를 사용하면 데이터 조작과 분석을 보다 간결하고 직관적으로 수행할 수 있다. 파이프 연산자(%&gt;%)를 사용하여 연산을 연결하면 코드의 가독성도 향상된다. group_by(), summarise(), mutate() 등의 함수를 조합하여 다양한 분석 작업을 수행할 수 있다.알겠습니다. 계속해서 교정 작업을 진행하겠습니다.\n\n\n\n\n\n\n도전과제\n\n\n\n실제로 실행하지 말고, 다음 중 어떤 코드가 대륙별 평균 기대수명을 계산하는지 살펴보자.\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(gapminder$continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  filter(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n답안 3은 대륙별 평균 기대 수명을 계산한다.\n\n\n\n\n\n\n\n\n\nWickham, Hadley. 2011. “The split-apply-combine strategy for data analysis”. Journal of statistical software 40: 1–29.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  }
]
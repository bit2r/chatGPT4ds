[
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n20  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n21 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "21  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "22  챗GPT 자연어",
    "section": "",
    "text": "23 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n24 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25 Notable.ai\n\n\n26 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 자연어</span>"
    ]
  },
  {
    "objectID": "simpson.html",
    "href": "simpson.html",
    "title": "23  심슨의 역설",
    "section": "",
    "text": "24 심슨의 역설 사례\n심슨의 역설 사례를 책페이지수와 책가격의 관계를 살펴보자. 데이터는 책 유형(하드커버, 페이퍼백)은 두가지가 있고, 페이지수와 책가격이 달러로 구성된 데이터프레임이다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA",
    "href": "simpson.html#simpson-paradox-case-study-EDA",
    "title": "23  심슨의 역설",
    "section": "\n24.1 데이터 시각화",
    "text": "24.1 데이터 시각화\n이를 시각적으로 표현하면 관계가 음의 상관관계를 갖는 것을 알 수 있다.\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(extrafont)\nloadfonts()\n\nsimp_df &lt;- tribble(\n    ~book_type, ~num_pages, ~book_price,\n    \"hardcover\", 150, 27.43, \n    \"hardcover\", 225, 48.76, \n    \"hardcover\", 342, 50.25, \n    \"hardcover\", 185, 32.01, \n    \"paperback\", 475, 10.00, \n    \"paperback\", 834, 15.73, \n    \"paperback\", 1020, 20.00, \n    \"paperback\", 790, 17.89)\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE)",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "title": "23  심슨의 역설",
    "section": "\n24.2 기술통계량",
    "text": "24.2 기술통계량\nnum_pages, book_price 두변수를 추출하여 상관계수를 도출한다. 그리고 나서, 책 유형에 따른 상관관계도 도출해 낸다. 먼저 책 유형에 관계없이 num_pages, book_price 상관관계는 -0.5949366으로 나름 강한 음의 상관계수가 관측된다.\n\nsimp_df %&gt;% \n    summarise(book_cor = cor(num_pages, book_price)) %&gt;% \n    pull()\n#&gt; [1] -0.5949366\n\n이번에는 책 유형에 따른 상관계수는 어떤지 계산해 보자. 이 경우, 하드커버는 0.848, 페이퍼백은 0.956로 강한 양의 상관관계가 존재함이 확인된다.\n\nsimp_df %&gt;% \n    group_by(book_type) %&gt;% \n    summarise(book_cor = cor(num_pages, book_price))\n#&gt; # A tibble: 2 × 2\n#&gt;   book_type book_cor\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 hardcover    0.848\n#&gt; 2 paperback    0.956",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "title": "23  심슨의 역설",
    "section": "\n24.3 상관관계 시각화",
    "text": "24.3 상관관계 시각화\n앞서 확인한 결과를 책 유형별로 나눠 상관계수를 시각화한다.\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price, color=book_type)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"책페이지 수\", y=\"책가격($)\", title=\"심슨의 역설 사례\", color=\"책유형\" )+\n      theme(legend.position = \"top\")",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-berkeley",
    "href": "simpson.html#simpson-paradox-berkeley",
    "title": "23  심슨의 역설",
    "section": "\n25.1 UC 버클리 입학 2\n",
    "text": "25.1 UC 버클리 입학 2\n\n심슨의 역설관련 가장 유명한 사례는 1973년 UC 버클리 대학 입학데이터로 입학에 성차별이 존재하는지에 관한 데이터다.\n\n\n성별에 따른 입학률 비교\n\nlibrary(datasets)\nadmin_df &lt;- UCBAdmissions %&gt;% tbl_df\n\nadmin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n복사하여 붙여넣기\n\nadmin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n기술통계량을 통해 살펴본 사항을 그래프로 시각화한다. 막대 그래프를 통해 남성 합격률이 여성보다 높은 것으로 나타나 성차별이 존재하는 것으로 파악되지만, 학과별로 놓고 보면 여성 합격률이 더 높거나 남성과 유사한 것으로 시각적으로 나타난다.\n\n# A barplot for overall admission percentage for each gender.\n\nadmit_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, width = 0.2, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"성별\", y=\"입학합격율\", title=\"버클리 전체 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_minimal(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") \n\nadmit_dept_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    facet_grid(. ~ Dept) +\n    labs(x=\"성별\", y=\"\", title=\"버클리 학과별 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          legend.position = \"none\") \n\nplot_grid(admit_g, admit_dept_g, labels = \"\")",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#simpson-paradox-news-article-game-case",
    "href": "simpson.html#simpson-paradox-news-article-game-case",
    "title": "23  심슨의 역설",
    "section": "\n26.1 게임 업데이터 사례 3\n",
    "text": "26.1 게임 업데이터 사례 3\n\n예전에 모 게임에서 큰 규모의 업데이트를 한 후 게임 고객 동향을 분석한 적이 있습니다. 이 게임은 전체 게임 고객을 약 십 여가지 유형으로 분류하고 있는데, 크게 보면 게임 활동이 왕성하고 충성도가 높은 ‘진성’ 유형, 게임 활동이 그리 활발하지 않은 ‘라이트’ 유형, 자동 사냥 유저로 의심되는 ‘봇’ 유형 등이 있죠.\n이 게임의 업데이트 전/후 일별접속자수(DAU)와 유저당 결제금액(ARPU) 지표를 확인해 보니 아래와 같이 나왔습니다.\n\n일별 접속자수(DAU)가 크게 늘었지만 유저당 결재금액(ARPU)가 하락하여 뭔가 특단의 조치가 필요한 것으로 파악되지만, 이를 고객 집단을 반영하여 분석을 하게 되면 진성유저는 큰 차이가 없고, 크게 늘어난 유저가 봇이거나 Non-PU 유저라 봇을 비용으로 간주하여 제거하거나 Non-PU유저를 PU로 바꾸거나 PU 유저의 결재금액을 높이는 방향으로 사업적인 조치를 취하는 것이 바람직스러워 보인다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "simpson.html#footnotes",
    "href": "simpson.html#footnotes",
    "title": "23  심슨의 역설",
    "section": "",
    "text": "나무위키, “심슨의 역설”↩︎\nJohnny Hong(January 30, 2016), “A (very) brief introduction to ggplot2”↩︎\nNC소프트 (2017-07-05) “데이터 분석을 이용한 게임 고객 모델링 #4”↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>심슨의 역설</span>"
    ]
  },
  {
    "objectID": "basic_stat.html",
    "href": "basic_stat.html",
    "title": "\n24  통계\n",
    "section": "",
    "text": "24.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#기술통계",
    "href": "basic_stat.html#기술통계",
    "title": "\n24  통계\n",
    "section": "\n24.2 기술통계",
    "text": "24.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n\n\n\n\n24.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n#&gt; # A tibble: 1 × 2\n#&gt;   평균_체중 최빈종\n#&gt;       &lt;dbl&gt; &lt;fct&gt; \n#&gt; 1     4202. Adelie\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\nprint(f'펭귄 최빈종: {mode_species}')\n\n\n\n\n\n24.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n#&gt; # A tibble: 1 × 2\n#&gt;   분산_체중 표준편차_체중\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1   643131.          802.\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n#&gt; # A tibble: 3 × 2\n#&gt;   species   빈도수\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 Adelie       152\n#&gt; 2 Gentoo       124\n#&gt; 3 Chinstrap     68\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#가능성",
    "href": "basic_stat.html#가능성",
    "title": "\n24  통계\n",
    "section": "\n24.3 가능성",
    "text": "24.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n24.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n#&gt; [1] \"부산\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n#&gt;  [1] \"울산\" \"충북\" \"강원\" \"강원\" \"제주\" \"광주\" \"전남\" \"전남\" \"울산\" \"충북\"\n#&gt; [11] \"서울\" \"충북\" \"인천\" \"인천\" \"경기\" \"충남\" \"울산\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n#&gt; [1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n#&gt; [1] 0.0564\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n#&gt; 대전\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n#&gt; 경기\n#&gt; 제주\n#&gt; 대전\n#&gt; 경남\n#&gt; 서울\n#&gt; 울산\n#&gt; 충북\n#&gt; 대구\n#&gt; 강원\n#&gt; 울산\n#&gt; 강원\n#&gt; 경기\n#&gt; 전북\n#&gt; 서울\n#&gt; 서울\n#&gt; 제주\n#&gt; 충북\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n#&gt; 0.058823529411764705\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n#&gt; 0.0604\n\n\n\n\n\n24.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}\nPr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\\n                   &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)\n\\end{aligned}\\)\n넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n#&gt; [1] 0.1367713\n\nmean(서건창 | 이정후)\n#&gt; [1] 0.5852018\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n#&gt; [1] 0.5852018\n\n두 선수가 동시에 안타를 칠 확률은 0.14이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.59이 된다.\n200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n#&gt; [1] 0.11626\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n#&gt; [1] 0.1189903\n\n\n24.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\nR 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n#&gt; [1] 5.0053\nmean(binom_df$y)\n#&gt; [1] 9.9718\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n#&gt; # A tibble: 1 × 1\n#&gt;   mean_z\n#&gt;    &lt;dbl&gt;\n#&gt; 1   15.0",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#분포",
    "href": "basic_stat.html#분포",
    "title": "\n24  통계\n",
    "section": "\n24.4 분포",
    "text": "24.4 분포",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#footnotes",
    "href": "basic_stat.html#footnotes",
    "title": "\n24  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "25.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n#&gt; # A tibble: 1 × 2\n#&gt;   amount_est amount_var\n#&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1       40.9       35.7\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#밀린_병원비_추정",
    "href": "sampling.html#밀린_병원비_추정",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\n\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept",
    "href": "sampling.html#basic-concept",
    "title": "\n25  표본 추출\n",
    "section": "\n25.2 표본추출",
    "text": "25.2 표본추출\n\n25.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n#&gt; \n#&gt;  Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n#&gt; Rows: 1,338\n#&gt; Columns: 8\n#&gt; $ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n#&gt; $ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n#&gt; $ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n#&gt; $ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n#&gt; $ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n#&gt; $ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n#&gt; $ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n#&gt; $ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n25.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n#&gt; # A tibble: 10 × 8\n#&gt;    total_cup_points species coo          farm_name aroma  body balance sweetness\n#&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1             83.6 Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.67        10\n#&gt;  2             82.4 Arabica China        menglian…  7.67  7.42    7.42        10\n#&gt;  3             82.8 Arabica Guatemala    linda vi…  7.58  7.5     7.42        10\n#&gt;  4             85   Arabica Uganda       mount el…  8.17  7.67    7.75        10\n#&gt;  5             83   Arabica Colombia     &lt;NA&gt;       7.5   7.58    7.58        10\n#&gt;  6             84.5 Arabica Guatemala    nueva gr…  7.67  7.5     7.83        10\n#&gt;  7             81   Arabica Guatemala    chapulte…  7.42  7.25    7.17        10\n#&gt;  8             81.3 Arabica Brazil       sertao f…  7.17  7.42    7.33        10\n#&gt;  9             81.7 Arabica Tanzania, U… multiple   7.42  7.5     7.42        10\n#&gt; 10             80.1 Arabica Mexico       &lt;NA&gt;       7.25  7.33    7.17        10\n\n\n25.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n#&gt; # A tibble: 10 × 9\n#&gt;    rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n#&gt;    &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n#&gt;  2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n#&gt;  3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n#&gt;  4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n#&gt;  5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n#&gt;  6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n#&gt;  7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n#&gt;  8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n#&gt;  9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n#&gt; 10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n#&gt; # A tibble: 3 × 9\n#&gt;   rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n#&gt;   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1   446             82.5 Arabica Colomb… &lt;NA&gt;       7.67  7.25    7.67        10\n#&gt; 2   892             83.7 Arabica Colomb… various    7.83  7.58    7.67        10\n#&gt; 3  1338             85.9 Arabica Guatem… la esper…  7.92  8.08    7.83        10\n\n\n25.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n#&gt; # A tibble: 96 × 8\n#&gt; # Groups:   coo [37]\n#&gt;    total_cup_points species coo      farm_name     aroma  body balance sweetness\n#&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1             81   Arabica Brazil   cachoeira da…  7.33  7.33    7.17        10\n#&gt;  2             82.3 Arabica Brazil   rio verde      8.25  8.17    8.08        10\n#&gt;  3             82.8 Arabica Brazil   capoeirinha    7.67  7.5     7.42        10\n#&gt;  4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n#&gt;  5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n#&gt;  6             85.3 Arabica China    pu'er city l…  8     7.83    7.75        10\n#&gt;  7             82.4 Arabica China    menglian gao…  7.67  7.42    7.42        10\n#&gt;  8             82.3 Arabica China    yun lan coff…  7.5   7.42    7.42        10\n#&gt;  9             83   Arabica Colombia &lt;NA&gt;           7.75  7.58    7.17        10\n#&gt; 10             82.8 Arabica Colombia &lt;NA&gt;           7.67  7.5     7.42        10\n#&gt; # ℹ 86 more rows\n\n\n25.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n#&gt; # A tibble: 5 × 8\n#&gt;   total_cup_points species coo           farm_name aroma  body balance sweetness\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1             79.7 Arabica El Salvador   la monta…  7.25  7.33    7.17        10\n#&gt; 2             82.8 Arabica El Salvador   several …  7.75  7.25    7.5         10\n#&gt; 3             83.1 Arabica United State… &lt;NA&gt;       7.33  7.83    7.67        10\n#&gt; 4             79.9 Arabica United State… &lt;NA&gt;       7.5   7.33    7.17        10\n#&gt; 5             83.1 Arabica United State… &lt;NA&gt;       7.33  7.67    7.58        10",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept-comparison",
    "href": "sampling.html#basic-concept-comparison",
    "title": "\n25  표본 추출\n",
    "section": "\n25.3 표본추출 비교",
    "text": "25.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n25.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n#&gt; [1] 82.1512\n\n\n25.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n#&gt; [1] 81.87774\n\n\n25.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n#&gt; [1] 82.12846\n\n\n25.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n#&gt; [1] 80.94727",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-errors",
    "href": "sampling.html#calculate-errors",
    "title": "\n25  표본 추출\n",
    "section": "\n25.4 오차 측정",
    "text": "25.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n#&gt; # A tibble: 1 × 4\n#&gt;   population   srs stratifed cluster\n#&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1       82.2  81.9      82.1    80.9\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n#&gt; # A tibble: 4 × 3\n#&gt;   method     estimation relative_error\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 population       82.2         0     \n#&gt; 2 srs              81.9         0.333 \n#&gt; 3 stratifed        82.1         0.0277\n#&gt; 4 cluster          80.9         1.47\n\n\n25.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n#&gt; [1] 82.32256\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n#&gt;   [1] 82.03361 82.14602 82.11233 81.83947 82.07414 81.94902 81.80143 81.90098\n#&gt;   [9] 81.99541 82.24323 82.43143 82.41759 81.69571 82.00797 82.00526 81.90113\n#&gt;  [17] 82.24707 81.68549 82.28677 82.23579 81.96248 82.52338 81.90534 82.49910\n#&gt;  [25] 81.82579 82.08203 81.97203 82.25195 82.22038 81.97699 82.15985 81.78368\n#&gt;  [33] 82.15789 82.17278 82.16962 82.47256 82.11534 81.97414 82.28586 82.21617\n#&gt;  [41] 82.10850 82.54887 82.42053 82.18692 82.09436 82.38504 82.11684 82.18120\n#&gt;  [49] 82.57195 82.44391 82.59729 82.10820 82.23030 82.40271 81.97857 82.14241\n#&gt;  [57] 82.21609 82.25406 82.30812 82.00030 82.22602 82.15564 82.29271 82.34549\n#&gt;  [65] 82.01308 82.48278 82.38406 82.48519 82.39865 82.18729 81.91211 81.73023\n#&gt;  [73] 82.14977 82.28135 82.00143 82.23504 82.35925 82.44692 82.30684 82.56346\n#&gt;  [81] 82.20579 82.49729 81.72429 82.12211 82.05647 81.91316 82.26256 81.98496\n#&gt;  [89] 81.92970 82.53451 82.07677 81.90647 81.94060 82.39353 82.15090 82.39474\n#&gt;  [97] 82.32789 82.35714 82.54474 81.95391\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n#&gt;   [1] 82.09045 82.01361 82.38414 82.51338 82.12481 82.58940 82.32534 81.82218\n#&gt;   [9] 82.11263 82.01098 82.16654 81.96639 82.23925 81.58594 82.19120 82.12256\n#&gt;  [17] 82.44511 81.85504 82.34820 81.66624 82.16902 82.07692 82.38391 81.99406\n#&gt;  [25] 82.27669 82.28865 82.06308 81.81887 82.06180 82.06143 82.20338 82.45632\n#&gt;  [33] 81.93301 82.08624 82.31977 81.89647 81.93992 82.35820 82.04323 82.22669\n#&gt;  [41] 82.37391 82.07248 82.36248 81.63023 81.90624 82.07173 82.56195 82.52293\n#&gt;  [49] 82.27774 81.94481 82.22218 81.81556 82.13609 82.24947 81.99714 81.98444\n#&gt;  [57] 82.18331 81.60278 82.27361 82.09353 82.16436 82.28323 81.50504 81.91308\n#&gt;  [65] 82.43887 82.11609 82.31233 81.85932 82.47692 82.07489 82.26737 82.20188\n#&gt;  [73] 81.76316 82.08150 81.70398 82.05451 82.03985 82.42947 82.31060 82.08729\n#&gt;  [81] 82.11188 82.21323 82.00180 82.37714 82.51361 81.81910 82.41511 81.99812\n#&gt;  [89] 82.02872 82.00669 82.30880 82.47556 81.36549 82.21316 82.47241 81.86195\n#&gt;  [97] 81.71647 82.22053 82.17120 82.22489\n\n\n25.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n25.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n#&gt; # A tibble: 5 × 3\n#&gt;   samp_prop mean_cup_points sd_cup_points\n#&gt;   &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 10 %                 82.1        0.239 \n#&gt; 2 33 %                 82.2        0.0997\n#&gt; 3 50 %                 82.2        0.0677\n#&gt; 4 75 %                 82.2        0.0473\n#&gt; 5 90 %                 82.2        0.0263",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-bootstrap",
    "href": "sampling.html#calculate-bootstrap",
    "title": "\n25  표본 추출\n",
    "section": "\n25.5 신뢰구간",
    "text": "25.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n25.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n#&gt; [1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n#&gt; [1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n#&gt; [1] 2.864368\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n#&gt; [1] 2.488636\n\n\n25.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n#&gt; # A tibble: 1 × 3\n#&gt;   lower  mean upper\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  82.0  82.4  82.8",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#footnotes",
    "href": "sampling.html#footnotes",
    "title": "\n25  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "26.1 통계적 가설검정\n통계적 가설 검정(統計的假說檢定, statistical hypothesis test)은 통계적 추측의 하나로서, 모집단 실제의 값이 얼마가 된다는 주장과 관련해, 표본의 정보를 사용해서 가설의 합당성 여부를 판정하는 과정을 의미하는데 이를 위해서 프로세스(Process)와 함께 검정 통계량을 수식으로 나타낼 수 있어야 하고 이를 해석하는 별도의 훈련도 받아야 했고 이 과정에서 상당량의 수학 및 통계학적 지식이 요구된다. 2\n통계적 가설검정(Statistical Testing)은 기존 통계학 전공자의 전유물이었으나, 컴퓨터의 일반화와 누구나 코딩을 할 수 있는 현재(2024-03-17)는 더 이상 기존 통념이 통용되지는 않게 되었다. 특히 파이썬 진영에서 이런 움직임이 활발하다. 그렇다고 R 진영에서도 기존의 방식을 고수하는 것은 아니다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference",
    "href": "hypothesis.html#computer-age-statistical-inference",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "유의수준의 결정, 귀무가설과 대립가설 설정\n검정통계량의 설정 (예를 들어, t-검정)\n\n\\(t_{검정통계량} \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\)\n자유도: \\(\\nu \\quad  \\approx \\quad {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\\)\n\n\n\n기각역의 설정\n검정통계량 계산\n통계적인 의사결정",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-tidyverse-inference",
    "href": "hypothesis.html#computer-age-tidyverse-inference",
    "title": "26  코딩 가설검정",
    "section": "\n26.2 tidyverse 가설검정",
    "text": "26.2 tidyverse 가설검정\n데이터 과학을 이끌어 나가는 있는 R과 파이썬 진영의 현재 주도적인 흐름을 살펴보자. 우선 다소 차이가 있지만, for 반복루프를 이해하고 이를 코드로 구현할 수만 있다면 컴퓨터를 활용한 가설검정이 가능한 것은 사실이다. 하지만, 2011년 Allen Downey 교수가 주장했던 것처럼 오랜동안 검정된 해석학적 방법(Analytic Method)에 대한 교차검정하는 방식으로 활용하는 것이 추천된다. 3\n\n\nR과 파이썬 검정 가설검정 프레임워크 비교\n\n코딩기반 가설검정은 우선 데이터로부터 시작된다. 데이터를 컴퓨터의 기능을 활용하여 모의실험 표본을 생성하고 나서 귀무가설(\\(H_0\\)) 모형에서 검정통계량을 추출하여 이를 바탕으로 \\(p-값\\)을 계산하여 의사결정을 추진한다.\n통계검정에도 tidyverse를 반영하고 Allen Downey 교수가 주창한 통계검정 프레임워크를 도입하여 극단적으로 말하며 딥러닝 모형이 거의 모든 통계, 기계학습 모형을 통일해 나가듯이 다양한 통계검정에 대해서도 비숫한 위치를 점할 것으로 예측된다.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "href": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "title": "26  코딩 가설검정",
    "section": "\n26.3 가설검정과 신뢰구간",
    "text": "26.3 가설검정과 신뢰구간\ninfer 팩키지는 tidyverse 철학(?)에 따라 가설검정과 신뢰구간을 추정하는 목적으로 개발되었다. 크게 통계적 추론은 가설검정과 신뢰구간 추정이 주된 작업이다. 이를 위해서 5가지 동사(verb)를 새로 익혀야 한다.\n\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nvisualize()\n\n\n\n가설검정과 신뢰구간",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "href": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "title": "26  코딩 가설검정",
    "section": "\n26.4 사례: 맥주와 모기",
    "text": "26.4 사례: 맥주와 모기\n맥주를 마시는 사람이 말라리아 모기에게 매력적으로 보여 더 잘 물리는가? 라는 흥미로운 논문이 발표되었다. (Lefèvre 기타 2010) 이 연구는 맥주를 마신 후 사람의 냄새 (호흡 및 피부 방출 냄새)가 아노펠레스 감비아(Anopheles gambiae, 아프리카 주요 말라리아 매개체)에게 어떤 영향을 미치는지 조사하였다. 맥주를 마신 사람들의 몸 냄새는 모기의 활성화 (이륙 및 상향 풍속 비행에 참여하는 모기의 비율)와 방향성 (사람의 냄새를 향해 비행하는 모기의 비율)을 증가시켰다. 물을 마신 경우에는 사람이 모기에게 끌리는 것에 영향을 미치지 않았다.\n\n26.4.1 가설검정 환경설정\n데이터 전처리와 시각화, 한글설정을 위한 팩키지를 준비한다. 특히 infer 코딩기반 가설검정을 위해 필수적인 팩키지로 활용하는데 기본적인 사용방법은 mtcars, flights 데이터를 활용한 사례를 살펴본다.\n\nflights 데이터 소품문\nmtcars 데이터 소품문\n\n\n# 0. 환경설정 ----------\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(skimr)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(extrafont)\nloadfonts()\n\n# 1. 데이터 가져오기 -----\n\n# beer_dat &lt;- read_csv(\"https://raw.githubusercontent.com/aloy/m107/master/data/mosquitos.csv\")\nbeer_dat &lt;- read_csv(\"data/mosquitos.csv\")\n\nbeer_df &lt;- beer_dat %&gt;% \n    mutate(treatment = factor(treatment, levels = c(\"beer\", \"water\"), labels=c(\"맥주\", \"맹물\"))) \n\n\n26.4.2 탐색적 데이터 분석\n탐색적 데이터 분석을 통해서 말라리아 모기가 맥주를 마신 사람과 맹물을 마신 사람 어디에 더 많이 접근을 하는지 개체수 차이를 살펴본다. 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 우연에 의한 것인지 아니면 맥주가 더 모기에게 섹시하게 반응하는 역할을 하기 때문인지 살펴본다.\n\n# 2. 탐색적 데이터 분석 -----\n## 2.1. 전체 데이터 \nskim(beer_df)\n\n\nData summary\n\n\nName\nbeer_df\n\n\nNumber of rows\n43\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\n맥주: 25, 맹물: 18\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ncount\n0\n1\n21.77\n4.47\n12\n19\n21\n24\n31\n▂▃▇▅▂\n\n\n\n\n## 2.2. 맥주와 맹물 투여 집단 비교\nbeer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(최소 = min(count),\n                분위수_25 = quantile(count, 0.25),\n                평균 = mean(count),\n                중위수 = median(count),\n                분위수_75 = quantile(count, 0.75),\n                표준편차 = sd(count),\n                중위절대편차 = mad(count)) %&gt;% \n    mutate(맥주맹물차이 = max(평균) - min(평균))\n#&gt; # A tibble: 2 × 9\n#&gt;   treatment  최소 분위수_25  평균 중위수 분위수_75 표준편차 중위절대편차\n#&gt;   &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 맥주         17      20    23.6     24        27     4.13         5.93\n#&gt; 2 맹물         12      16.5  19.2     20        22     3.67         2.97\n#&gt; # ℹ 1 more variable: 맥주맹물차이 &lt;dbl&gt;\n\n## 2.3. 맥주와 맹물 투여 집단 비교 시각화\n\nbeer_density_g &lt;- ggplot(data = beer_df, mapping = aes(x = count, fill=treatment)) +\n    geom_density(aes(y = ..count..), alpha = 0.7) +\n    scale_x_continuous(limits=c(5,40)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    labs(title=\"맥주를 마시면 모기에게 섹시하게 보일까라고 쓰고 잘 물릴까라고 읽는다.\",\n        x=\"채집된 모기 개체수\", y=\"빈도수\", fill=\"실험처리(treatment): \")\n\nbeer_boxplot_g &lt;- ggplot(data = beer_df, mapping = aes(x = treatment, y = count, fill=treatment)) +\n    geom_boxplot(alpha = 0.5) +\n    geom_jitter(width = 0.2) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    coord_flip() +\n    theme(legend.position = \"none\") +\n    labs(title=\"\",\n         y=\"채집된 모기 개체수\", x=\"실험처리\", fill=\"실험처리(treatment): \")\n\ngrid.arrange(beer_density_g, beer_boxplot_g, nrow=2)\n\n\n\n\n\n\n\n\n26.4.3 의사결정\n맥주를 마신 집단과 맹물을 마신 집단간에 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 유의적인지 전통적인 t-검정과 코딩기반 모의실험을 통해서 살펴보자.\n전통 t-검정\nt-검정 결과 유의적인 차이가 나타나느 것으로 나타난다. p-값이 무척이나 작게 나온다.\n\n# 3. 통계 검정 -----\n## 3.1. 전통적인 해석적인 방법 (t-검정)\nt.test(count ~ treatment, beer_df, null = 0, var.equal = TRUE, alternative=\"greater\")\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  count by treatment\n#&gt; t = 3.587, df = 41, p-value = 0.0004416\n#&gt; alternative hypothesis: true difference in means between group 맥주 and group 맹물 is greater than 0\n#&gt; 95 percent confidence interval:\n#&gt;  2.323889      Inf\n#&gt; sample estimates:\n#&gt; mean in group 맥주 mean in group 맹물 \n#&gt;           23.60000           19.22222\n\n코딩기반 t-검정\n코딩기반 t-검정은 다음 절차를 통해 준비한다.\n\n코딩기반 t-검정을 수행할 경우 beer_df가 \\(\\delta^*\\)에 해당되어 데이터에서 사전에 계산해 놓는다.\n가설검정 공식을 specify 함수에 명세한다.\n귀무가설을 hypothesize 함수에서 적시한다.\n컴퓨터에서 모의실험 난수를 generate에서 생성시킨다.\n검정 통계량을 calculate 함수에 명시한다.\n\n그리고 나서 p-값, 95% 신뢰구간을 모의실험결과에서 단순히 세어서 정리하면 된다.\n마지막으로 시각적으로 한번 더 확인한다. 즉, 4.38번 더 물리는 것은 극히 드물게 일어나는 사례로 맥주를 마시면 모기에 더 잘 물리게 된다고 볼 수 있다.\n\n## 3.2. `infer` 팩키지 -----\n\n### 3.2.1. 데이터에서 두 집단 간 차이 산출\nbeer_diff &lt;- beer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(mean = mean(count)) %&gt;% \n    summarise(abs(diff(mean))) %&gt;% \n    pull\n\n### 3.2.2. 귀무가설 모형에서 모의실험을 통해서 통계량 산출\nnull_model &lt;- beer_df %&gt;%\n    specify(count ~ treatment) %&gt;%\n    hypothesize(null = \"independence\") %&gt;% \n    generate(reps = 1000, type = \"permute\") %&gt;% \n    calculate(stat = \"diff in means\", order = c(\"맥주\", \"맹물\"))\n\n### 3.2.3. p-갑과 95% 신뢰구간: 백분위수(Percentile) 방법\nnull_model %&gt;%\n    summarize(p_value = mean(stat &gt; beer_diff))\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1       0\n\nnull_model %&gt;%\n    summarize(l = quantile(stat, 0.025),\n              u = quantile(stat, 0.975))\n#&gt; # A tibble: 1 × 2\n#&gt;       l     u\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 -2.88  2.56\n\n### 3.2.4. 시각화\nggplot(null_model, aes(x = stat, fill=\"gray75\")) +\n    geom_density(aes(y=..count..), alpha=0.7) +\n    geom_vline(xintercept = beer_diff, color = \"red\", size=1.5) +\n    scale_x_continuous(limits=c(-5,5)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(title=\"맥주를 마시고 4.38번 더 모기에 물린다면...  \",\n         x=\"맥주와 맹물 개체수 차이\", y=\"빈도수\")\n\n\n\n\n\n\n\n\n\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric Elguero, Didier Fontenille, François Renaud, Carlo Costantini, 와/과 Frédéric Thomas. 2010. “Beer consumption increases human attractiveness to malaria mosquitoes”. PloS one 5 (3): e9546.",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#footnotes",
    "href": "hypothesis.html#footnotes",
    "title": "26  코딩 가설검정",
    "section": "",
    "text": "Allen Downey (2016), There is still only one test↩︎\n위키 백과 - 가설 검정↩︎\nHadley Wickham(2017-11-13), The tidy tools manifesto↩︎",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "NHST.html",
    "href": "NHST.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "R.A. Fisher는 NHST의 토대를 만들었으며 분산분석의 개념과 제한된 표본을 이용해 실험을 설계하는 실험계획법에 큰 기여를 했다. 1925년에 그가 발표한 ’Statistical Methods for Research Workers’라는 책에서 유의성 검정(significance test) 개념이 소개된 것을 확인할 수 있다. (Fisher 1970)\n귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 바탕으로 한 가설검정(hypothesis testing) 개념을 Neyman과 Pearson이 정립했고, 이를 적용한 최초의 사례는 1940년에 발표한 “Statistical Analysis in Educational Research”라는 책에서 NHST(Null Hypothesis Significance Testing) 개념을 처음으로 사용한 것으로 알려져 있다. (Lindquist 1940)\n\n\n\n\n\nflowchart TD\n    A[귀무가설 & 대립가설 설정]:::process\n    B[\"유의수준 선택 (예, 0.05)\"]:::process\n    C[데이터 수집 및 분석]:::process\n    D[\"검정 통계량 계산 (예, t-점수, z-점수)\"]:::process\n    E[검정 통계량을 임계값과 비교]:::decision\n    F[귀무가설 기각 또는 기각하지 않음]:::decision\n    G[\"결과 보고\"]:::report\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    classDef process fill:#efefef,stroke:#333,stroke-width:1px;\n    classDef decision fill:#ffefef,stroke:#333,stroke-width:1px;\n    classDef report fill:#efefff,stroke:#333,stroke-width:1px;\n\n\n\n\n\n\n\n\n\n\nFisher, Ronald Aylmer. 1970. “Statistical methods for research workers”. In Breakthroughs in statistics: Methodology and distribution, 66–70. Springer.\n\n\nLindquist, Everet Franklin. 1940. “Statistical analysis in educational research.”",
    "crumbs": [
      "**8부** 데이터 과학",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NHST.html</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFisher, Ronald Aylmer. 1970. “Statistical Methods for Research\nWorkers.” In Breakthroughs in Statistics: Methodology and\nDistribution, 66–70. Springer.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric\nElguero, Didier Fontenille, François Renaud, Carlo Costantini, and\nFrédéric Thomas. 2010. “Beer Consumption Increases Human\nAttractiveness to Malaria Mosquitoes.” PloS One 5 (3):\ne9546.\n\n\nLindquist, Everet Franklin. 1940. “Statistical Analysis in\nEducational Research.”\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#책의-구성",
    "href": "index.html#책의-구성",
    "title": "챗GPT 데이터 과학",
    "section": "책의 구성",
    "text": "책의 구성",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 데이터 과학",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 4월 속초 영금정\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "tidyr.html",
    "href": "tidyr.html",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "8.1 시작하기\n먼저 설치하지 않았다면 tidyr 패키지를 설치한다(아마도 앞에서 dplyr 패키지는 설치했을 것이다):\n#install.packages(\"tidyr\")\n#install.packages(\"dplyr\")\n패키지를 로드한다\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\n먼저 원래 gapminder 데이터프레임의 구조를 살펴보자:\nstr(gapminder)\n#&gt; 'data.frame':    1704 obs. of  6 variables:\n#&gt;  $ country  : chr  \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n#&gt;  $ year     : int  1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n#&gt;  $ pop      : num  8425333 9240934 10267083 11537966 13079460 ...\n#&gt;  $ continent: chr  \"Asia\" \"Asia\" \"Asia\" \"Asia\" ...\n#&gt;  $ lifeExp  : num  28.8 30.3 32 34 36.1 ...\n#&gt;  $ gdpPercap: num  779 821 853 836 740 ...\ngapminder 데이터셋처럼, 관측된 데이터에는 다양한 자료 형식이 있다. 대부분 순도 100% ‘long’ 혹은 순도 100% ‘wide’ 자료 형식 사이 어딘가에 위치하게 된다. gapminder 데이터셋에는 “ID” 변수가 3개(continent, country, year), “관측변수”가 3개(pop, lifeExp, gdpPercap)가 있다. 저자는 일반적으로 대부분의 경우에 중간단계 형식 데이터를 선호한다. 칼럼 1곳에 모든 관측점이 3가지 서로 다른 단위를 갖는 일은 거의 없다(예를 들어, ID변수 4개, 관측변수 1개).\n흔히 벡터 기반인 다수의 R 함수를 사용할 때, 서로 다른 단위를 갖는 값에 수학적 연산작업을 수행하지는 않는다. 예를 들어, 순수 ‘long’ 형식을 사용할 때, 인구, 기대수명, GDP의 모든 값에 대한 평균은 의미가 없는데, 이는 상호 호환되지 않는 3가지 단위를 갖는 평균값을 계산하여 반환하기 때문이다. 해법은 먼저 집단으로 그룹지어서 데이터를 솜씨 있게 다루거나(dplyr 학습교재 참조), 데이터프레임 구조를 변경시키는 것이다. 주의: R에서 일부 도식화 함수는 ‘wide’ 형식 데이터에 더 잘 작동한다.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#시작하기",
    "href": "tidyr.html#시작하기",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "도전과제\n\n\n\ngapminder는 순수한 ‘long’ 형식인가, ‘wide’ 형식인가, 혹은 두 가지 특징을 갖는 중간 형식인가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n원 gapminder 데이터프레임은 두 가지 특징을 갖는 중간 형식이다. 데이터프레임에 다수의 관측변수(pop, lifeExp, gdpPercap)가 있다는 점에서, 순수한 long 형식이라고 보기는 어렵다.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "href": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.2 pivot_longer(): wide에서 long 형식 전환",
    "text": "8.2 pivot_longer(): wide에서 long 형식 전환\n지금까지 깔끔한 형식을 갖는 원본 gapminder 데이터셋으로 작업을 했다. 하지만 ‘실제’ 데이터(즉, 자체 연구 데이터)는 절대로 잘 구성되어 있지 못하다. gapminder 데이터셋에 대한 wide 형식 버전을 가지고 시작해보자.\n\n이곳에서 ‘wide’ 형태를 갖는 gapminder 데이터를 다운로드 받아서, 로컬 data 폴더에 저장시킨다.\n\n데이터 파일을 불러와서 살펴보자. 주의: continent, country 칼럼이 요인형 자료형이 될 필요가 없으므로 read.csv() 함수 인자로 stringsAsFactors을 거짓(FALSE)으로 설정한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nwide 형식 데이터프레임\n\n깔끔한 중간 데이터 형식을 얻는 첫 단추는 먼저 ‘wide’ 형식에서 ‘long’ 형식으로 변환하는 것이다. tidyr 패키지의 pivot_longer() 함수는 관측 변수를 모아서(gather) long 형식 단일 변수로 변환한다. wide에서 long 형식으로 변환하기 위해 pivot_longer() 함수를 사용한다. pivot_longer()는 행의 수를 늘리고 열의 수를 줄임으로써 데이터셋을 더 길게 만들거나 관측 변수를 단일 변수로 ’연장’한다.\n\n\n\n\n\n그림 8.2: wide 형식에서 long 형식 전환과정 도식화\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n위에서 파이프 구문을 사용했는데, 이는 앞서 dplyr로 작업한 것과 유사하다. 사실, dplyr과 tidyr은 상호 호환되어 파이프 구문으로 dplyr과 tidyr 팩키지 함수를 파이핑하여 혼합하여 사용할 수 있다.\n먼저 pivot_longer()에 longer 형식으로 피벗될 열 이름 벡터를 제공한다. 모든 관측 변수를 입력할 수도 있지만 dplyr 레슨의 select() 함수처럼 starts_with() 인수를 사용하여 원하는 문자열로 시작하는 모든 변수를 선택할 수 있다. pivot_longer()는 피벗하지 않을 변수(즉, ID 변수)를 식별하기 위해 - 기호를 사용하는 대체 구문도 허용한다. pivot_longer()에 대한 다음 인수는 새 ID 변수(obstype_year)를 포함할 열의 이름을 지정하는 names_to와 새로 합쳐진 관측 변수(obs_value)의 이름을 지정하는 values_to이다. 새 열 이름을 문자열로 제공하여 후속 작업 가독성을 높인다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n특정 데이터프레임에서는 사소해 보일 수 있지만, 때로는 ID 변수 1개와 불규칙한 변수 이름을 가진 관측 변수 40개를 가질 수 있다. 이런 유연성은 시간을 상당히 절약해 준다!\n이제 obstype_year은 정보가 두 조각으로 나뉜다. 관측 유형(pop, lifeExp, gdpPercap)과 연도(year). separate() 함수를 사용하여 문자열을 여러 변수로 분할할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\ngap_long을 사용해서 각 대륙별로 평균 기대수명, 인구, 1인당 GDP를 계산한다. 힌트: dplyr에서 학습한 group_by()와 summarize() 함수를 사용한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "href": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.3 pivot_wider(): ’long’에서 중간 형식으로",
    "text": "8.3 pivot_wider(): ’long’에서 중간 형식으로\n작업을 항상 확인하는 것이 좋다. pivot_wider()는 pivot_longer()의 반대로, 열의 수를 늘리고 행의 수를 줄여 데이터셋을 더 넓게 만든다. pivot_wider()를 사용하여 gap_long을 원래의 중간 형식 또는 가장 넓은 형식으로 피벗하거나 재구성할 수 있다. 중간 형식에서부터 시작해보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 최초 데이터프레임 gapminder와 동일한 차원을 갖는 중간 데이터프레임 gap_normal이 있다. 하지만 변수 순서가 다르다. 순서를 수정하기 전에 all.equal() 함수를 사용해서 동일한지 확인한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n거의 다 왔다. 최초 데이터프레임은 country로 정렬된 다음 year로 정렬되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n훌륭하다! ‘long’ 형식에서 다시 중간 형식으로 돌아왔지만, 코드에 어떤 오류도 스며들지 않았다.\n이제 long에서 wide로 완전히 변환해 보자. wide 형식에서는 country와 continent를 ID 변수로 유지하고 관측치를 3개의 측정 기준(pop, lifeExp, gdpPercap)과 시간(year)에 걸쳐 피벗할 것이다. 먼저 모든 새 변수(시간*측정 기준 조합)에 대한 적절한 레이블을 만들어야 하며 gap_wide를 정의하는 과정을 단순화하기 위해 ID 변수를 통합해야 한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nunite()를 사용하여 이제 continent와 country의 조합인 단일 ID 변수를 가지고 있고 변수 이름을 정의했다. 이제 pivot_wider()로 파이핑할 준비가 되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n국가, 연도 및 3개의 측정 기준에 대해 피벗하여 gap_ludicrously_wide 형식 데이터를 만드시오.\n힌트 이 새로운 데이터 프레임은 행이 5개만 있어야 한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n이제 훌륭한 ‘wide’ 형식 데이터프레임을 가지고 있지만 ID_var가 더 사용하기 편할 수 있다. separate()를 사용하여 2개의 변수로 분리해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 되돌아왔다!",
    "crumbs": [
      "**3부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "9  논문 품질 그래프 생성",
    "section": "",
    "text": "9.1 계층\n산점도는 시간에 따른 정보를 시각화하는 데 가장 적합한 방법은 아닐 수 있다. 대신 ggplot에게 선 그래프로 데이터를 표현하도록 지시할 수 있다.\ngeom_point 계층 대신 geom_line 계층을 추가했다. aes에 by를 추가하여 ggplot이 각 국가를 선으로 연결하여 표현하도록 했다.\n그런데 선과 점을 모두 표시하려면 어떻게 해야 할까? 간단히 그래프에 또 다른 계층을 추가하면 된다.\n각 계층이 이전 계층 위에 그려진다는 점에 주목하자. 이번 예제에서는 점이 선 위에 표시되었다. 다음에 표시된 그래프를 보자.\n이 예제에서 aesthetic color 매핑이 ggplot의 전역 설정에서 geom_line 계층으로 이동했다. 따라서 더 이상 점에는 적용되지 않는다. 이제 선이 점 위에 그려진 것을 명확히 볼 수 있다.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#계층",
    "href": "ggplot2.html#계층",
    "title": "9  논문 품질 그래프 생성",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\n지금까지 (색상같은) aesthetic 를 데이터의 변수로 매핑(mapping)해서 사용하는 법을 살펴봤다. 예를 들어, geom_line(aes(color=continent))을 사용하면, ggplot에서 자동으로 각 대륙별로 다른 색상을 입힌다. 그런데, 모든 선을 파란색으로 바꾸고자 하면 어떨까? geom_line(aes(color=\"blue\")) 명령어가 동작해야 된다고 생각하지만, 사실은 그렇지 않다.특정 변수에 대한 매핑을 생성하지 않았기 대문에, aes() 함수 밖으로 색상을 명세하는 부분을 예를 들어, geom_line(color=\"blue\")와 같이 빼내기만 하면 된다.\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n선이 점 위에 그려진다!",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#변환과-통계",
    "href": "ggplot2.html#변환과-통계",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.2 변환과 통계",
    "text": "9.2 변환과 통계\nggplot을 사용하면 데이터에 통계 모델을 쉽게 겹칠 수 있다. 이를 시연하기 위해서, 첫번째 예제로 돌아가보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n현재 1인당 GDP에 몇 가지 극단적인 이상치가 있어서 점 사이의 내재된 관계를 보기 어렵다. scale 함수를 사용하여 y 축 스케일을 조정할 수 있다. 이를 통해 데이터 값과 aesthetic 시각적 표현 사이의 매핑을 제어할 수 있다. alpha 함수를 사용하여 투명도도 조정할 수 있는데, 이는 특히 많은 데이터가 한 곳에 밀집되어 있을 때 유용하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래프를 렌더링하기 전에 log10 함수가 gdpPercap 열 값을 변환했다. 따라서 변환된 스케일에서 10의 거듭제곱 단위마다 1씩 증가한다. 예를 들어, 1인당 GDP 1,000은 y-축에 3, 10,000은 y-축에 4에 대응된다. 로그 변환은 x 축에 넓게 퍼져 있는 데이터를 시각화하는 데 도움이 된다.\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\ngeom_point(alpha = 0.5)를 사용한 것에 주목하자. 앞서 언급했듯이, aes() 함수 외부에서 설정한 것은 모든 점에 동일한 값이 적용된다. 이 경우 투명도를 지정하는 것은 원하는 바로 문제가 없다. 그러나 다른 aesthetic 설정과 마찬가지로 alpha 투명도를 데이터의 변수에 매핑할 수도 있다. 예를 들어, 각 대륙마다 다른 투명도를 적용하려면 geom_point(aes(alpha = continent))와 같이 코딩하면 된다.\n\n\n또 다른 계층(geom_smooth)을 추가하여 관계를 단순하게 모델링할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n굵은 선의 두께는 geom_smooth 계층의 aesthetic size를 설정하여 조정할 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\naesthetic을 지정하는 방법에는 두 가지가 있다. 방금 전에는 geom_smooth 함수에 인수로 전달하여 size에 대한 aesthetic을 설정했다. 앞에서는 aes 함수를 사용하여 데이터 변수와 시각적 표현 사이의 매핑을 정의했다.\n\n\n\n\n\n\n도전과제\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자.\n힌트: aes 함수를 사용하지 않는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자. 힌트: aes 함수를 사용하지 않는다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#다중-패널-그래프",
    "href": "ggplot2.html#다중-패널-그래프",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.3 다중-패널 그래프",
    "text": "9.3 다중-패널 그래프\n앞에서는 하나의 그래프에 모든 국가에 대한 시간의 흐름에 따른 기대수명 변화를 시각화했다. 대안으로 facet 계층을 추가하여 그래프를 여러 개의 패널로 나눌 수 있다. 이번에는 국가명이 “A” 또는 “Z”로 시작하는 국가에만 초점을 맞춰보자.\n\n\n\n\n\n\n유용한 팁\n\n\n\n데이터의 부분 집합을 추출하는 것으로 시작해보자. substr 함수를 사용하여 문자열의 일부를 추출할 수 있다. 이 경우에는 gapminder$country 벡터의 시작과 끝 위치 문자를 추출한다. %in% 연산자를 사용하면 여러 개의 비교를 간단하게 수행할 수 있다. (이 경우 starts.with %in% c(\"A\", \"Z\")는 starts.with == \"A\" | starts.with == \"Z\"와 동일하다)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfacet_wrap 계층은 틸드(~)로 표시되는 “공식”을 인수로 받는다. gapminder 데이터셋의 country 열에 있는 각 고유값에 대해 별도의 패널로 그래프를 생성한다.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#텍스트-수정하기",
    "href": "ggplot2.html#텍스트-수정하기",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.4 텍스트 수정하기",
    "text": "9.4 텍스트 수정하기\n출판용 그래프를 만들기 위해서는 텍스트 요소를 일부 변경해야 할 필요가 있다. x 축이 너무 복잡하고, y 축은 데이터프레임 열 이름이 아닌 “Life expectancy”로 표시되어야 한다.\n몇 가지 다른 계층을 추가하여 텍스트를 수정할 수 있다. theme 계층은 각 축의 텍스트와 전반적인 텍스트 크기를 제어한다. 축, 그래프 제목, 범례는 labs() 함수를 사용하여 설정할 수 있다. 범례 제목은 aes() 함수에서 지정한 것과 동일한 이름을 사용한다. 따라서 색상 범례 제목은 color = \"Continent\"가 되고, 채우기(fill) 범례는 fill = \"MyTitle\"이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#그래프-저장하기",
    "href": "ggplot2.html#그래프-저장하기",
    "title": "9  논문 품질 그래프 생성",
    "section": "9.5 그래프 저장하기",
    "text": "9.5 그래프 저장하기\nggsave() 함수를 사용하여 ggplot으로 생성한 그래프를 로컬 컴퓨터에 저장할 수 있다. 출판을 위한 고품질 그래픽을 생성하기 위해 그래프의 크기와 해상도를 ggsave() 함수의 인수(width, height, dpi)로 전달할 수 있다. 앞에서 생성한 그래프를 저장하려면 먼저 lifeExp_plot 변수에 그래프를 할당한 다음 ggsave() 함수를 사용하여 png 형식으로 results 디렉터리에 저장하도록 지정한다. (작업 디렉터리에 results/ 폴더가 생성되어 있어야 한다.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nggsave() 함수에는 두 가지 좋은 점이 있다. 첫째, 기본값으로 가장 최근에 생성한 그래프가 저장되므로 plot 인수를 생략하면 ggplot으로 생성한 마지막 그래프가 자동으로 저장된다. 둘째, 저장되는 그래프 이미지 형식이 파일 확장자(예: .png 또는 .pdf)에 따라 결정된다. 필요한 경우 device 인수를 사용하여 명시적으로 파일 형식을 지정할 수도 있다.\n지금까지 ggplot2의 기본을 살펴보았다. RStudio는 다른 계층 사용법에 대한 참고자료로 유용한 치트 시트를 제공하고 있으며, ggplot2 웹사이트에는 추가 기능에 대한 자세한 정보가 있다. 마지막으로, 어떻게 수정해야 할지 모르겠다면 구글 검색을 통해 Stack Overflow에서 재사용 가능한 코드와 함께 관련 질문과 답변을 쉽게 찾을 수 있다.\n\n\n\n\n\n\n도전과제 (심화)\n\n\n\n가용한 연도 기간동안 각 대륙 간 기대수명을 비교하는 상자 그림(boxplot)을 생성한다.\n\ny축의 이름을 “기대수명”으로 변경한다.\nx축 레이블은 제거한다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n가능한 해법 중 하나는 다음과 같다. xlab()과 ylab()은 각각 x축과 y축 레이블을 설정한다. 축의 제목, 텍스트, 눈금은 테마의 속성이며 theme() 호출 내에서 수정되어야 한다.\n\n9.5.1 R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**4부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  }
]
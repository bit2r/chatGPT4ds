[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "1 데이터 과학"
  },
  {
    "objectID": "index.html#교육",
    "href": "index.html#교육",
    "title": "챗GPT 데이터 과학",
    "section": "1.2 교육",
    "text": "1.2 교육\n\nTidyverse Hands-on"
  },
  {
    "objectID": "index.html#관련-링크",
    "href": "index.html#관련-링크",
    "title": "챗GPT 데이터 과학",
    "section": "1.3 관련 링크",
    "text": "1.3 관련 링크\n\nR사용자회\n페이스북\n유튜브\nGitHub"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n2  소개\n",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lang_r.html",
    "href": "lang_r.html",
    "title": "\n3  R 언어\n",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lang_py.html",
    "href": "lang_py.html",
    "title": "4  파이썬",
    "section": "",
    "text": "About this site\n\nprint(\"챗GPT 데이터 과학 세상에 오신 것을 환영합니다.\")\n\n챗GPT 데이터 과학 세상에 오신 것을 환영합니다."
  },
  {
    "objectID": "index.html#데이터-과학",
    "href": "index.html#데이터-과학",
    "title": "챗GPT 데이터 과학",
    "section": "0.1 데이터 과학",
    "text": "0.1 데이터 과학"
  },
  {
    "objectID": "index.html#참고",
    "href": "index.html#참고",
    "title": "챗GPT 데이터 과학",
    "section": "1.1 참고",
    "text": "1.1 참고"
  },
  {
    "objectID": "penguins.html",
    "href": "penguins.html",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "4 펭귄 데이터셋"
  },
  {
    "objectID": "penguins.html#펭귄-데이터-출현",
    "href": "penguins.html#펭귄-데이터-출현",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.1 펭귄 데이터 출현",
    "text": "4.1 펭귄 데이터 출현\n미국에서 “George Floyd”가 경찰에 의해 살해되면서 촉발된 “Black Lives Matter” 운동은 아프리카계 미국인을 향한 폭력과 제도적 인종주의에 반대하는 사회운동이다. 한국에서도 소수 정당인 정의당에서 여당 의원 176명 중 누가?…차별금지법 발의할 ’의인’을 구합니다로 기사로 낼 정도로 적극적으로 나서고 있다.\n데이터 과학에서 최근 R.A. Fisher의 과거 저술한 “The genetical theory of natural selection” [@fisher1958genetical] 우생학(Eugenics) 대한 관점이 논란이 되면서 R 데이터 과학의 첫 데이터셋으로 붓꽃 iris 데이터를 다른 데이터, 즉 펭귄 데이터로 대체하는 움직임이 활발히 전개되고 있다. palmerpenguins [@penguin2020] 데이터셋이 대안으로 많은 호응을 얻고 있다. [@AbdulMajedRaja2020, @Levy2019]"
  },
  {
    "objectID": "penguins.html#penguins-study",
    "href": "penguins.html#penguins-study",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.2 펭귄 공부",
    "text": "4.2 펭귄 공부\n팔머(Palmer) 펭귄은 3종이 있으며 자세한 내용은 다음 나무위키를 참조한다. 1\n\n\n젠투 펭귄(Gentoo Penguin): 머리에 모자처럼 둘러져 있는 하얀 털 때문에 알아보기가 쉽다. 암컷이 회색이 뒤에, 흰색이 앞에 있다. 펭귄들 중에 가장 빠른 시속 36km의 수영 실력을 자랑하며, 짝짓기 할 준비가 된 펭귄은 75-90cm까지도 자란다.\n\n아델리 펭귄(Adelie Penguin): 프랑스 탐험가인 뒤몽 뒤르빌(Dumont D’Urville) 부인의 이름을 따서 ’아델리’라 불리게 되었다. 각진 머리와 작은 부리 때문에 알아보기 쉽고, 다른 펭귄들과 마찬가지로 암수가 비슷하게 생겼지만 암컷이 조금 더 작다.\n\n턱끈 펭귄(Chinstrap Penguin): 언뜻 보면 아델리 펭귄과 매우 비슷하지만, 몸집이 조금 더 작고, 목에서 머리 쪽으로 이어지는 검은 털이 눈에 띈다. 어린 고삐 펭귄들은 회갈색 빛을 띄는 털을 가지고 있으며, 목 아래 부분은 더 하얗다. 무리를 지어 살아가며 일부일처제를 지키기 때문에 짝짓기 이후에도 부부로써 오랫동안 함께 살아간다.\n\n\n\n팔머 펭귄 3종 세트\n\n다음으로 iris 데이터와 마찬가지로 펭귄 3종을 구분하기 위한 변수로 조류의 부리에 있는 중앙 세로선의 융기를 지칭하는 능선(culmen) 길이(culmen length)와 깊이(culmen depth)를 이해하면 된다.\n\n\n팔머 펭귄 능선 변수"
  },
  {
    "objectID": "penguins.html#penguin-home",
    "href": "penguins.html#penguin-home",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.3 펭귄 서식지",
    "text": "4.3 펭귄 서식지\nleaflet 팩키지로 펭귄 서식지를 남극에서 특정한다. geocoding을 해야 하는데 구글에서 위치 정보를 구글링하면 https://latitude.to/에서 직접 위경도를 반환하여 준다. 이 정보를 근거로 하여 펭귄 서식지를 시각화한다.\n\n\n\n\n파머 연구소와 펭귄 서식지\n\n\n\n펭귄 3종\n\n\n\n\n\n아델리, 젠투, 턱끈 펭귄이 함께한 사진\n\n\n\n토르거센 섬에서 새끼를 키우는 아델리 펭귄\n\n\n\n비스코 지점 젠투 펭귄 서식지\n\n\n\n펭귄과 함께 현장에서 일하는 크리스틴 고먼 박사\n\n\n\n\n파머 펭귄 데이터셋\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(palmerpenguins)\n# library(tidygeocoder)\n\npenguins %&gt;% \n  count(island)\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\nisland_df &lt;- tribble(~\"address\", ~\"lat\", ~\"lng\",\n                     \"Torgersen Island antarctica\", -64.772819, -64.074325,\n                     \"Dream Island antarctica\", -64.725558, -64.225562,\n                     \"Biscoe Island antarctica\", -64.811565, -63.777947,\n                     \"Palmer Station\", -64.774312, -64.054213)\n\nisland_df %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addMarkers(lng=~lng, lat=~lat, \n                   popup = ~ as.character(paste0(\"&lt;strong&gt;\", paste0(\"명칭:\",`address`), \"&lt;/strong&gt;&lt;br&gt;\",\n                                                 \"-----------------------------------------------------------&lt;br&gt;\",\n                                                 \"&middot; latitude: \", `lat`, \"&lt;br&gt;\",\n                                                 \"&middot; longitude: \", `lng`, \"&lt;br&gt;\"\n                   )))"
  },
  {
    "objectID": "penguins.html#데이터-설치",
    "href": "penguins.html#데이터-설치",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.4 데이터 설치",
    "text": "4.4 데이터 설치\nremotes 팩키지 install_github() 함수로 펭귄 데이터를 설치한다.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"allisonhorst/palmerpenguins\")\n\ntidyverse 팩키지 glimpse() 함수로 펭귄 데이터를 일별한다.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "penguins.html#penguin-EDA-skimr",
    "href": "penguins.html#penguin-EDA-skimr",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.5 자료구조 일별",
    "text": "4.5 자료구조 일별\nskimr 팩키지를 사용해서 penguins 데이터프레임 자료구조를 일별한다. 이를 통해서 344개 펭귄 관측값이 있으며, 7개 칼럼으로 구성된 것을 확인할 수 있다. 또한, 범주형 변수가 3개, 숫자형 변수가 4개로 구성되어 있다. 그외 더 자세한 사항은 범주형, 숫자형 변수에 대한 요약 통계량을 참조한다.\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n데이터가 크지 않아 DT 팩키지를 통해 데이터 전반적인 내용을 살펴볼 수 있다.\n\npenguins %&gt;% \n  reactable::reactable()"
  },
  {
    "objectID": "penguins.html#penguin-EDA",
    "href": "penguins.html#penguin-EDA",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.6 탐색적 데이터 분석",
    "text": "4.6 탐색적 데이터 분석\npalmerpenguins 데이터셋 소개에 포함되어 있는 미국 팔머 연구소 (palmer station) 펭귄 물갈퀴(flipper) 길이와 체질량(body mass) 산점도를 그려보자.\n\nlibrary(tidyverse)\nlibrary(extrafont)\nloadfonts()\n\nmass_flipper &lt;- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  theme_minimal(base_family = \"NanumGothic\") +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"펭귄 크기\",\n       subtitle = \"남극 펭귄 3종 물갈퀴 길이와 체질량 관계\",\n       x = \"물갈퀴 길이 (mm)\",\n       y = \"체질량 (g)\",\n       color = \"펭귄 3종\",\n       shape = \"펭귄 3종\") +\n  theme(legend.position = c(0.2, 0.7),\n        legend.background = element_rect(fill = \"white\", color = NA),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\")\n\nmass_flipper"
  },
  {
    "objectID": "penguins.html#footnotes",
    "href": "penguins.html#footnotes",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "신발끈 여행사, 관광안내자료↩︎"
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "4  챗GPT 자연어",
    "section": "",
    "text": "5 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n6 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Notable.ai\n\n\n8 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기"
  },
  {
    "objectID": "lang_sql.html",
    "href": "lang_sql.html",
    "title": "7  파이썬",
    "section": "",
    "text": "About this site\n\nprint(\"챗GPT 데이터 과학 세상에 오신 것을 환영합니다.\")\n\n챗GPT 데이터 과학 세상에 오신 것을 환영합니다."
  },
  {
    "objectID": "simpson.html",
    "href": "simpson.html",
    "title": "8  심슨의 역설",
    "section": "",
    "text": "9 심슨의 역설 사례\n심슨의 역설 사례를 책페이지수와 책가격의 관계를 살펴보자. 데이터는 책 유형(하드커버, 페이퍼백)은 두가지가 있고, 페이지수와 책가격이 달러로 구성된 데이터프레임이다.\n심슨의 역설에 대해서 동아사이언스에서 2008년, 2013년 3회 심슨의 역설에 대한 기사를 실었는데 “명문대 남녀 합격생의 반전”, “오류를 잡아라! 확률 법정” 의 기사가 눈에 띈다."
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA",
    "href": "simpson.html#simpson-paradox-case-study-EDA",
    "title": "8  심슨의 역설",
    "section": "\n9.1 데이터 시각화",
    "text": "9.1 데이터 시각화\n이를 시각적으로 표현하면 관계가 음의 상관관계를 갖는 것을 알 수 있다.\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(extrafont)\nloadfonts()\n\nsimp_df &lt;- tribble(\n    ~book_type, ~num_pages, ~book_price,\n    \"hardcover\", 150, 27.43, \n    \"hardcover\", 225, 48.76, \n    \"hardcover\", 342, 50.25, \n    \"hardcover\", 185, 32.01, \n    \"paperback\", 475, 10.00, \n    \"paperback\", 834, 15.73, \n    \"paperback\", 1020, 20.00, \n    \"paperback\", 790, 17.89)\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE)"
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "title": "8  심슨의 역설",
    "section": "\n9.2 기술통계량",
    "text": "9.2 기술통계량\nnum_pages, book_price 두변수를 추출하여 상관계수를 도출한다. 그리고 나서, 책 유형에 따른 상관관계도 도출해 낸다. 먼저 책 유형에 관계없이 num_pages, book_price 상관관계는 -0.5949366으로 나름 강한 음의 상관계수가 관측된다.\n\nsimp_df %&gt;% \n    summarise(book_cor = cor(num_pages, book_price)) %&gt;% \n    pull()\n\n[1] -0.5949366\n\n\n이번에는 책 유형에 따른 상관계수는 어떤지 계산해 보자. 이 경우, 하드커버는 0.848, 페이퍼백은 0.956로 강한 양의 상관관계가 존재함이 확인된다.\n\nsimp_df %&gt;% \n    group_by(book_type) %&gt;% \n    summarise(book_cor = cor(num_pages, book_price))\n\n# A tibble: 2 × 2\n  book_type book_cor\n  &lt;chr&gt;        &lt;dbl&gt;\n1 hardcover    0.848\n2 paperback    0.956"
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "title": "8  심슨의 역설",
    "section": "\n9.3 상관관계 시각화",
    "text": "9.3 상관관계 시각화\n앞서 확인한 결과를 책 유형별로 나눠 상관계수를 시각화한다.\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price, color=book_type)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"책페이지 수\", y=\"책가격($)\", title=\"심슨의 역설 사례\", color=\"책유형\" )+\n      theme(legend.position = \"top\")"
  },
  {
    "objectID": "simpson.html#simpson-paradox-berkeley",
    "href": "simpson.html#simpson-paradox-berkeley",
    "title": "8  심슨의 역설",
    "section": "\n10.1 UC 버클리 입학 3\n",
    "text": "10.1 UC 버클리 입학 3\n\n심슨의 역설관련 가장 유명한 사례는 1973년 UC 버클리 대학 입학데이터로 입학에 성차별이 존재하는지에 관한 데이터다.\n\n\n성별에 따른 입학률 비교\n\nlibrary(datasets)\nadmin_df &lt;- UCBAdmissions %&gt;% tbl_df\n\nadmin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n\n복사하여 붙여넣기\n\nadmin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n\n기술통계량을 통해 살펴본 사항을 그래프로 시각화한다. 막대 그래프를 통해 남성 합격률이 여성보다 높은 것으로 나타나 성차별이 존재하는 것으로 파악되지만, 학과별로 놓고 보면 여성 합격률이 더 높거나 남성과 유사한 것으로 시각적으로 나타난다.\n\n# A barplot for overall admission percentage for each gender.\n\nadmit_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, width = 0.2, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"성별\", y=\"입학합격율\", title=\"버클리 전체 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_minimal(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") \n\nadmit_dept_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    facet_grid(. ~ Dept) +\n    labs(x=\"성별\", y=\"\", title=\"버클리 학과별 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          legend.position = \"none\") \n\nplot_grid(admit_g, admit_dept_g, labels = \"\")"
  },
  {
    "objectID": "simpson.html#simpson-paradox-news-article-game-case",
    "href": "simpson.html#simpson-paradox-news-article-game-case",
    "title": "8  심슨의 역설",
    "section": "\n11.1 게임 업데이터 사례 4\n",
    "text": "11.1 게임 업데이터 사례 4\n\n예전에 모 게임에서 큰 규모의 업데이트를 한 후 게임 고객 동향을 분석한 적이 있습니다. 이 게임은 전체 게임 고객을 약 십 여가지 유형으로 분류하고 있는데, 크게 보면 게임 활동이 왕성하고 충성도가 높은 ‘진성’ 유형, 게임 활동이 그리 활발하지 않은 ‘라이트’ 유형, 자동 사냥 유저로 의심되는 ‘봇’ 유형 등이 있죠.\n이 게임의 업데이트 전/후 일별접속자수(DAU)와 유저당 결제금액(ARPU) 지표를 확인해 보니 아래와 같이 나왔습니다.\n\n일별 접속자수(DAU)가 크게 늘었지만 유저당 결재금액(ARPU)가 하락하여 뭔가 특단의 조치가 필요한 것으로 파악되지만, 이를 고객 집단을 반영하여 분석을 하게 되면 진성유저는 큰 차이가 없고, 크게 늘어난 유저가 봇이거나 Non-PU 유저라 봇을 비용으로 간주하여 제거하거나 Non-PU유저를 PU로 바꾸거나 PU 유저의 결재금액을 높이는 방향으로 사업적인 조치를 취하는 것이 바람직스러워 보인다."
  },
  {
    "objectID": "simpson.html#footnotes",
    "href": "simpson.html#footnotes",
    "title": "8  심슨의 역설",
    "section": "",
    "text": "Paul van der Laken (27 September 2017), “Simpson’s Paradox: Two HR examples with R code.”↩︎\n나무위키, “심슨의 역설”↩︎\nJohnny Hong(January 30, 2016), “A (very) brief introduction to ggplot2”↩︎\nNC소프트 (2017-07-05) “데이터 분석을 이용한 게임 고객 모델링 #4”↩︎"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "9  쿼토",
    "section": "",
    "text": "쿼토(Quarto)는 데이터 과학을 위한 통합 저작 프레임워크로 프로그래밍 코드, 실행산출물, 저작 텍스트를 통합한다. 쿼토는 재현가능한 과학기술 문서 저작을 지원하고 PDF, 마이크로소프트 워드 파일, PPT 슬라이드 등 수십 가지 출력 형식을 지원한다. 쿼토의 설계 사상은 다음과 같이 정리할 수 있다.\n\n분석 이면의 코드가 아닌 결론에 집중하고자 하는 의사 결정권자와 커뮤니케이션.\n결론과 결론에 도달한 방법(즉, 코드)에 관심이 있는 다른 데이터 과학자(미래 자신 포함!)와 공동 작업.\n데이터 과학을 수행할 수 있는 환경으로서, 자신이 수행한 작업뿐만 아니라 생각한 내용까지 담아낼 수 있는 최신 연구실 연구노트.\n\n쿼토는 R 패키지가 아니면 차세대 R마크다운 별명을 갖고 있듯이 R마크다운 이전 R 생태계에서 축적한 경험을 집대성한 문서저작 프로그램으로 R 뿐만 아니라 파이썬, 자바스크립트, 쥴리아도 지원한다. R마크다운에 익숙한 분이라면 쿼토가 차세대 R마크다운이라는 사실을 지원되는 언어와 일관되고 통합된 인터페이스를 통해 쉽게 확인할 수 있다.\nEnter"
  },
  {
    "objectID": "quarto.html#윈도우-설치",
    "href": "quarto.html#윈도우-설치",
    "title": "\n9  쿼토\n",
    "section": "\n9.1 윈도우 설치",
    "text": "9.1 윈도우 설치\nQuarto를 운영체제에 맞춰 설치한다. Quarto 는 기본적으로 CLI 라서 설치 후 제대로 설정이 되었는지는 환경설정에 경로를 등록해줘야 한다.\n\n\nQuarto 다운로드\nQuarto 설치\nQuarto CLI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n윈도우 시스템의 경우 quarto.exe가 아니고 quarto.cmd 라 이에 유의한다. 즉, 제어판 → 환경 변수 설정 … 에서 \"C:\\Users\\사용자명\\AppData\\Local\\Programs\\Quarto\\bin 디렉토리를 등록한 후 quarto.cmd 을 사용해서 출판한다. 쿼토 1.3 버전이 출시되면서 이런 문제는 해결되었다.\n\nSys.which(\"quarto\")\n                                                                  quarto \n\"C:\\\\Users\\\\STATKC~1\\\\AppData\\\\Local\\\\Programs\\\\Quarto\\\\bin\\\\quarto.cmd\""
  },
  {
    "objectID": "quarto.html#설치방법",
    "href": "quarto.html#설치방법",
    "title": "\n9  쿼토\n",
    "section": "\n9.2 설치방법",
    "text": "9.2 설치방법\nquarto 웹사이트에서 Quarto CLI 엔진을 설치한다. 통합개발도구(IDE)를 설치한다. Quarto CLI를 지원하는 IDE는 VS Code, RStudio, Jupyter, VIM/Emacs 와 같은 텍스트 편집기가 포함된다. IDE까지 설치를 했다면 literate programming 방식으로 마크다운과 프로그래밍 언어를 결합하여 출판을 위한 전문 문서 저작을 시작한다."
  },
  {
    "objectID": "quarto.html#쿼토-출판",
    "href": "quarto.html#쿼토-출판",
    "title": "\n12  쿼토\n",
    "section": "\n12.2 쿼토 출판",
    "text": "12.2 쿼토 출판\n\n12.2.1 출판 플랫폼\n데이터 사이언스 저작물을 제작하게 되면 그 다음 단계로 출판을 해야하는데 다양한 문서를 모아 프로젝트로 담아 Quarto Pub에 전자출판한다. 다른 출판 플랫폼으로 netlify, GitHub Pages, RStudio Connect가 많이 사용된다.\n\n\n\n\n\n12.2.2 Quarto Pub 출판 1\n\nQuarto Pub 웹사이트에 출판하는 방식은 Quarto CLI 를 사용한다. 필히 RStudio 내부 Terminal을 사용해서 Quarto Pub으로 출판한다.\n\nquarto.cmd publish quarto-pub\n? Authorize (Y/n) › \n❯ In order to publish to Quarto Pub you need to\n  authorize your account. Please be sure you are\n  logged into the correct Quarto Pub account in \n  your default web browser, then press Enter or \n  'Y' to authorize.\n\n첫번째 출판하게 되면 인증작업을 수행하고 나면 _publish.yml 파일이 하나 생성된다.\n\n- source: project\n  quarto-pub:\n    - id: 1fa3ab1f-c010-453a-aaf2-f462bd074a66\n      url: 'https://quartopub.com/sites/statkclee/quarto-ds'\n\n이제 모든 준비가 되었기 때문에 다음 명령어로 작성한 출판 문서를 포함한 웹사이트를 로컬에서 미리 확인 한 후에 Quarto Pub으로 전자출판한다. 윈도우에서는 RStudio 내부 Terminal CLI를 사용하는 것을 권장한다.\n\nquarto preview\nquarto publish quarto-pub"
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "\n12  쿼토\n",
    "section": "",
    "text": "Quarto Pub↩︎\nAdam Hyde (Aug 16, 2021), “Single Source Publishing - A investigation of what Single Source Publishing is and how this ‘holy grail’ can be achieved.”↩︎"
  },
  {
    "objectID": "quarto.html#쿼토설치",
    "href": "quarto.html#쿼토설치",
    "title": "\n12  쿼토\n",
    "section": "\n12.1 쿼토설치",
    "text": "12.1 쿼토설치\n\n12.1.1 설치방법\nquarto 웹사이트에서 Quarto CLI 엔진을 설치한다. 통합개발도구(IDE)를 설치한다. Quarto CLI를 지원하는 IDE는 VS Code, RStudio, Jupyter, VIM/Emacs 와 같은 텍스트 편집기가 포함된다. IDE까지 설치를 했다면 literate programming 방식으로 마크다운과 프로그래밍 언어를 결합하여 출판을 위한 전문 문서 저작을 시작한다.\n\n\n\n\n\n12.1.2 윈도우 설치\nQuarto를 운영체제에 맞춰 설치한다. Quarto 는 기본적으로 CLI 라서 설치 후 제대로 설정이 되었는지는 환경설정에 경로를 등록해줘야 한다.\n\n\nQuarto 다운로드\nQuarto 설치\nQuarto CLI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n윈도우 시스템의 경우 quarto.exe가 아니고 quarto.cmd 라 이에 유의한다. 즉, 제어판 → 환경 변수 설정 … 에서 \"C:\\Users\\사용자명\\AppData\\Local\\Programs\\Quarto\\bin 디렉토리를 등록한 후 quarto.cmd 을 사용해서 출판한다. 쿼토 1.3 버전이 출시되면서 이런 문제는 해결되었다.\n\nSys.which(\"quarto\")\n                                                                  quarto \n\"C:\\\\Users\\\\STATKC~1\\\\AppData\\\\Local\\\\Programs\\\\Quarto\\\\bin\\\\quarto.cmd\""
  },
  {
    "objectID": "quarto.html#문서-컴파일러-1",
    "href": "quarto.html#문서-컴파일러-1",
    "title": "\n12  쿼토\n",
    "section": "\n12.3 문서 컴파일러 2\n",
    "text": "12.3 문서 컴파일러 2\n\nQuarto 는 Pandoc에 기반한 오픈소스 과학기술 출판시스템이다. 하지만 특정 언어에 종속되지 않고 R, 파이썬, 쥴리아, 자바스크립트(Observable JS) 를 지원하고 있으며 이를 통해 다음 출판 저작물 작성이 가능하다.\n\n\n\n\n크게 세가지 부분에 대해 출판시스템에 대한 고민이 필요하다.\n\n콘텐츠(Content): 저작과 관련된 문서 내용\n디자인(Design): 출판 결과물에 대한 외양(Look and Feel)\n형식(Format): 출판물 최종 산출물\n\nQuarto 는 Literate Programming System 으로 다양한 언어를 지원하고 다양한 출판결과물을 연결시키는 핵심 엔진으로 Pandoc 을 사용한다.\n\n\n\n\n\n\n\nComputations\n문서 저작\n출력물\n\n\nPython, R, Julia, Observable JS\nPandoc, 마크다운 (Markdown)\n문서, 웹사이트, PPT, 책, 블로그등\n\n\n좀더 구체적으로 전문적인 출판을 위해서 문서저작에 다양한 기능과 함께 출판 산출물을 지원한다.\n\n문서저작(pandoc): 마크다운, 수식, 인용, 서지관리, 콜아웃(callout), 고급 layout 등\n출판산출물: 고품질 기사(article), 보고서, PPT, 웹사이트, 블로그, (HTML, PDF, MS 워드, ePub 등) 전자책"
  },
  {
    "objectID": "quarto.html#single-sourcing-출판저작",
    "href": "quarto.html#single-sourcing-출판저작",
    "title": "\n12  쿼토\n",
    "section": "\n12.4 Single Sourcing 출판저작",
    "text": "12.4 Single Sourcing 출판저작\n데이터 사이언스 출판저작에 다소 차이는 있지만 출판에 대한 대체적인 방식은 유사할 것으로 보인다. 즉, Single Sourcing 을 콘텐츠 저작, 디자인, 최종 출판물 관리까지 일원화되어 자동화되어 체계적으로 관리된다면 중복되는 낭비는 물론 재현가능성도 높여 과학기술 출판저작물로 가장 이상적으로 간주되고 있다.\n\n\n문제점\n개념\nSingle Sourcing Multi-Use\n\n\n\n\n\n그림 12.1: 문제점\n\n\n\n\n\n그림 12.2: Single Sourcing 개념\n\n\n\n\n\n그림 12.3: Single Sourcing Multi-Use"
  },
  {
    "objectID": "quarto.html#작업흐름",
    "href": "quarto.html#작업흐름",
    "title": "\n12  쿼토\n",
    "section": "\n12.5 작업흐름",
    "text": "12.5 작업흐름\n기존 R .Rmd, 파이썬 .ipynb 확장자를 갖는 작업흐름이 .qmd 파일로 단일화되는 것이 가장 큰 특징이다. 따라서 마크다운으로 콘텐츠를 작성하고 프로그래밍 코드를 R, 파이썬, 자바스크립트, 쥴리아 로 작성하게 되면 자동으로 계산을 수행하고 결과물을 마크다운으로 변환시키기 때문에 후속 작업을 신경쓰지 않고 원하는 결과물을 얻을 수 있는 장점이 있다.\n\n\nR (.Rmd)\n파이썬 (주피터)\nQuarto - R\nQuarto - 파이썬"
  },
  {
    "objectID": "quarto.html#주요-기능",
    "href": "quarto.html#주요-기능",
    "title": "\n12  쿼토\n",
    "section": "\n12.6 주요 기능",
    "text": "12.6 주요 기능\n\n\nFeature\nR Markdown\nQuarto\n\n\n\nBasic Formats\n\nhtml_document\npdf_document\nword_document\n\n\nhtml\npdf\ndocx\n\n\n\nBeamer\n\nbeamer_presentation\n\n\nbeamer\n\n\n\nPowerPoint\n\npowerpoint_presentation\n\n\npptx\n\n\n\nHTML Slides\n\nxaringan\nrevealjs\n\n\nrevealjs\n\n\n\nAdvanced Layout\n\ntufte\ndistill\n\n\nQuarto Article Layout\n\n\n\nCross References\n\nhtml_document2\npdf_document2\nword_document2\n\n\nQuarto Crossrefs\n\n\n\nWebsites & Blogs\n\nblogdown\ndistill\n\n\nQuarto Websites\nQuarto Blogs\n\n\n\nBooks\n\nbookdown\n\nQuarto Books\n\n\nInteractivity\nShiny Documents\nQuarto Interactive Documents\n\n\nPaged HTML\npagedown\nSummer 2022\n\n\nJournal Articles\nrticles\nSummer 2022\n\n\nDashboards\n\nflexdashboard |\nFall 2022"
  },
  {
    "objectID": "quarto.html#위지윅-vs-위지윔",
    "href": "quarto.html#위지윅-vs-위지윔",
    "title": "\n12  쿼토\n",
    "section": "\n12.7 위지윅 vs 위지윔",
    "text": "12.7 위지윅 vs 위지윔\n신속하고 빠르게 누구나 짧은 학습을 통해서 문서를 저작하고 출판할 수 있는 방식은 아래한글 혹은 MS워드 워드프로세서를 사용하는데 이는 위지위그(WYSIWYG: What You See Is What You Get, “보는 대로 얻는다”)에 기초한 것으로 문서 편집 과정에서 화면에 포맷된 낱말, 문장이 출력물과 동일하게 나오는 방식을 말한다. 위지윅의 대척점에 있는 것이 위지윔(WYSIWYM, What You See Is What You Mean)으로 대표적인 것인 \\(\\LaTeX\\) 으로 구조화된 방식으로 문서를 작성하면 컴파일을 통해서 최종 문서가 미려한 출판가능한 PDF, PS, DVI 등 확장자를 갖는 출판결과물을 얻을 수 있다.\n\n\n\n\n\n12.7.1 블로그 저작 소프트웨어\n개인용 컴퓨터가 보급되면서 아래한글과 같은 워드 프로세서를 사용해서 저작을 하는 것이 일반화되었지만 곧이어 인터넷이 보급되면서 웹에 문서를 저작하는 것이 이제는 더욱 중요하게 되었다. 전문 개발자가 아닌 일반인이 HTML, CSS, JavaScript를 학습하여 웹에 문서를 제작하고 출판하는 것은 난이도가 있다보니 워드프레스와 티스토리 같은 위지위그 패러다임을 채택한 저작도구가 사용되고 있으나 상대적으로 HTML, CSS, JavaScript을 조합한 방식과 비교하여 고급스러운 면과 함께 정교함에 있어 아쉬움이 있는 것도 사실이다.\n\n\n워드프레스와 티스토리\nHTML + CSS + 자바스크립트"
  },
  {
    "objectID": "quarto.html#쿼토-저작",
    "href": "quarto.html#쿼토-저작",
    "title": "\n12  쿼토\n",
    "section": "\n12.8 쿼토 저작",
    "text": "12.8 쿼토 저작\nQuarto 는 10년전부터 시작된 knitr 경험을 많이 녹여냈고 위지윔 패러다임에 기초를 하고 있다고 볼 수 있다. RStudio를 IDE로 Quarto CLI와 함께 출판물을 저작한다면 편집기에 있는 Visual 모드가 있어 위지윅 패러다임도 문서저작에 사용이 가능하다. 특히, R, 파이썬, SQL, 자바스크립트 등 컴퓨팅 엔진을 달리하여 문서에 그래프, 표, 인터랙티브 결과물도 함께 담을 수 있는 것은 커다란 장점이다.\n\n\n\n\nQuarto 저작은 크게 3가지 구성요소로 되어 있다.\n\n메타데이터: YAML\n텍스트: 마크다운\n코드: knitr, jupyter\n\n\n상기 구성요소를 조합하게 되면 다양한 데이터 사이언스 웹사이트를 비롯한 출판물을 제작하게 된다.\n\n\nQuarto 문서 구성요소\n\n\n12.8.1 YAML\n메타데이터는 YAML인데 GNU처럼 “Yet Another Markup Language” 혹은 “YAML Ain’t Markup Language”을 줄인단어다.\n\n\n키값\n출력옵션\n상세 출력옵션\n\n\n\n---\nkey: value\n---\n\n\n---\nformat: something\n---\n. . .\n---\nformat: pdf\n---\n---\nformat: pdf\n---\n---\nformat: revealjs\n---\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n왜 YAML이 필요하게 된 것인가? YAML은 단순히 KEY: Value 에 불과한데 CLI를 이해하게 되면 왜 YAML을 사용하는 것이 유용한지 이해할 수 있다. 먼저 간단한 CLI 명령어를 YAML로 변환해보자.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html\n\n\n\n\n---\nformat: pdf\n---\n\n\n한단계 더 들어가서 좀더 많은 선택옵션을 넣어 고급 기능을 넣는 사례를 살펴본다.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html -M code fold:true\n\n\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n12.8.2 마크다운\n데이터 과학 문서 웹사이트에 “마크다운 기초”, “고급 마크다운”, “R 마크다운 실무” 를 참조한다.\n\n12.8.3 코드\nR, 파이썬, SQL, 자바스크립트 등 버그 없이 정상 동작하는 프로그램을 작성하여 포함시킨다.\n\n12.8.4 YAML 코드편집\nRStudio, VSCode IDE는 탭-자동완성(tab-completion)을 제공한다. 즉, 첫단어를 타이핑하고 탭을 연결하여 키보드를 치게되면 연관 명령어가 나와 선택하면 된다. 혹은 Ctrl + space 단축키를 치게되면 전체 명령어가 나온다."
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "9  통계",
    "section": "",
    "text": "통계학은 왕의 학문이라는 별명을 갖고 있으며, 왕(국가)이 기원전 3,050년 피라미드 건립을 위해 인구조사(센서스)를 했다는 기록이 최초로 남아있다. 근대 국가운영을 위해 인구, 출생, 사망, 실업자수, 세금 수입, 지출, 수입과 수출 등 자료가 필요하여 발전했으며 통계학(Statistics)는 라틴어 Status 정치국가(Political State) 를 의미한다.\n\n\n\n\ngraph LR\n\n  World[\"현실/가상&lt;br&gt;세계\"] --&gt; Data\n  Data[\"데이터\"] --&gt; Analysis[\"분석\"]\n  Analysis --&gt; Summary_Technique_Inference[\"요약기술&lt;br&gt;추론\"]\n  \n  style World fill:#f5d06c,stroke:#333,stroke-width:3px\n  style Data fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style Analysis fill:#c6def1,stroke:#333,stroke-width:3px\n  style Summary_Technique_Inference fill:#e1d5e7,stroke:#333,stroke-width:3px"
  },
  {
    "objectID": "statistics.html#통계-분야",
    "href": "statistics.html#통계-분야",
    "title": "\n9  통계\n",
    "section": "\n9.1 통계 분야",
    "text": "9.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다."
  },
  {
    "objectID": "statistics.html#기술통계",
    "href": "statistics.html#기술통계",
    "title": "\n9  통계\n",
    "section": "\n9.2 기술통계",
    "text": "9.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n\n\n\n9.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n\n# A tibble: 1 × 2\n  평균_체중 최빈종\n      &lt;dbl&gt; &lt;fct&gt; \n1     4202. Adelie\n\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\n\n펭귄 체중: 4201.754385964912\n\nprint(f'펭귄 최빈종: {mode_species}')\n\n펭귄 최빈종: Adelie\n\n\n\n\n\n\n9.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n\n\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  분산_체중 표준편차_체중\n      &lt;dbl&gt;         &lt;dbl&gt;\n1   643131.          802.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\n\n펭귄 체중 분산: 643131.0773267478\n\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n펭귄 체중 표준편차: 801.9545356980955\n\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n\n# A tibble: 3 × 2\n  species   빈도수\n  &lt;fct&gt;      &lt;int&gt;\n1 Adelie       152\n2 Gentoo       124\n3 Chinstrap     68\n\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies\n\n     species  빈도수\n0     Adelie  152\n1     Gentoo  124\n2  Chinstrap   68"
  },
  {
    "objectID": "statistics.html#가능성",
    "href": "statistics.html#가능성",
    "title": "\n9  통계\n",
    "section": "\n9.3 가능성",
    "text": "9.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n9.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"경남\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"전남\" \"충남\" \"충남\" \"인천\" \"전북\" \"전남\" \"충남\" \"경북\" \"광주\" \"부산\"\n[11] \"전북\" \"충남\" \"부산\" \"광주\" \"울산\" \"서울\" \"세종\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.05882353\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0535\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n광주\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n대전\n경기\n강원\n울산\n충남\n전남\n충남\n충남\n광주\n경북\n서울\n경기\n서울\n경기\n경북\n인천\n경기\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.11764705882352941\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0577\n\n\n\n\n\n\n9.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}  Pr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\  &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)  \\end{aligned}\\)\n\n9.3.2.1 넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n\n[1] 0.1300448\n\nmean(서건창 | 이정후)\n\n[1] 0.6098655\n\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n\n[1] 0.6098655\n\n\n두 선수가 동시에 안타를 칠 확률은 0.13이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.61이 된다.\n\n9.3.2.2 200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n\n[1] 0.11804\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n\n[1] 0.1189903\n\n\n\n9.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n\n9.3.3.1 두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\n\n9.3.3.2 R 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n\n[1] 5.0183\n\nmean(binom_df$y)\n\n[1] 10.0214\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n\n# A tibble: 1 × 1\n  mean_z\n   &lt;dbl&gt;\n1   15.0"
  },
  {
    "objectID": "statistics.html#footnotes",
    "href": "statistics.html#footnotes",
    "title": "\n9  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎"
  },
  {
    "objectID": "statistics.html#확률",
    "href": "statistics.html#확률",
    "title": "\n9  통계\n",
    "section": "\n9.4 확률",
    "text": "9.4 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"충북\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"대전\" \"대전\" \"울산\" \"세종\" \"인천\" \"대전\" \"세종\" \"세종\" \"충북\" \"강원\"\n[11] \"인천\" \"전북\" \"충남\" \"강원\" \"대구\" \"제주\" \"경기\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0605\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n울산\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n세종\n충남\n경북\n서울\n전북\n경기\n전남\n전남\n강원\n전북\n경기\n세종\n강원\n서울\n경기\n세종\n부산\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.0\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0624"
  },
  {
    "objectID": "statistics.html#분포",
    "href": "statistics.html#분포",
    "title": "\n9  통계\n",
    "section": "\n9.4 분포",
    "text": "9.4 분포"
  },
  {
    "objectID": "basic_stat.html#통계-분야",
    "href": "basic_stat.html#통계-분야",
    "title": "\n9  통계\n",
    "section": "\n9.1 통계 분야",
    "text": "9.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다."
  },
  {
    "objectID": "basic_stat.html#기술통계",
    "href": "basic_stat.html#기술통계",
    "title": "\n9  통계\n",
    "section": "\n9.2 기술통계",
    "text": "9.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n\n\n\n9.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n\n# A tibble: 1 × 2\n  평균_체중 최빈종\n      &lt;dbl&gt; &lt;fct&gt; \n1     4202. Adelie\n\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\n\n펭귄 체중: 4201.754385964912\n\nprint(f'펭귄 최빈종: {mode_species}')\n\n펭귄 최빈종: Adelie\n\n\n\n\n\n\n9.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n\n\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  분산_체중 표준편차_체중\n      &lt;dbl&gt;         &lt;dbl&gt;\n1   643131.          802.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\n\n펭귄 체중 분산: 643131.0773267478\n\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n펭귄 체중 표준편차: 801.9545356980955\n\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n\n# A tibble: 3 × 2\n  species   빈도수\n  &lt;fct&gt;      &lt;int&gt;\n1 Adelie       152\n2 Gentoo       124\n3 Chinstrap     68\n\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies\n\n     species  빈도수\n0     Adelie  152\n1     Gentoo  124\n2  Chinstrap   68"
  },
  {
    "objectID": "basic_stat.html#가능성",
    "href": "basic_stat.html#가능성",
    "title": "\n9  통계\n",
    "section": "\n9.3 가능성",
    "text": "9.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n9.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"충남\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"전북\" \"강원\" \"광주\" \"광주\" \"경기\" \"광주\" \"경남\" \"충남\" \"대전\" \"경북\"\n[11] \"경남\" \"충남\" \"울산\" \"부산\" \"경남\" \"세종\" \"전북\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0596\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n제주\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n부산\n인천\n인천\n강원\n전북\n강원\n부산\n강원\n충북\n충북\n대전\n울산\n제주\n전남\n대전\n전남\n서울\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.0\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0616\n\n\n\n\n\n\n9.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}  Pr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\  &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)  \\end{aligned}\\)\n\n9.3.2.1 넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n\n[1] 0.1188341\n\nmean(서건창 | 이정후)\n\n[1] 0.5358744\n\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n\n[1] 0.5358744\n\n\n두 선수가 동시에 안타를 칠 확률은 0.12이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.54이 된다.\n\n9.3.2.2 200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n\n[1] 0.12042\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n\n[1] 0.1189903\n\n\n\n9.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n\n9.3.3.1 두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\n\n9.3.3.2 R 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n\n[1] 4.9945\n\nmean(binom_df$y)\n\n[1] 10.023\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n\n# A tibble: 1 × 1\n  mean_z\n   &lt;dbl&gt;\n1   15.0"
  },
  {
    "objectID": "basic_stat.html#분포",
    "href": "basic_stat.html#분포",
    "title": "\n9  통계\n",
    "section": "\n9.4 분포",
    "text": "9.4 분포"
  },
  {
    "objectID": "basic_stat.html#footnotes",
    "href": "basic_stat.html#footnotes",
    "title": "\n9  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎"
  },
  {
    "objectID": "intermediate_stat.html#밀린_병원비_추정",
    "href": "intermediate_stat.html#밀린_병원비_추정",
    "title": "\n10  표본 추출\n",
    "section": "\n10.1 병원비 추정",
    "text": "10.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\n\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n\n# A tibble: 1 × 2\n  amount_est amount_var\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       40.9       35.7\n\n\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\n\n\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다."
  },
  {
    "objectID": "intermediate_stat.html#basic-concept",
    "href": "intermediate_stat.html#basic-concept",
    "title": "\n10  표본 추출\n",
    "section": "\n10.2 표본추출",
    "text": "10.2 표본추출\n\n10.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n\nRows: 1,338\nColumns: 8\n$ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n$ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n$ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n$ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n$ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n$ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n$ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n$ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n\n\n\n\n10.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   total_cup_points species coo          farm_name aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             84.2 Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.92        10\n 2             87.2 Arabica Mexico       la herra…  8.17  7.83    8.17        10\n 3             84   Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.75        10\n 4             80.7 Arabica Costa Rica   gamboa     7.83  7.83    7.5         10\n 5             81.8 Arabica United Stat… kona pac…  7.25  7.75    7.75        10\n 6             81.1 Arabica United Stat… hacienda…  7.67  7.58    7.67        10\n 7             78.5 Arabica Mexico       various    7.25  7.25    7.58        10\n 8             86.2 Arabica Ethiopia     haider a…  8     8.08    8.08        10\n 9             78.6 Arabica Mexico       cofradia   7.92  7.92    7.5         10\n10             84.7 Arabica Peru         &lt;NA&gt;       7.67  7.92    7.83        10\n\n\n\n10.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n\n# A tibble: 10 × 9\n   rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n 2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n 3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n 4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n 5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n 6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n 7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n 8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n 9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n\n# A tibble: 3 × 9\n  rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n  &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1   446             85   Arabica Brazil  sitío sã…  8     7.67    7.67     10   \n2   892             83.1 Arabica Colomb… &lt;NA&gt;       7.83  7.42    7.92     10   \n3  1338             83.7 Arabica United… &lt;NA&gt;       7.75  7.75    8         9.33\n\n\n\n10.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n\n# A tibble: 96 × 8\n# Groups:   coo [37]\n   total_cup_points species coo      farm_name     aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             82.8 Arabica Brazil   capoeirinha    7.67  7.5     7.42        10\n 2             82.2 Arabica Brazil   caxambu        7.67  7.5     7.58        10\n 3             83   Arabica Brazil   fazenda sant…  7.67  7.42    7.5         10\n 4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n 5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n 6             82.3 Arabica China    yun lan coff…  7.5   7.42    7.42        10\n 7             83.6 Arabica China    menglian man…  7.67  7.67    7.75        10\n 8             84.5 Arabica China    puer jiangch…  7.67  7.75    7.58        10\n 9             83.8 Arabica Colombia &lt;NA&gt;           7.83  7.58    7.58        10\n10             83.9 Arabica Colombia &lt;NA&gt;           7.75  7.75    7.92        10\n# ℹ 86 more rows\n\n\n\n10.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n\n# A tibble: 5 × 8\n  total_cup_points species coo    farm_name   aroma  body balance sweetness\n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1             82.3 Arabica Brazil &lt;NA&gt;         7.42  7.5     7.58        10\n2             83.2 Arabica Brazil água limpa   7.75  7.58    7.58        10\n3             82.7 Arabica Brazil santa maria  7.33  7.67    7.5         10\n4             81.8 Arabica Brazil rio verde    7.17  7.75    7.25        10\n5             83   Arabica Brazil capoeirinha  7.5   7.58    7.42        10"
  },
  {
    "objectID": "intermediate_stat.html#basic-concept-comparison",
    "href": "intermediate_stat.html#basic-concept-comparison",
    "title": "\n10  표본 추출\n",
    "section": "\n10.3 표본추출 비교",
    "text": "10.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n10.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n\n[1] 82.1512\n\n\n\n10.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.30519\n\n\n\n10.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n\n[1] 82.0522\n\n\n\n10.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n\n[1] 80.54842"
  },
  {
    "objectID": "intermediate_stat.html#calculate-errors",
    "href": "intermediate_stat.html#calculate-errors",
    "title": "\n10  표본 추출\n",
    "section": "\n10.4 오차 측정",
    "text": "10.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n\n# A tibble: 1 × 4\n  population   srs stratifed cluster\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1       82.2  82.3      82.1    80.5\n\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n\n# A tibble: 4 × 3\n  method     estimation relative_error\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 population       82.2          0    \n2 srs              82.3          0.187\n3 stratifed        82.1          0.121\n4 cluster          80.5          1.95 \n\n\n\n10.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.05226\n\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n\n  [1] 82.23203 82.41060 82.21902 82.15038 82.29481 82.61955 82.33789 81.64466\n  [9] 82.00895 82.25940 82.59985 81.84677 81.83910 82.09511 82.33707 82.22308\n [17] 81.94211 82.07850 82.44759 82.28609 82.47602 82.29060 82.15789 82.28947\n [25] 82.37865 82.30263 82.39188 82.19263 82.41226 82.20165 82.00233 82.06647\n [33] 81.71241 82.31361 82.33263 82.12023 82.30188 82.04744 82.28105 82.25752\n [41] 81.94684 82.06068 81.79880 82.23647 82.84211 82.37203 82.11406 82.34000\n [49] 82.43624 82.43113 81.66158 82.08346 82.09992 82.02774 82.20556 82.19820\n [57] 81.96895 82.08602 81.92992 82.36120 81.81812 82.13662 82.21383 82.06173\n [65] 82.46594 81.91632 81.99331 82.44113 82.54376 82.45444 82.28075 82.45617\n [73] 82.29474 81.62850 82.06173 82.11466 82.46617 82.67143 82.32647 82.14150\n [81] 82.11962 82.51925 82.13797 81.84293 82.01639 82.13489 82.23579 81.99158\n [89] 82.63880 82.22729 81.98158 82.50925 82.44744 82.18368 82.33323 82.34098\n [97] 81.94985 81.91188 81.90188 82.25722\n\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n\n  [1] 81.49827 82.33940 82.18970 82.12195 82.59744 82.16805 82.35135 81.98383\n  [9] 82.26707 82.36812 81.96436 81.90556 82.08165 82.32586 82.35571 82.38820\n [17] 82.05165 82.19459 82.09767 82.12887 82.36030 82.38549 82.05414 82.28241\n [25] 82.14211 81.90015 81.96211 81.89902 81.66774 82.13444 81.87188 82.52895\n [33] 82.53632 82.27797 82.39308 82.19759 81.57737 82.25850 82.27774 82.21090\n [41] 82.25195 82.06579 81.95180 82.03789 81.62083 82.03504 82.09218 82.25459\n [49] 82.00113 82.26617 82.11286 82.12361 82.17564 82.15398 82.36008 82.08233\n [57] 81.94015 82.17398 82.30414 81.99346 82.06218 82.37068 81.94376 82.34105\n [65] 81.77752 82.43925 81.79556 82.13827 82.44737 81.85797 82.46135 81.90444\n [73] 81.79782 82.13180 82.29594 82.19541 81.85301 82.09910 82.43436 82.13872\n [81] 82.13075 82.05331 82.33180 82.42278 82.02865 82.52143 81.66895 81.83947\n [89] 82.41812 81.82729 82.25180 81.97353 82.08218 82.31699 82.13692 81.71083\n [97] 82.18338 82.00955 82.12308 81.98767\n\n\n\n10.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n10.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n\n# A tibble: 5 × 3\n  samp_prop mean_cup_points sd_cup_points\n  &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n1 10 %                 82.1        0.212 \n2 33 %                 82.2        0.111 \n3 50 %                 82.1        0.0783\n4 75 %                 82.1        0.0404\n5 90 %                 82.1        0.0266"
  },
  {
    "objectID": "intermediate_stat.html#calculate-bootstrap",
    "href": "intermediate_stat.html#calculate-bootstrap",
    "title": "\n10  표본 추출\n",
    "section": "\n10.5 신뢰구간",
    "text": "10.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n10.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n\n[1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n\n[1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n\n[1] 2.624232\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n\n[1] 2.488636\n\n\n\n10.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  82.0  82.4  82.8"
  },
  {
    "objectID": "intermediate_stat.html#footnotes",
    "href": "intermediate_stat.html#footnotes",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎"
  },
  {
    "objectID": "sampling.html#밀린_병원비_추정",
    "href": "sampling.html#밀린_병원비_추정",
    "title": "\n10  표본 추출\n",
    "section": "\n10.1 병원비 추정",
    "text": "10.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\n\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n\n# A tibble: 1 × 2\n  amount_est amount_var\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       40.9       35.7\n\n\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\n\n\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다."
  },
  {
    "objectID": "sampling.html#basic-concept",
    "href": "sampling.html#basic-concept",
    "title": "\n10  표본 추출\n",
    "section": "\n10.2 표본추출",
    "text": "10.2 표본추출\n\n10.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n\nRows: 1,338\nColumns: 8\n$ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n$ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n$ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n$ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n$ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n$ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n$ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n$ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n\n\n\n\n10.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   total_cup_points species coo        farm_name   aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             83.3 Arabica Costa Rica several      7.67  7.67    7.67        10\n 2             82.7 Arabica Guatemala  finca medi…  7.67  7.5     7.5         10\n 3             78.5 Arabica Mexico     various      7.25  7.25    7.58        10\n 4             85.4 Arabica Brazil     fazenda se…  8     7.75    8           10\n 5             78.3 Arabica Guatemala  various      7.17  7.17    7.17        10\n 6             81.7 Arabica Mexico     &lt;NA&gt;         7.67  7.5     7.25        10\n 7             83.2 Arabica Guatemala  finca medi…  7.5   7.58    7.5         10\n 8             83.3 Arabica Colombia   &lt;NA&gt;         7.5   7.67    7.58        10\n 9             83.5 Arabica Guatemala  santo toma…  7.83  7.67    7.5         10\n10             83.2 Arabica Guatemala  nueva gran…  7.33  7.33    7.67        10\n\n\n\n10.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n\n# A tibble: 10 × 9\n   rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n 2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n 3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n 4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n 5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n 6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n 7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n 8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n 9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n\n# A tibble: 3 × 9\n  rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n  &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1   446             81.2 Arabica Hondur… cerro bu…  7.42  7.33    7.25        10\n2   892             84.5 Arabica Mexico  finca el…  7.92  7.75    7.83        10\n3  1338             82.2 Arabica Hondur… bethel     7.58  7.42    7.42        10\n\n\n\n10.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n\n# A tibble: 96 × 8\n# Groups:   coo [37]\n   total_cup_points species coo      farm_name     aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             78.4 Arabica Brazil   santa maria    6.83  6.92    6.83        10\n 2             81   Arabica Brazil   cachoeira da…  7.33  7.33    7.17        10\n 3             73.5 Arabica Brazil   &lt;NA&gt;           7.25  7       6.42        10\n 4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n 5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n 6             84.5 Arabica China    puer jiangch…  7.67  7.75    7.58        10\n 7             83.2 Arabica China    chen lin       7.5   7.67    7.67        10\n 8             82.8 Arabica China    man ganna es…  7.67  7.67    7.67        10\n 9             83   Arabica Colombia &lt;NA&gt;           7.58  7.58    7.42        10\n10             83.5 Arabica Colombia &lt;NA&gt;           7.75  7.75    7.5         10\n# ℹ 86 more rows\n\n\n\n10.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n\n# A tibble: 5 × 8\n  total_cup_points species coo           farm_name aroma  body balance sweetness\n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1             82.1 Arabica Indonesia     various    7.5   7.5     7.42        10\n2             80.7 Arabica Indonesia     darmawi    7.58  7.5     7.5         10\n3             83.3 Arabica United State… &lt;NA&gt;       7.92  7.67    7.67        10\n4             81.5 Arabica United State… &lt;NA&gt;       7.33  7.5     7.83        10\n5             80.3 Arabica United State… &lt;NA&gt;       7.17  7.67    7.33        10"
  },
  {
    "objectID": "sampling.html#basic-concept-comparison",
    "href": "sampling.html#basic-concept-comparison",
    "title": "\n10  표본 추출\n",
    "section": "\n10.3 표본추출 비교",
    "text": "10.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n10.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n\n[1] 82.1512\n\n\n\n10.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.13789\n\n\n\n10.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n\n[1] 81.95732\n\n\n\n10.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n\n[1] 81.65486"
  },
  {
    "objectID": "sampling.html#calculate-errors",
    "href": "sampling.html#calculate-errors",
    "title": "\n10  표본 추출\n",
    "section": "\n10.4 오차 측정",
    "text": "10.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n\n# A tibble: 1 × 4\n  population   srs stratifed cluster\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1       82.2  82.1      82.0    81.7\n\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n\n# A tibble: 4 × 3\n  method     estimation relative_error\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 population       82.2         0     \n2 srs              82.1         0.0162\n3 stratifed        82.0         0.236 \n4 cluster          81.7         0.604 \n\n\n\n10.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.19203\n\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n\n  [1] 82.15835 82.59120 82.22406 82.15233 82.01256 82.03602 82.10098 82.01857\n  [9] 82.24218 82.68609 81.95293 81.79617 82.54105 81.91368 82.03451 82.23744\n [17] 82.11105 81.96466 82.57759 82.34887 81.91564 82.15880 82.52820 82.03534\n [25] 82.47654 82.28902 82.07015 82.35301 81.87038 82.16624 82.29248 82.19752\n [33] 82.16459 81.94436 82.00917 82.09278 82.16594 82.21541 81.93000 81.81211\n [41] 82.58090 82.27429 81.93038 81.85150 81.87098 82.40707 82.07865 81.90165\n [49] 81.91060 81.85594 82.25947 82.12556 82.43812 82.17023 82.02910 82.22797\n [57] 82.14226 82.11165 81.88955 81.84489 82.34774 81.54902 82.18188 82.09165\n [65] 81.84526 81.85338 81.87128 82.00308 81.72338 81.94293 81.91947 82.13105\n [73] 82.55263 82.20090 82.07737 82.25692 82.09586 81.95872 82.38038 82.61098\n [81] 82.29940 82.45632 82.12925 82.01000 81.79090 82.38865 81.77925 81.93970\n [89] 82.01699 82.40361 81.82150 82.21865 82.46180 81.93429 81.91970 82.34173\n [97] 82.10436 82.38820 82.02195 82.22835\n\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n\n  [1] 82.16571 81.88188 82.43504 82.68647 81.83128 81.80256 82.38203 82.12955\n  [9] 82.09947 81.82436 82.20308 82.01068 82.47526 82.45098 81.72361 82.08053\n [17] 81.88865 82.25053 82.71579 82.33098 82.21023 81.83985 82.12511 81.93759\n [25] 81.78195 82.12639 82.27865 82.21887 82.18932 82.37150 82.41414 82.02820\n [33] 82.77090 82.16316 82.33008 82.35842 82.04015 82.16511 81.80594 82.05932\n [41] 82.35639 81.86782 82.12812 82.07677 82.47842 82.09489 82.29865 82.32368\n [49] 82.07331 82.25850 82.09639 82.02519 82.04699 81.68015 82.45436 82.20654\n [57] 82.02459 81.63805 82.35008 82.10346 82.24865 82.26090 82.12068 82.17504\n [65] 82.03541 82.31226 82.57398 82.28286 81.87098 82.02113 82.20887 82.07226\n [73] 82.08827 82.16068 82.04143 82.23714 82.32353 82.28496 82.10925 82.68444\n [81] 82.07429 81.99767 82.28662 82.14248 82.30165 82.11211 82.29211 82.25211\n [89] 82.23722 82.05947 82.31594 82.13406 82.05850 82.44880 82.33481 81.95820\n [97] 82.40992 82.50293 82.20045 82.13564\n\n\n\n10.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n10.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n\n# A tibble: 5 × 3\n  samp_prop mean_cup_points sd_cup_points\n  &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n1 10 %                 82.2        0.219 \n2 33 %                 82.2        0.0904\n3 50 %                 82.2        0.0700\n4 75 %                 82.1        0.0409\n5 90 %                 82.1        0.0228"
  },
  {
    "objectID": "sampling.html#calculate-bootstrap",
    "href": "sampling.html#calculate-bootstrap",
    "title": "\n10  표본 추출\n",
    "section": "\n10.5 신뢰구간",
    "text": "10.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n10.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n\n[1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n\n[1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n\n[1] 2.558876\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n\n[1] 2.488636\n\n\n\n10.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  82.0  82.4  82.8"
  },
  {
    "objectID": "sampling.html#footnotes",
    "href": "sampling.html#footnotes",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎"
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference",
    "href": "hypothesis.html#computer-age-statistical-inference",
    "title": "11  코딩 가설검정",
    "section": "\n11.1 통계적 가설검정",
    "text": "11.1 통계적 가설검정\n통계적 가설 검정(統計的假說檢定, statistical hypothesis test)은 통계적 추측의 하나로서, 모집단 실제의 값이 얼마가 된다는 주장과 관련해, 표본의 정보를 사용해서 가설의 합당성 여부를 판정하는 과정을 의미하는데 이를 위해서 프로세스(Process)와 함께 검정 통계량을 수식으로 나타낼 수 있어야 하고 이를 해석하는 별도의 훈련도 받아야 했고 이 과정에서 상당량의 수학 및 통계학적 지식이 요구된다. 2\n\n유의수준의 결정, 귀무가설과 대립가설 설정\n검정통계량의 설정 (예를 들어, t-검정)\n\n\\(t_{검정통계량} \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\)\n자유도: \\(\\nu \\quad \\approx \\quad {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\\)\n\n\n\n기각역의 설정\n검정통계량 계산\n통계적인 의사결정\n\n통계적 가설검정(Statistical Testing)은 기존 통계학 전공자의 전유물이었으나, 컴퓨터의 일반화와 누구나 코딩을 할 수 있는 현재(2023-08-04)는 더 이상 기존 통념이 통용되지는 않게 되었다. 특히 파이썬 진영에서 이런 움직임이 활발하다. 그렇다고 R 진영에서도 기존의 방식을 고수하는 것은 아니다."
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "href": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "title": "11  코딩 가설검정",
    "section": "\n11.3 가설검정과 신뢰구간",
    "text": "11.3 가설검정과 신뢰구간\ninfer 팩키지는 tidyverse 철학(?)에 따라 가설검정과 신뢰구간을 추정하는 목적으로 개발되었다. 크게 통계적 추론은 가설검정과 신뢰구간 추정이 주된 작업이다. 이를 위해서 5가지 동사(verb)를 새로 익혀야 한다.\n\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nvisualize()\n\n\n\n가설검정과 신뢰구간"
  },
  {
    "objectID": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "href": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "title": "11  코딩 가설검정",
    "section": "\n11.4 사례: 맥주와 모기",
    "text": "11.4 사례: 맥주와 모기\n맥주를 마시는 사람이 말라리아 모기에게 매력적으로 보여 더 잘 물리는가? 라는 흥미로운 논문이 발표되었다. (Lefèvre 기타 2010) 이 연구는 맥주를 마신 후 사람의 냄새 (호흡 및 피부 방출 냄새)가 아노펠레스 감비아(Anopheles gambiae, 아프리카 주요 말라리아 매개체)에게 어떤 영향을 미치는지 조사하였다. 맥주를 마신 사람들의 몸 냄새는 모기의 활성화 (이륙 및 상향 풍속 비행에 참여하는 모기의 비율)와 방향성 (사람의 냄새를 향해 비행하는 모기의 비율)을 증가시켰다. 물을 마신 경우에는 사람이 모기에게 끌리는 것에 영향을 미치지 않았다.\n\n11.4.1 가설검정 환경설정\n데이터 전처리와 시각화, 한글설정을 위한 팩키지를 준비한다. 특히 infer 코딩기반 가설검정을 위해 필수적인 팩키지로 활용하는데 기본적인 사용방법은 mtcars, flights 데이터를 활용한 사례를 살펴본다.\n\nflights 데이터 소품문\nmtcars 데이터 소품문\n\n\n# 0. 환경설정 ----------\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(skimr)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(extrafont)\nloadfonts()\n\n# 1. 데이터 가져오기 -----\n\n# beer_dat &lt;- read_csv(\"https://raw.githubusercontent.com/aloy/m107/master/data/mosquitos.csv\")\nbeer_dat &lt;- read_csv(\"data/mosquitos.csv\")\n\nbeer_df &lt;- beer_dat %&gt;% \n    mutate(treatment = factor(treatment, levels = c(\"beer\", \"water\"), labels=c(\"맥주\", \"맹물\"))) \n\n\n11.4.2 탐색적 데이터 분석\n탐색적 데이터 분석을 통해서 말라리아 모기가 맥주를 마신 사람과 맹물을 마신 사람 어디에 더 많이 접근을 하는지 개체수 차이를 살펴본다. 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 우연에 의한 것인지 아니면 맥주가 더 모기에게 섹시하게 반응하는 역할을 하기 때문인지 살펴본다.\n\n# 2. 탐색적 데이터 분석 -----\n## 2.1. 전체 데이터 \nskim(beer_df)\n\n\nData summary\n\n\nName\nbeer_df\n\n\nNumber of rows\n43\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\n맥주: 25, 맹물: 18\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ncount\n0\n1\n21.77\n4.47\n12\n19\n21\n24\n31\n▂▃▇▅▂\n\n\n\n## 2.2. 맥주와 맹물 투여 집단 비교\nbeer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(최소 = min(count),\n                분위수_25 = quantile(count, 0.25),\n                평균 = mean(count),\n                중위수 = median(count),\n                분위수_75 = quantile(count, 0.75),\n                표준편차 = sd(count),\n                중위절대편차 = mad(count)) %&gt;% \n    mutate(맥주맹물차이 = max(평균) - min(평균))\n\n# A tibble: 2 × 9\n  treatment  최소 분위수_25  평균 중위수 분위수_75 표준편차 중위절대편차\n  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1 맥주         17      20    23.6     24        27     4.13         5.93\n2 맹물         12      16.5  19.2     20        22     3.67         2.97\n# ℹ 1 more variable: 맥주맹물차이 &lt;dbl&gt;\n\n## 2.3. 맥주와 맹물 투여 집단 비교 시각화\n\nbeer_density_g &lt;- ggplot(data = beer_df, mapping = aes(x = count, fill=treatment)) +\n    geom_density(aes(y = ..count..), alpha = 0.7) +\n    scale_x_continuous(limits=c(5,40)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    labs(title=\"맥주를 마시면 모기에게 섹시하게 보일까라고 쓰고 잘 물릴까라고 읽는다.\",\n        x=\"채집된 모기 개체수\", y=\"빈도수\", fill=\"실험처리(treatment): \")\n\nbeer_boxplot_g &lt;- ggplot(data = beer_df, mapping = aes(x = treatment, y = count, fill=treatment)) +\n    geom_boxplot(alpha = 0.5) +\n    geom_jitter(width = 0.2) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    coord_flip() +\n    theme(legend.position = \"none\") +\n    labs(title=\"\",\n         y=\"채집된 모기 개체수\", x=\"실험처리\", fill=\"실험처리(treatment): \")\n\ngrid.arrange(beer_density_g, beer_boxplot_g, nrow=2)\n\n\n\n\n\n\n\n\n11.4.3 의사결정\n맥주를 마신 집단과 맹물을 마신 집단간에 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 유의적인지 전통적인 t-검정과 코딩기반 모의실험을 통해서 살펴보자.\n\n11.4.3.1 전통 t-검정\nt-검정 결과 유의적인 차이가 나타나느 것으로 나타난다. p-값이 무척이나 작게 나온다.\n\n# 3. 통계 검정 -----\n## 3.1. 전통적인 해석적인 방법 (t-검정)\nt.test(count ~ treatment, beer_df, null = 0, var.equal = TRUE, alternative=\"greater\")\n\n\n    Two Sample t-test\n\ndata:  count by treatment\nt = 3.587, df = 41, p-value = 0.0004416\nalternative hypothesis: true difference in means between group 맥주 and group 맹물 is greater than 0\n95 percent confidence interval:\n 2.323889      Inf\nsample estimates:\nmean in group 맥주 mean in group 맹물 \n          23.60000           19.22222 \n\n\n\n11.4.3.2 코딩기반 t-검정\n코딩기반 t-검정은 다음 절차를 통해 준비한다.\n\n코딩기반 t-검정을 수행할 경우 beer_df가 \\(\\delta^*\\)에 해당되어 데이터에서 사전에 계산해 놓는다.\n가설검정 공식을 specify 함수에 명세한다.\n귀무가설을 hypothesize 함수에서 적시한다.\n컴퓨터에서 모의실험 난수를 generate에서 생성시킨다.\n검정 통계량을 calculate 함수에 명시한다.\n\n그리고 나서 p-값, 95% 신뢰구간을 모의실험결과에서 단순히 세어서 정리하면 된다.\n마지막으로 시각적으로 한번 더 확인한다. 즉, 4.38번 더 물리는 것은 극히 드물게 일어나는 사례로 맥주를 마시면 모기에 더 잘 물리게 된다고 볼 수 있다.\n\n## 3.2. `infer` 팩키지 -----\n\n### 3.2.1. 데이터에서 두 집단 간 차이 산출\nbeer_diff &lt;- beer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(mean = mean(count)) %&gt;% \n    summarise(abs(diff(mean))) %&gt;% \n    pull\n\n### 3.2.2. 귀무가설 모형에서 모의실험을 통해서 통계량 산출\nnull_model &lt;- beer_df %&gt;%\n    specify(count ~ treatment) %&gt;%\n    hypothesize(null = \"independence\") %&gt;% \n    generate(reps = 1000, type = \"permute\") %&gt;% \n    calculate(stat = \"diff in means\", order = c(\"맥주\", \"맹물\"))\n\n### 3.2.3. p-갑과 95% 신뢰구간: 백분위수(Percentile) 방법\nnull_model %&gt;%\n    summarize(p_value = mean(stat &gt; beer_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.001\n\nnull_model %&gt;%\n    summarize(l = quantile(stat, 0.025),\n              u = quantile(stat, 0.975))\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -2.69  2.66\n\n### 3.2.4. 시각화\nggplot(null_model, aes(x = stat, fill=\"gray75\")) +\n    geom_density(aes(y=..count..), alpha=0.7) +\n    geom_vline(xintercept = beer_diff, color = \"red\", size=1.5) +\n    scale_x_continuous(limits=c(-5,5)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(title=\"맥주를 마시고 4.38번 더 모기에 물린다면...  \",\n         x=\"맥주와 맹물 개체수 차이\", y=\"빈도수\")\n\n\n\n\n\n\n\n\n\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric Elguero, Didier Fontenille, François Renaud, Carlo Costantini, 와/과 Frédéric Thomas. 2010. “Beer consumption increases human attractiveness to malaria mosquitoes”. PloS one 5 (3): e9546."
  },
  {
    "objectID": "hypothesis.html#footnotes",
    "href": "hypothesis.html#footnotes",
    "title": "11  코딩 가설검정",
    "section": "",
    "text": "Allen Downey (2016), There is still only one test↩︎\n위키 백과 - 가설 검정↩︎\nHadley Wickham(2017-11-13), The tidy tools manifesto↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Lefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric\nElguero, Didier Fontenille, François Renaud, Carlo Costantini, and\nFrédéric Thomas. 2010. “Beer Consumption Increases Human\nAttractiveness to Malaria Mosquitoes.” PloS One 5 (3):\ne9546."
  },
  {
    "objectID": "hypothesis.html#computer-age-tidyverse-inference",
    "href": "hypothesis.html#computer-age-tidyverse-inference",
    "title": "11  코딩 가설검정",
    "section": "\n11.2 tidyverse 가설검정",
    "text": "11.2 tidyverse 가설검정\n데이터 과학을 이끌어 나가는 있는 R과 파이썬 진영의 현재 주도적인 흐름을 살펴보자. 우선 다소 차이가 있지만, for 반복루프를 이해하고 이를 코드로 구현할 수만 있다면 컴퓨터를 활용한 가설검정이 가능한 것은 사실이다. 하지만, 2011년 Allen Downey 교수가 주장했던 것처럼 오랜동안 검정된 해석학적 방법(Analytic Method)에 대한 교차검정하는 방식으로 활용하는 것이 추천된다. 3\n\n\nR과 파이썬 검정 가설검정 프레임워크 비교\n\n코딩기반 가설검정은 우선 데이터로부터 시작된다. 데이터를 컴퓨터의 기능을 활용하여 모의실험 표본을 생성하고 나서 귀무가설(\\(H_0\\)) 모형에서 검정통계량을 추출하여 이를 바탕으로 \\(p-값\\)을 계산하여 의사결정을 추진한다.\n통계검정에도 tidyverse를 반영하고 Allen Downey 교수가 주창한 통계검정 프레임워크를 도입하여 극단적으로 말하며 딥러닝 모형이 거의 모든 통계, 기계학습 모형을 통일해 나가듯이 다양한 통계검정에 대해서도 비숫한 위치를 점할 것으로 예측된다."
  },
  {
    "objectID": "NHST.html",
    "href": "NHST.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "R.A. Fisher는 NHST의 토대를 만들었으며 분산분석의 개념과 제한된 표본을 이용해 실험을 설계하는 실험계획법에 큰 기여를 했다. 1925년에 그가 발표한 ’Statistical Methods for Research Workers’라는 책에서 유의성 검정(significance test) 개념이 소개된 것을 확인할 수 있다. (Fisher 1970)\n귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 바탕으로 한 가설검정(hypothesis testing) 개념을 Neyman과 Pearson이 정립했고, 이를 적용한 최초의 사례는 1940년에 발표한 “Statistical Analysis in Educational Research”라는 책에서 NHST(Null Hypothesis Significance Testing) 개념을 처음으로 사용한 것으로 알려져 있다. (Lindquist 1940)\n\n\n\n\nflowchart TD\n    A[귀무가설 & 대립가설 설정]:::process\n    B[\"유의수준 선택 (예, 0.05)\"]:::process\n    C[데이터 수집 및 분석]:::process\n    D[\"검정 통계량 계산 (예, t-점수, z-점수)\"]:::process\n    E[검정 통계량을 임계값과 비교]:::decision\n    F[귀무가설 기각 또는 기각하지 않음]:::decision\n    G[\"결과 보고\"]:::report\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    classDef process fill:#efefef,stroke:#333,stroke-width:1px;\n    classDef decision fill:#ffefef,stroke:#333,stroke-width:1px;\n    classDef report fill:#efefff,stroke:#333,stroke-width:1px;\n\n\n\n\n\n\n\n\n\nFisher, Ronald Aylmer. 1970. “Statistical methods for research workers”. In Breakthroughs in statistics: Methodology and distribution, 66–70. Springer.\n\n\nLindquist, Everet Franklin. 1940. “Statistical analysis in educational research.”"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "\n13  함수\n",
    "section": "",
    "text": "14 함수 구성요소\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다."
  },
  {
    "objectID": "functions.html#defuse-and-inject-패턴",
    "href": "functions.html#defuse-and-inject-패턴",
    "title": "\n13  함수\n",
    "section": "\n13.1 Defuse-and-Inject 패턴",
    "text": "13.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n\n# A tibble: 11 × 5\n   species island    sex    year body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n 2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n 3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n 4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n 5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n 6 Adelie  Dream     &lt;NA&gt;   2007        2975\n 7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n 8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n 9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\n\n\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다."
  },
  {
    "objectID": "functions.html#역사",
    "href": "functions.html#역사",
    "title": "\n13  함수\n",
    "section": "\n13.2 역사",
    "text": "13.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다."
  },
  {
    "objectID": "functions.html#all-about-function",
    "href": "functions.html#all-about-function",
    "title": "\n13  함수\n",
    "section": "\n13.3 함수 기본 지식",
    "text": "13.3 함수 기본 지식\n함수는 입력값(x)를 넣어 어떤 작업(f)을 수행한 결과를 반환(y) 과정으로 이해할 수 있는데, 인자로 다양한 값을 함수에 넣을 수 있고, 물론 함수가 뭔가 유용한 작업을 수행하기 위한 전제조건을 만족시키는지 확인하는 과정을 assert 개념을 넣어 확인하고 기술된 작업을 수행한 후에 출력값을 변환시키게 된다."
  },
  {
    "objectID": "functions.html#why-write-function",
    "href": "functions.html#why-write-function",
    "title": "\n13  함수\n",
    "section": "\n13.4 왜 함수가 필요한가?",
    "text": "13.4 왜 함수가 필요한가?\n왜 함수가 필요한지를 데이터를 분석할 때 자주 나오는 변수 정규화 사례를 바탕으로 살펴보자. 데이터프레임에 담긴 변수의 측도가 상이하여 변수를 상대적으로 비교하기 위해 측도를 재조정하여 표준화할 필요가 있다. 변수에서 평균을 빼고 표준편차로 나누는 정규화도 있지만, 최대값에서 최소값을 빼서 분모에 두고 분자에 최소값을 빼서 나누면 모든 변수가 0–1 사이 값으로 척도가 조정된다.\n\\[ f(x)_{\\text{척도조정}} = \\frac{x-min(x)}{max(x)-min(x)} \\]\n\ndf &lt;- data.frame(a=c(1,2,3,4,5),\n                         b=c(10,20,30,40,50),\n                         c=c(7,8,6,1,3),\n                         d=c(5,4,6,5,2))\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) /\n        (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) /\n        (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\ndf        \n\n     a         b         c    d\n1 0.00  0.000000 0.8571429 0.75\n2 0.25 -1.111111 1.0000000 0.50\n3 0.50 -2.222222 0.7142857 1.00\n4 0.75 -3.333333 0.0000000 0.75\n5 1.00 -4.444444 0.2857143 0.00\n\n\n상기 R 코드는 측도를 모두 맞춰서 변수 4개(a, b, c, d)를 비교하거나 향후 분석을 위한 것이다. 하지만, 읽어야 하는 코드중복이 심하고 길어 코드를 작성한 개발자의 의도 가 본의 아니게 숨겨져 있다. 작성한 R 코드에 실수한 것이 있는 경우, 다음 프로그램 실행에서 버그(특히, 구문론이 아닌 의미론적 버그)가 숨겨지게 된다. 상기 코드가 작성되는 과정을 살펴보면 본의 아니게 의도가 숨겨진다는 의미가 어떤 것인지 명확해진다.\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) 코드를 작성한 후, 정상적으로 돌아가는지 확인한다.\n1번 코드가 잘 동작하게 되면 다음 복사하여 붙여넣기 신공을 사용하여 다른 칼럼 작업을 확장해 나간다. df$b, df$c, df$d를 생성하게 된다.\n즉, 복사해서 붙여넣은 것을 변수명을 편집해서 df$b, df$c, df$d 변수를 순차적으로 생성해 낸다.\n\n\n\n\n\n\n\n위캠 어록\n\n\n\n\n중복은 의도를 숨기게 되고, 복사하여 붙여넣기 두번하면 함수를 작성할 시점이 되었다. (Duplication hides the intent. If you have copied-and-pasted twice, it is time to write a function.)"
  },
  {
    "objectID": "functions.html#time-to-write-function",
    "href": "functions.html#time-to-write-function",
    "title": "\n13  함수\n",
    "section": "\n13.5 함수를 작성하는 시점",
    "text": "13.5 함수를 작성하는 시점\n복사해서 붙여넣는 것을 두번 하게 되면, 함수를 작성할 시점이다. 중복을 제거하는 한 방법이 함수를 작성하는 것이고, 함수를 작성하게 되면 의도가 명확해진다. 함수명을 rescale로 붙이고 이를 실행하게 되면, 의도가 명확하게 드러나게 되고, 복사해서 붙여넣게 되면서 생겨나는 중복과 반복에 의한 실수를 줄일 수 있게 되고, 향후 코드를 갱신할 때도 도움이 된다.\n\nrescale &lt;- function(x){\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a &lt;- rescale(df$a)\ndf$b &lt;- rescale(df$b)\ndf$c &lt;- rescale(df$c)\ndf$d &lt;- rescale(df$d)\n\nrescale() 함수를 사용해서 복사하여 붙여넣는 중복을 크게 줄였으나, 여전히 함수명을 반복해서 복사하여 붙여넣기를 통해 코드를 작성했다. 함수형 프로그래밍을 사용하는 것으로 함수명을 반복적으로 사용하는 것조차도 피할 수 있다.\n\nlibrary(purrr)\ndf &lt;- map_df(df, rescale)\ndf\n\n# A tibble: 5 × 4\n      a     b     c     d\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0     1    0.857  0.75\n2  0.25  0.75 1      0.5 \n3  0.5   0.5  0.714  1   \n4  0.75  0.25 0      0.75\n5  1     0    0.286  0   \n\n\n함수를 사용하지 않고 복사하여 붙여넣기 방식으로 코드를 작성한 경우 의도하지 않은 실수가 있어 함수를 도입하여 작성한 코드와 결과가 다른 것이 존재한다. 코드를 읽어 찾아보거나 실행한 후 결과를 통해 버그를 찾아보는 것도 함수의 의미와 중요성을 파악하는데 도움이 된다."
  },
  {
    "objectID": "functions.html#criteria-on-good-function",
    "href": "functions.html#criteria-on-good-function",
    "title": "\n13  함수\n",
    "section": "\n13.6 좋은 함수",
    "text": "13.6 좋은 함수\n좋은 함수를 작성하려면 다음과 같은 조건이 만족되어야 한다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 좋은 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable\n\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것."
  },
  {
    "objectID": "functions.html#function-component-argument",
    "href": "functions.html#function-component-argument",
    "title": "\n13  함수\n",
    "section": "\n14.1 인자(argument)",
    "text": "14.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)"
  },
  {
    "objectID": "functions.html#function-component-argument-assert",
    "href": "functions.html#function-component-argument-assert",
    "title": "\n13  함수\n",
    "section": "\n14.2 인자값 확인 - assert\n",
    "text": "14.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다. 다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n\n[1] 15\n\nsum_numbers(na_vector)\n\nError in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n\nError in if (assert_any_are_na(vec)) {: the condition has length &gt; 1"
  },
  {
    "objectID": "functions.html#function-component-return",
    "href": "functions.html#function-component-return",
    "title": "\n13  함수\n",
    "section": "\n14.3 반환값 확인",
    "text": "14.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\ninter\n\n[1] 37.88458\n\nslope\n\n[1] -2.87579\n\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 쪼개 저장시킨다.\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\niris_df %&gt;% \n  count(train_test)\n\n  train_test   n\n1       test  32\n2      train 118\n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\nglimpse(train)\n\nRows: 118\nColumns: 7\n$ Sepal.Length &lt;dbl&gt; 4.9, 4.7, 4.6, 5.0, 5.4, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.0, 3.2, 3.1, 3.6, 3.9, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.4, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ runif        &lt;dbl&gt; 0.6449041, 0.4740827, 0.5294489, 0.9515431, 0.4366536, 0.…\n$ train_test   &lt;chr&gt; \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"tr…\n\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n\n$intercept\n(Intercept) \n   37.88458 \n\n$beta\n     cyl \n-2.87579"
  },
  {
    "objectID": "functions.html#understand-function",
    "href": "functions.html#understand-function",
    "title": "\n13  함수\n",
    "section": "\n15.1 함수 이해",
    "text": "15.1 함수 이해\n함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 사이언스 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 비교도 겸해보자.\n\n\nR 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n\n[[1]]\n[1] 10\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 21\n\n[[4]]\n[1] 2.333333\n\n\n\n\n파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n\n[10, 4, 21, 2.3333333333333335]"
  },
  {
    "objectID": "functions.html#understand-argument",
    "href": "functions.html#understand-argument",
    "title": "\n13  함수\n",
    "section": "\n15.2 함수 인자",
    "text": "15.2 함수 인자\n함수를 구성하는 중요한 요소는 함수인자다."
  },
  {
    "objectID": "functions.html#call-function",
    "href": "functions.html#call-function",
    "title": "\n13  함수\n",
    "section": "\n15.3 함수 호출",
    "text": "15.3 함수 호출\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 사용하기 위해서 먼저 함수명을 알아야 되고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n\nfunction (x, na.rm = FALSE) \nNULL\n\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야 하고 따라서, 데이터프레임의 변수 하나(lifeExp)를 지정하여 전달하고 na.rm = TRUE도 명세하여 다시 전달해둔다. 이와 같이 인자값이 기본디폴트 설정된 경우 타이핑을 줄일 수 있고, 경우에 따라서 다른 인자를 넣어 전달시켜주면 된다.\n\nlibrary(gapminder)\nsd(gapminder$lifeExp, na.rm = TRUE)\n\n[1] 12.91711"
  },
  {
    "objectID": "functions.html#writing-funciton-dice",
    "href": "functions.html#writing-funciton-dice",
    "title": "\n13  함수\n",
    "section": "\n16.1 주사위",
    "text": "16.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n\n[1] 2\n\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n\n[1] 5\n\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  simulated_value &lt;- sample(dice, size=num_try)\n  return(simulated_value) # 불필요함.\n}\n\ndraw_dice(3)\n\n[1] 6 5 3"
  },
  {
    "objectID": "functions.html#how-to-write-function",
    "href": "functions.html#how-to-write-function",
    "title": "\n13  함수\n",
    "section": "\n16.2 함수작성 사례 - rescale\n",
    "text": "16.2 함수작성 사례 - rescale\n\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만들어본다.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)"
  },
  {
    "objectID": "functions.html#function-is-argument",
    "href": "functions.html#function-is-argument",
    "title": "\n13  함수\n",
    "section": "\n16.3 함수를 하나의 인자로 넘기는 함수 제작",
    "text": "16.3 함수를 하나의 인자로 넘기는 함수 제작\n기능 먼저 구현 추후 중복 제거하여 함수로 제작한다. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성해야 한다. 숫자 1,2,3 만 차이날 뿐 함수 중복이 심하다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n앞서 학습한 내용을 바탕으로 기초통계 함수를 제작해 본다. 여기서 기초통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘긴다는 점에 유의한다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 상정한다.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n\n[1] 0.5000000 0.5000000 0.7142857 0.7500000\n\ncol_summary(df, fun = mean)\n\n[1] 0.5000000 0.5000000 0.5714286 0.6000000\n\ncol_summary(df, fun = sd)\n\n[1] 0.3952847 0.3952847 0.4164966 0.3791438\n\n\n\n\npurrr 함수형 프로그래밍\nmap(.x, .f, ...) .x 원소 각각에 대해서 .f 함수를 적용시키는 연산작업을 한다.\n\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall."
  },
  {
    "objectID": "functions.html#funciton-component",
    "href": "functions.html#funciton-component",
    "title": "\n13  함수\n",
    "section": "\n13.7 함수 구성요소",
    "text": "13.7 함수 구성요소\n\n13.7.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n\n13.7.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다. 다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n\n[1] 15\n\nsum_numbers(na_vector)\n\nError in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n\nError in if (assert_any_are_na(vec)) {: the condition has length &gt; 1\n\n\n\n13.7.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\ninter\n\n[1] 37.88458\n\nslope\n\n[1] -2.87579\n\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 쪼개 저장시킨다.\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\niris_df %&gt;% \n  count(train_test)\n\n  train_test   n\n1       test  32\n2      train 118\n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\nglimpse(train)\n\nRows: 118\nColumns: 7\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 5.0, 4.4, 4.9, 5.4, 4.3, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 2.9, 3.1, 3.7, 3.0, 4.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.5, 1.4, 1.5, 1.5, 1.1, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.2, 0.2, 0.1, 0.2, 0.1, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ runif        &lt;dbl&gt; 0.3301616, 0.3573125, 0.3313473, 0.7636627, 0.7563461, 0.…\n$ train_test   &lt;chr&gt; \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"tr…\n\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n\n$intercept\n(Intercept) \n   37.88458 \n\n$beta\n     cyl \n-2.87579"
  },
  {
    "objectID": "functions.html#how-to-use-function",
    "href": "functions.html#how-to-use-function",
    "title": "\n13  함수\n",
    "section": "\n13.8 함수 사용하는 방법",
    "text": "13.8 함수 사용하는 방법\n\n13.8.1 함수 이해\n함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 사이언스 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 비교도 겸해보자.\n\n\nR 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n\n[[1]]\n[1] 10\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 21\n\n[[4]]\n[1] 2.333333\n\n\n\n\n파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n\n[10, 4, 21, 2.3333333333333335]\n\n\n\n\n\n13.8.2 함수 인자\n함수를 구성하는 중요한 요소는 함수인자다.\n\n13.8.3 함수 호출\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 사용하기 위해서 먼저 함수명을 알아야 되고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n\nfunction (x, na.rm = FALSE) \nNULL\n\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야 하고 따라서, 데이터프레임의 변수 하나(lifeExp)를 지정하여 전달하고 na.rm = TRUE도 명세하여 다시 전달해둔다. 이와 같이 인자값이 기본디폴트 설정된 경우 타이핑을 줄일 수 있고, 경우에 따라서 다른 인자를 넣어 전달시켜주면 된다.\n\nlibrary(gapminder)\nsd(gapminder$lifeExp, na.rm = TRUE)\n\n[1] 12.91711"
  },
  {
    "objectID": "functions.html#convert-scripts-to-function",
    "href": "functions.html#convert-scripts-to-function",
    "title": "\n13  함수\n",
    "section": "\n13.9 스크립트 → 함수",
    "text": "13.9 스크립트 → 함수\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다.\n\nR 함수 템플릿을 제작한다.\n\n함수명 &lt;- function() { }\n\n\n스크립트를 함수 몸통에 복사하여 붙인다.\n반복작업되는 인자를 찾아내 이를 인자로 넣어둔다.\n인자값과 연동되는 부분을 찾아 맞춰준다.\n함수명을 적절한 동사를 갖춘 이름으로 작명한다.\n\nreturn이 불필요하기 때문에 R 언어 특성을 반영하여 필요한 경우 제거한다.\n\n\n13.9.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n\n[1] 3\n\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n\n[1] 4\n\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  simulated_value &lt;- sample(dice, size=num_try)\n  return(simulated_value) # 불필요함.\n}\n\ndraw_dice(3)\n\n[1] 6 2 4\n\n\n\n13.9.2 함수작성 사례 - rescale\n\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만들어본다.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)\n\n\n13.9.3 함수를 하나의 인자로 넘기는 함수 제작\n기능 먼저 구현 추후 중복 제거하여 함수로 제작한다. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성해야 한다. 숫자 1,2,3 만 차이날 뿐 함수 중복이 심하다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n앞서 학습한 내용을 바탕으로 기초통계 함수를 제작해 본다. 여기서 기초통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘긴다는 점에 유의한다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 상정한다.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n\n[1] 0.5000000 0.5000000 0.7142857 0.7500000\n\ncol_summary(df, fun = mean)\n\n[1] 0.5000000 0.5000000 0.5714286 0.6000000\n\ncol_summary(df, fun = sd)\n\n[1] 0.3952847 0.3952847 0.4164966 0.3791438\n\n\n\n\npurrr 함수형 프로그래밍\nmap(.x, .f, ...) .x 원소 각각에 대해서 .f 함수를 적용시키는 연산작업을 한다.\n\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall."
  }
]
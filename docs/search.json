[
  {
    "objectID": "basic_stat.html",
    "href": "basic_stat.html",
    "title": "\n9  통계\n",
    "section": "",
    "text": "9.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#기술통계",
    "href": "basic_stat.html#기술통계",
    "title": "\n9  통계\n",
    "section": "\n9.2 기술통계",
    "text": "9.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n\n\n\n\n9.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n\n# A tibble: 1 × 2\n  평균_체중 최빈종\n      &lt;dbl&gt; &lt;fct&gt; \n1     4202. Adelie\n\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\nprint(f'펭귄 최빈종: {mode_species}')\n\n\n\n\n\n9.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  분산_체중 표준편차_체중\n      &lt;dbl&gt;         &lt;dbl&gt;\n1   643131.          802.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n\n# A tibble: 3 × 2\n  species   빈도수\n  &lt;fct&gt;      &lt;int&gt;\n1 Adelie       152\n2 Gentoo       124\n3 Chinstrap     68\n\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#가능성",
    "href": "basic_stat.html#가능성",
    "title": "\n9  통계\n",
    "section": "\n9.3 가능성",
    "text": "9.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n9.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"세종\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"강원\" \"전남\" \"대전\" \"전남\" \"광주\" \"경북\" \"광주\" \"경기\" \"강원\" \"세종\"\n[11] \"경북\" \"부산\" \"경북\" \"부산\" \"전남\" \"서울\" \"광주\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.05882353\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0598\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n대전\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n인천\n부산\n세종\n서울\n세종\n울산\n충북\n대구\n충남\n서울\n충북\n울산\n울산\n강원\n경기\n울산\n경남\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.0\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0601\n\n\n\n\n\n\n9.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}\nPr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\\n                   &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)\n\\end{aligned}\\)\n넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n\n[1] 0.1121076\n\nmean(서건창 | 이정후)\n\n[1] 0.5470852\n\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n\n[1] 0.5470852\n\n\n두 선수가 동시에 안타를 칠 확률은 0.11이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.55이 된다.\n200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n\n[1] 0.11882\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n\n[1] 0.1189903\n\n\n\n9.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\nR 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n\n[1] 4.9931\n\nmean(binom_df$y)\n\n[1] 10.0144\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n\n# A tibble: 1 × 1\n  mean_z\n   &lt;dbl&gt;\n1   15.0",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#분포",
    "href": "basic_stat.html#분포",
    "title": "\n9  통계\n",
    "section": "\n9.4 분포",
    "text": "9.4 분포",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "basic_stat.html#footnotes",
    "href": "basic_stat.html#footnotes",
    "title": "\n9  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>통계</span>"
    ]
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "10.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n\n# A tibble: 1 × 2\n  amount_est amount_var\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       40.9       35.7\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#밀린_병원비_추정",
    "href": "sampling.html#밀린_병원비_추정",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\n\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept",
    "href": "sampling.html#basic-concept",
    "title": "\n10  표본 추출\n",
    "section": "\n10.2 표본추출",
    "text": "10.2 표본추출\n\n10.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n\nRows: 1,338\nColumns: 8\n$ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n$ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n$ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n$ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n$ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n$ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n$ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n$ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n\n10.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   total_cup_points species coo         farm_name  aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             81.2 Arabica Philippines irisan, b…  7.25  7.58    7.5         10\n 2             80.7 Arabica Colombia    &lt;NA&gt;        7.17  7.08    7.42        10\n 3             83.4 Arabica El Salvador la joya     7.58  7.75    7.67        10\n 4             80.3 Arabica Guatemala   la castel…  7.33  7       7.17        10\n 5             82.8 Arabica Guatemala   providenc…  7.5   7.58    7.5         10\n 6             80.7 Arabica Guatemala   chiquimul…  7     7       7.33        10\n 7             83.9 Arabica Mexico      finca san…  7.5   7.75    7.75        10\n 8             82.1 Arabica Brazil      rio verde   7.25  7.25    7.92        10\n 9             83.4 Arabica Thailand    matsuzawa…  7.5   7.75    7.67        10\n10             81.8 Arabica Brazil      rio verde   7.33  7.83    7.33        10\n\n\n\n10.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n\n# A tibble: 10 × 9\n   rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n 2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n 3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n 4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n 5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n 6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n 7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n 8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n 9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n\n# A tibble: 3 × 9\n  rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n  &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1   446             83.8 Arabica Ethiop… phone nu…  7.58  7.83    7.5      10   \n2   892             81.5 Arabica Taiwan  good moo…  7.42  7.42    7.5       9.33\n3  1338             78   Arabica Colomb… &lt;NA&gt;       7.42  7.67    7.17      9.33\n\n\n\n10.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n\n# A tibble: 96 × 8\n# Groups:   coo [37]\n   total_cup_points species coo      farm_name     aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             83.2 Arabica Brazil   campo das fl…  7.75  7.58    7.58     10   \n 2             84.9 Arabica Brazil   fazenda grot…  8     8       7.75     10   \n 3             83.8 Arabica Brazil   rio verde      7.67  7.83    8.17      9.33\n 4             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75     10   \n 5             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08     10   \n 6             87.2 Arabica China    echo coffee    8.42  7.92    8        10   \n 7             82.4 Arabica China    menglian gao…  7.67  7.42    7.42     10   \n 8             78.7 Arabica China    alicia's farm  7     7.17    7.17      9.33\n 9             84.2 Arabica Colombia &lt;NA&gt;           7.75  7.83    7.75     10   \n10             82.9 Arabica Colombia &lt;NA&gt;           7.58  7.67    7.58     10   \n# ℹ 86 more rows\n\n\n\n10.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n\n# A tibble: 5 × 8\n  total_cup_points species coo      farm_name      aroma  body balance sweetness\n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1             81.6 Arabica Malawi   kavuzi          7.42  7.33    7.33        10\n2             81.2 Arabica Thailand &lt;NA&gt;            6.83  7.5     7.42        10\n3             84.8 Arabica Thailand matsuzawa cof…  7.75  8       7.75        10\n4             83.8 Arabica Thailand doi chaang co…  7.75  7.67    7.58        10\n5             82.9 Arabica Thailand doi tung deve…  7.42  7.5     8           10",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#basic-concept-comparison",
    "href": "sampling.html#basic-concept-comparison",
    "title": "\n10  표본 추출\n",
    "section": "\n10.3 표본추출 비교",
    "text": "10.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n10.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n\n[1] 82.1512\n\n\n\n10.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.34053\n\n\n\n10.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n\n[1] 82.25276\n\n\n\n10.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n\n[1] 80.12235",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-errors",
    "href": "sampling.html#calculate-errors",
    "title": "\n10  표본 추출\n",
    "section": "\n10.4 오차 측정",
    "text": "10.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n\n# A tibble: 1 × 4\n  population   srs stratifed cluster\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1       82.2  82.3      82.3    80.1\n\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n\n# A tibble: 4 × 3\n  method     estimation relative_error\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 population       82.2          0    \n2 srs              82.3          0.230\n3 stratifed        82.3          0.124\n4 cluster          80.1          2.47 \n\n\n\n10.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.19165\n\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n\n  [1] 82.28910 82.22511 82.44865 81.79030 82.18895 82.28602 81.93398 82.16669\n  [9] 82.44263 81.88150 82.04083 82.01519 82.44579 81.79023 81.98820 82.10662\n [17] 82.16571 81.77722 82.48150 82.33707 82.15068 82.47970 82.04263 82.42812\n [25] 81.99278 82.14045 82.07940 82.34436 82.35406 82.34722 81.97707 82.33519\n [33] 82.27902 82.16744 81.99699 82.34782 82.19429 82.22534 82.04842 81.58609\n [41] 81.69188 82.48827 82.52692 82.34752 82.16955 82.23511 82.48233 81.56188\n [49] 82.31188 81.85571 82.53579 82.66857 82.39353 82.36902 82.46338 82.05015\n [57] 82.28128 82.27105 81.80571 82.27526 82.07414 82.00256 82.08662 82.06444\n [65] 82.04556 81.98812 82.09451 82.15556 82.35158 82.25797 82.42759 81.90053\n [73] 82.12128 82.02128 82.13835 81.87474 81.95338 82.36053 81.77835 82.37669\n [81] 82.42992 82.31857 82.34203 82.46188 82.13120 82.39511 81.81436 82.35496\n [89] 82.31466 82.10842 82.30466 81.74632 82.00835 82.34165 81.77489 82.13857\n [97] 81.93271 81.83812 81.57677 82.13038\n\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n\n  [1] 82.06579 82.19805 82.32586 82.15820 82.28083 82.35143 82.61331 82.14970\n  [9] 82.44113 81.96286 82.02504 82.14496 82.19564 81.71955 81.85541 82.20955\n [17] 82.18376 82.29902 82.40647 82.21880 81.59015 82.26188 82.04376 82.17797\n [25] 82.33496 82.26932 81.87872 82.67353 81.98301 82.23000 81.93737 81.96759\n [33] 82.03353 82.21113 82.30541 81.65947 81.90489 82.17496 82.00045 82.22962\n [41] 82.40068 82.56759 82.11451 82.35436 82.23511 82.68812 82.34850 82.12203\n [49] 81.97707 82.57504 81.66842 82.39684 82.17293 81.96383 82.35406 81.98233\n [57] 81.97677 82.18902 82.66955 82.17519 82.14714 82.31692 81.85481 81.84662\n [65] 81.97128 81.94789 82.03789 82.24105 81.95564 82.43541 82.16962 82.12925\n [73] 82.12466 82.08286 82.21782 81.79083 82.03759 81.80647 82.46346 81.93241\n [81] 81.92248 81.95797 82.48714 82.10150 82.21376 82.13865 81.95692 81.90293\n [89] 82.13316 82.24008 82.28308 82.02030 82.15165 82.15955 82.21654 82.23308\n [97] 82.11496 82.44331 82.41383 82.38361\n\n\n\n10.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n10.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n\n# A tibble: 5 × 3\n  samp_prop mean_cup_points sd_cup_points\n  &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n1 10 %                 82.2        0.218 \n2 33 %                 82.2        0.0990\n3 50 %                 82.2        0.0711\n4 75 %                 82.1        0.0422\n5 90 %                 82.1        0.0258",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#calculate-bootstrap",
    "href": "sampling.html#calculate-bootstrap",
    "title": "\n10  표본 추출\n",
    "section": "\n10.5 신뢰구간",
    "text": "10.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n10.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n\n[1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n\n[1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n\n[1] 2.591998\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n\n[1] 2.488636\n\n\n\n10.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  82.0  82.4  82.8",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "sampling.html#footnotes",
    "href": "sampling.html#footnotes",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>표본 추출</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "11  코딩 가설검정",
    "section": "",
    "text": "11.1 통계적 가설검정\n통계적 가설 검정(統計的假說檢定, statistical hypothesis test)은 통계적 추측의 하나로서, 모집단 실제의 값이 얼마가 된다는 주장과 관련해, 표본의 정보를 사용해서 가설의 합당성 여부를 판정하는 과정을 의미하는데 이를 위해서 프로세스(Process)와 함께 검정 통계량을 수식으로 나타낼 수 있어야 하고 이를 해석하는 별도의 훈련도 받아야 했고 이 과정에서 상당량의 수학 및 통계학적 지식이 요구된다. 2\n통계적 가설검정(Statistical Testing)은 기존 통계학 전공자의 전유물이었으나, 컴퓨터의 일반화와 누구나 코딩을 할 수 있는 현재(2024-01-21)는 더 이상 기존 통념이 통용되지는 않게 되었다. 특히 파이썬 진영에서 이런 움직임이 활발하다. 그렇다고 R 진영에서도 기존의 방식을 고수하는 것은 아니다.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference",
    "href": "hypothesis.html#computer-age-statistical-inference",
    "title": "11  코딩 가설검정",
    "section": "",
    "text": "유의수준의 결정, 귀무가설과 대립가설 설정\n검정통계량의 설정 (예를 들어, t-검정)\n\n\\(t_{검정통계량} \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\)\n자유도: \\(\\nu \\quad  \\approx \\quad {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\\)\n\n\n\n기각역의 설정\n검정통계량 계산\n통계적인 의사결정",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-tidyverse-inference",
    "href": "hypothesis.html#computer-age-tidyverse-inference",
    "title": "11  코딩 가설검정",
    "section": "\n11.2 tidyverse 가설검정",
    "text": "11.2 tidyverse 가설검정\n데이터 과학을 이끌어 나가는 있는 R과 파이썬 진영의 현재 주도적인 흐름을 살펴보자. 우선 다소 차이가 있지만, for 반복루프를 이해하고 이를 코드로 구현할 수만 있다면 컴퓨터를 활용한 가설검정이 가능한 것은 사실이다. 하지만, 2011년 Allen Downey 교수가 주장했던 것처럼 오랜동안 검정된 해석학적 방법(Analytic Method)에 대한 교차검정하는 방식으로 활용하는 것이 추천된다. 3\n\n\nR과 파이썬 검정 가설검정 프레임워크 비교\n\n코딩기반 가설검정은 우선 데이터로부터 시작된다. 데이터를 컴퓨터의 기능을 활용하여 모의실험 표본을 생성하고 나서 귀무가설(\\(H_0\\)) 모형에서 검정통계량을 추출하여 이를 바탕으로 \\(p-값\\)을 계산하여 의사결정을 추진한다.\n통계검정에도 tidyverse를 반영하고 Allen Downey 교수가 주창한 통계검정 프레임워크를 도입하여 극단적으로 말하며 딥러닝 모형이 거의 모든 통계, 기계학습 모형을 통일해 나가듯이 다양한 통계검정에 대해서도 비숫한 위치를 점할 것으로 예측된다.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "href": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "title": "11  코딩 가설검정",
    "section": "\n11.3 가설검정과 신뢰구간",
    "text": "11.3 가설검정과 신뢰구간\ninfer 팩키지는 tidyverse 철학(?)에 따라 가설검정과 신뢰구간을 추정하는 목적으로 개발되었다. 크게 통계적 추론은 가설검정과 신뢰구간 추정이 주된 작업이다. 이를 위해서 5가지 동사(verb)를 새로 익혀야 한다.\n\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nvisualize()\n\n\n\n가설검정과 신뢰구간",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "href": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "title": "11  코딩 가설검정",
    "section": "\n11.4 사례: 맥주와 모기",
    "text": "11.4 사례: 맥주와 모기\n맥주를 마시는 사람이 말라리아 모기에게 매력적으로 보여 더 잘 물리는가? 라는 흥미로운 논문이 발표되었다. (Lefèvre 기타 2010) 이 연구는 맥주를 마신 후 사람의 냄새 (호흡 및 피부 방출 냄새)가 아노펠레스 감비아(Anopheles gambiae, 아프리카 주요 말라리아 매개체)에게 어떤 영향을 미치는지 조사하였다. 맥주를 마신 사람들의 몸 냄새는 모기의 활성화 (이륙 및 상향 풍속 비행에 참여하는 모기의 비율)와 방향성 (사람의 냄새를 향해 비행하는 모기의 비율)을 증가시켰다. 물을 마신 경우에는 사람이 모기에게 끌리는 것에 영향을 미치지 않았다.\n\n11.4.1 가설검정 환경설정\n데이터 전처리와 시각화, 한글설정을 위한 팩키지를 준비한다. 특히 infer 코딩기반 가설검정을 위해 필수적인 팩키지로 활용하는데 기본적인 사용방법은 mtcars, flights 데이터를 활용한 사례를 살펴본다.\n\nflights 데이터 소품문\nmtcars 데이터 소품문\n\n\n# 0. 환경설정 ----------\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(skimr)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(extrafont)\nloadfonts()\n\n# 1. 데이터 가져오기 -----\n\n# beer_dat &lt;- read_csv(\"https://raw.githubusercontent.com/aloy/m107/master/data/mosquitos.csv\")\nbeer_dat &lt;- read_csv(\"data/mosquitos.csv\")\n\nbeer_df &lt;- beer_dat %&gt;% \n    mutate(treatment = factor(treatment, levels = c(\"beer\", \"water\"), labels=c(\"맥주\", \"맹물\"))) \n\n\n11.4.2 탐색적 데이터 분석\n탐색적 데이터 분석을 통해서 말라리아 모기가 맥주를 마신 사람과 맹물을 마신 사람 어디에 더 많이 접근을 하는지 개체수 차이를 살펴본다. 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 우연에 의한 것인지 아니면 맥주가 더 모기에게 섹시하게 반응하는 역할을 하기 때문인지 살펴본다.\n\n# 2. 탐색적 데이터 분석 -----\n## 2.1. 전체 데이터 \nskim(beer_df)\n\n\nData summary\n\n\nName\nbeer_df\n\n\nNumber of rows\n43\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\n맥주: 25, 맹물: 18\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ncount\n0\n1\n21.77\n4.47\n12\n19\n21\n24\n31\n▂▃▇▅▂\n\n\n\n## 2.2. 맥주와 맹물 투여 집단 비교\nbeer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(최소 = min(count),\n                분위수_25 = quantile(count, 0.25),\n                평균 = mean(count),\n                중위수 = median(count),\n                분위수_75 = quantile(count, 0.75),\n                표준편차 = sd(count),\n                중위절대편차 = mad(count)) %&gt;% \n    mutate(맥주맹물차이 = max(평균) - min(평균))\n\n# A tibble: 2 × 9\n  treatment  최소 분위수_25  평균 중위수 분위수_75 표준편차 중위절대편차\n  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1 맥주         17      20    23.6     24        27     4.13         5.93\n2 맹물         12      16.5  19.2     20        22     3.67         2.97\n# ℹ 1 more variable: 맥주맹물차이 &lt;dbl&gt;\n\n## 2.3. 맥주와 맹물 투여 집단 비교 시각화\n\nbeer_density_g &lt;- ggplot(data = beer_df, mapping = aes(x = count, fill=treatment)) +\n    geom_density(aes(y = ..count..), alpha = 0.7) +\n    scale_x_continuous(limits=c(5,40)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    labs(title=\"맥주를 마시면 모기에게 섹시하게 보일까라고 쓰고 잘 물릴까라고 읽는다.\",\n        x=\"채집된 모기 개체수\", y=\"빈도수\", fill=\"실험처리(treatment): \")\n\nbeer_boxplot_g &lt;- ggplot(data = beer_df, mapping = aes(x = treatment, y = count, fill=treatment)) +\n    geom_boxplot(alpha = 0.5) +\n    geom_jitter(width = 0.2) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    coord_flip() +\n    theme(legend.position = \"none\") +\n    labs(title=\"\",\n         y=\"채집된 모기 개체수\", x=\"실험처리\", fill=\"실험처리(treatment): \")\n\ngrid.arrange(beer_density_g, beer_boxplot_g, nrow=2)\n\n\n\n\n\n\n\n\n11.4.3 의사결정\n맥주를 마신 집단과 맹물을 마신 집단간에 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 유의적인지 전통적인 t-검정과 코딩기반 모의실험을 통해서 살펴보자.\n전통 t-검정\nt-검정 결과 유의적인 차이가 나타나느 것으로 나타난다. p-값이 무척이나 작게 나온다.\n\n# 3. 통계 검정 -----\n## 3.1. 전통적인 해석적인 방법 (t-검정)\nt.test(count ~ treatment, beer_df, null = 0, var.equal = TRUE, alternative=\"greater\")\n\n\n    Two Sample t-test\n\ndata:  count by treatment\nt = 3.587, df = 41, p-value = 0.0004416\nalternative hypothesis: true difference in means between group 맥주 and group 맹물 is greater than 0\n95 percent confidence interval:\n 2.323889      Inf\nsample estimates:\nmean in group 맥주 mean in group 맹물 \n          23.60000           19.22222 \n\n\n코딩기반 t-검정\n코딩기반 t-검정은 다음 절차를 통해 준비한다.\n\n코딩기반 t-검정을 수행할 경우 beer_df가 \\(\\delta^*\\)에 해당되어 데이터에서 사전에 계산해 놓는다.\n가설검정 공식을 specify 함수에 명세한다.\n귀무가설을 hypothesize 함수에서 적시한다.\n컴퓨터에서 모의실험 난수를 generate에서 생성시킨다.\n검정 통계량을 calculate 함수에 명시한다.\n\n그리고 나서 p-값, 95% 신뢰구간을 모의실험결과에서 단순히 세어서 정리하면 된다.\n마지막으로 시각적으로 한번 더 확인한다. 즉, 4.38번 더 물리는 것은 극히 드물게 일어나는 사례로 맥주를 마시면 모기에 더 잘 물리게 된다고 볼 수 있다.\n\n## 3.2. `infer` 팩키지 -----\n\n### 3.2.1. 데이터에서 두 집단 간 차이 산출\nbeer_diff &lt;- beer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(mean = mean(count)) %&gt;% \n    summarise(abs(diff(mean))) %&gt;% \n    pull\n\n### 3.2.2. 귀무가설 모형에서 모의실험을 통해서 통계량 산출\nnull_model &lt;- beer_df %&gt;%\n    specify(count ~ treatment) %&gt;%\n    hypothesize(null = \"independence\") %&gt;% \n    generate(reps = 1000, type = \"permute\") %&gt;% \n    calculate(stat = \"diff in means\", order = c(\"맥주\", \"맹물\"))\n\n### 3.2.3. p-갑과 95% 신뢰구간: 백분위수(Percentile) 방법\nnull_model %&gt;%\n    summarize(p_value = mean(stat &gt; beer_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.002\n\nnull_model %&gt;%\n    summarize(l = quantile(stat, 0.025),\n              u = quantile(stat, 0.975))\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -2.60  2.75\n\n### 3.2.4. 시각화\nggplot(null_model, aes(x = stat, fill=\"gray75\")) +\n    geom_density(aes(y=..count..), alpha=0.7) +\n    geom_vline(xintercept = beer_diff, color = \"red\", size=1.5) +\n    scale_x_continuous(limits=c(-5,5)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(title=\"맥주를 마시고 4.38번 더 모기에 물린다면...  \",\n         x=\"맥주와 맹물 개체수 차이\", y=\"빈도수\")\n\n\n\n\n\n\n\n\n\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric Elguero, Didier Fontenille, François Renaud, Carlo Costantini, 와/과 Frédéric Thomas. 2010. “Beer consumption increases human attractiveness to malaria mosquitoes”. PloS one 5 (3): e9546.",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#footnotes",
    "href": "hypothesis.html#footnotes",
    "title": "11  코딩 가설검정",
    "section": "",
    "text": "Allen Downey (2016), There is still only one test↩︎\n위키 백과 - 가설 검정↩︎\nHadley Wickham(2017-11-13), The tidy tools manifesto↩︎",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "NHST.html",
    "href": "NHST.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "R.A. Fisher는 NHST의 토대를 만들었으며 분산분석의 개념과 제한된 표본을 이용해 실험을 설계하는 실험계획법에 큰 기여를 했다. 1925년에 그가 발표한 ’Statistical Methods for Research Workers’라는 책에서 유의성 검정(significance test) 개념이 소개된 것을 확인할 수 있다. (Fisher 1970)\n귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 바탕으로 한 가설검정(hypothesis testing) 개념을 Neyman과 Pearson이 정립했고, 이를 적용한 최초의 사례는 1940년에 발표한 “Statistical Analysis in Educational Research”라는 책에서 NHST(Null Hypothesis Significance Testing) 개념을 처음으로 사용한 것으로 알려져 있다. (Lindquist 1940)\n\n\n\n\n\nflowchart TD\n    A[귀무가설 & 대립가설 설정]:::process\n    B[\"유의수준 선택 (예, 0.05)\"]:::process\n    C[데이터 수집 및 분석]:::process\n    D[\"검정 통계량 계산 (예, t-점수, z-점수)\"]:::process\n    E[검정 통계량을 임계값과 비교]:::decision\n    F[귀무가설 기각 또는 기각하지 않음]:::decision\n    G[\"결과 보고\"]:::report\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    classDef process fill:#efefef,stroke:#333,stroke-width:1px;\n    classDef decision fill:#ffefef,stroke:#333,stroke-width:1px;\n    classDef report fill:#efefff,stroke:#333,stroke-width:1px;\n\n\n\n\n\n\n\n\n\n\nFisher, Ronald Aylmer. 1970. “Statistical methods for research workers”. In Breakthroughs in statistics: Methodology and distribution, 66–70. Springer.\n\n\nLindquist, Everet Franklin. 1940. “Statistical analysis in educational research.”",
    "crumbs": [
      "**3부** 데이터 과학",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>NHST.html</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "\n13  모형\n",
    "section": "",
    "text": "13.1 회귀분석\n회귀분석은 갤톤(Galton) 부모와 자식의 신장간의 관계를 회귀식으로 표현한 데이터셋이 유명하다. (Caffo 2015) (Friendly 2023) 부모의 신장을 기초로 자녀의 신장을 예측하는 회귀식을 구하기 전에 산점도를 통해 관계를 살펴보면 다음과 같다. 성별에 대한 신장의 차이도 산점도를 통해 시각적으로 확인된다.\nlibrary(tidyverse)\nlibrary(HistData)\ndata(GaltonFamilies)\n\nlibrary(ggplot2)\n\n## 1. 산점도 \n# 성별 색상으로 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n\n\n\n\n\n\n# 다른 산점도로 성별 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight, colours=gender)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\") +\n    facet_wrap(~gender)\n\n\n\n\n\n\n# 성별 상관없는 회귀직선\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point() +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n선형대수로 회귀계수를 추정하는 문제를 풀면 다음과 같이 정의된다. 한번 미분해서 \\(\\nabla f(\\beta ) = -2Xy + X^t X \\beta =0\\) 0으로 놓고 푼 값은 최소값이 되는데 이유는 \\(\\beta\\)에 대해서 두번 미분하게 되면 \\(2 X^t X\\) 로 양수가 되기 때문이다.\n\\[f(\\beta ) = ||y - \\beta X ||^2 = (y - \\beta X)^t (y - \\beta X) = y^t y - 2 y^t X^t \\beta + \\beta^t X^t X \\beta\\]\n\\[\\nabla f(\\beta ) = -2Xy + X^t X \\beta\\]\n\\[\\beta = (X^t X)^{-1} X^t y \\]\n위에서 정의된 방식으로 수식을 정의하고 이를 R로 코딩하면 회귀계수를 다음과 같이 구할 수 있다.\n## 2. 회귀분석\n# 선형대수 수식으로 계산\n\nx &lt;- GaltonFamilies$midparentHeight\ny &lt;- GaltonFamilies$childHeight\n\nx &lt;- cbind(1, x)\n\nsolve(t(x) %*% x) %*% t(x) %*% y\n\n        [,1]\n  22.6362405\nx  0.6373609\n이를 lm 함수를 사용해서 다시 풀면 위에서 선형대수 수식으로 계산한 것과 동일함을 확인하게 된다.\n# lm 함수를 통해 계산\n\nlm(childHeight ~ midparentHeight, data=GaltonFamilies) %&gt;% coef()\n\n    (Intercept) midparentHeight \n     22.6362405       0.6373609",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형</span>"
    ]
  },
  {
    "objectID": "models.html#가내수공업-모형개발",
    "href": "models.html#가내수공업-모형개발",
    "title": "\n13  모형\n",
    "section": "\n13.2 가내수공업 모형개발",
    "text": "13.2 가내수공업 모형개발\n데이터 과학 분야의 제품 개발 방식은 다양하다. 엔지니어링의 관점에서 볼 때, 전통적인 장인의 기술이 제자에게 계승되는 방식에서 시작하여, 포드의 대량 생산 방식을 거치고, 대량 맞춤생산(Mass Customization) 방식으로 발전하여, 현재에는 기계 학습과 딥러닝이 통합된 혁신적인 개발 방식까지 다양한 방법론이 혼재되어 있다.\n전통적인 가내수공업 방식은 개별 주문에 따라 제품을 최적화하여 만드는 방식이다. 이 방식에서는 인간의 경험과 지식이 중요한 역할을 하며, 고객의 특별한 요구사항을 충족하기 위해 맞춤형 모델을 개발한다. 이러한 방식을 간략히 살펴보면, 각 고객의 고유한 필요에 따라 제품이나 서비스를 특별히 설계하고 생산하는 것이 핵심이다.\n\n##========================================================\n## 01. 데이터 준비\n##========================================================\n## 모의시험 데이터 생성\n\nx &lt;- seq(1, 100,1)\ny &lt;- x**2 + jitter(x, 1000)\n\ndf &lt;- tibble(x,y)\nhead(df)\n\n# A tibble: 6 × 2\n      x      y\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -161. \n2     2  -90.4\n3     3   55.5\n4     4  -91.7\n5     5 -141. \n6     6  186. \n\n##========================================================\n## 02. 탐색적 데이터 분석\n##========================================================\n# 통계량\npsych::describe(df)\n\n  vars   n    mean      sd  median trimmed     mad     min      max    range\nx    1 100   50.50   29.01   50.50    50.5   37.06    1.00   100.00    99.00\ny    2 100 3444.89 3048.71 2481.71  3151.6 3205.58 -161.15 10137.16 10298.31\n  skew kurtosis     se\nx 0.00    -1.24   2.90\ny 0.62    -0.89 304.87\n\n# 산점도\nplot(x, y)\n\n\n\n\n\n\n##========================================================\n## 03. 모형 적합\n##========================================================\n\n#---------------------------------------------------------\n# 3.1. 선형회귀 적합\nlm.m &lt;- lm(y ~ x, data=df)\nsummary(lm.m)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-985.4 -622.8 -250.1  608.7 1648.4 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1700.837    150.998  -11.26   &lt;2e-16 ***\nx             101.896      2.596   39.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 749.3 on 98 degrees of freedom\nMultiple R-squared:  0.9402,    Adjusted R-squared:  0.9396 \nF-statistic:  1541 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x,y, data=df, cex=0.7)\nabline(lm.m, col='blue')\n\n# 잔차 \nplot(resid(lm.m))\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n#---------------------------------------------------------\n# 3.2. 비선형회귀 적합\n# 비선형회귀적합\ndf$x2 &lt;- df$x**2\n\nnlm.m &lt;- lm(y ~ x + x2, data=df)\nsummary(nlm.m)\n\n\nCall:\nlm(formula = y ~ x + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-201.53 -105.43   -0.48  118.59  184.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -13.00820   36.38611  -0.358    0.721    \nx             2.61158    1.66294   1.570    0.120    \nx2            0.98301    0.01595  61.623   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 118.9 on 97 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9985 \nF-statistic: 3.251e+04 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x, y, data=df, cex=0.7)\nlines(x, fitted(nlm.m), col='blue')\n# 잔차 \nplot(resid(nlm.m), cex=0.7)\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n\n데이터 준비단계에서 모의시험 데이터를 생성한다. 독립변수 \\(x\\)는 1부터 100까지의 연속적인 수열이며, 종속변수 \\(y\\)는 \\(x^2\\)에 노이즈를 추가한 값으로 정의한다. 생성데이터는 df라는 데이터프레임에 저장하며, 데이터 처음 몇 행을 출력하여 확인한다.\n탐색적 데이터 분석 (EDA) 단계에서 데이터의 기술통계량을 psych 패키지 describe 함수를 활용하여 출력하고, 데이터 분포와 관계를 확인하기 위해 \\(x\\)와 \\(y\\)의 산점도로 시각화한다.\n모형 적합 단계에선 먼저, 선형회귀 모델을 사용하여 \\(y\\)를 \\(x\\)로 예측한다. 적합된 선형 모형 결과를 출력하고, 적합된 모델과 잔차를 시각적으로 확인한다. \\(y = \\beta_0 + \\beta_1 x + \\beta_1 x^2\\) 형태를 가진 2차 회귀 모형을 적합시킨다. \\(x^2\\) 항을 데이터프레임에 추가해서 2차 회귀 모델을 적합한 후, 결과를 출력하고, 적합된 모형과 잔차를 시각적으로 확인한다.\n\n\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science. Leanpub.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of Statistics and Data Visualization.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형</span>"
    ]
  },
  {
    "objectID": "models_building.html",
    "href": "models_building.html",
    "title": "\n14  모형 개발\n",
    "section": "",
    "text": "14.1 기계학습/회귀모형 구성요소\n일반 모형을 “신호 + 잡음(signal + noise)”로 가정하고 다음과 같은 수식으로 표현할 수 있다.\n\\[y = f(x) + \\epsilon\\]\n결국, 잡음이 낀 데이터에서 잡음을 제거하고 신호만 뽑아내는 것이 회귀모형, 기계학습 모형이라고 볼 수 있다. 회귀모형과 기계학습 모형은 회귀모형이 특정 함수형태를 가정하고 데이터에서 신호와 잡음을 구부하는데 초점이 과거 맞춰졌다면, 기계학습모형은 \\(x\\)는 \\(y\\)의 인과관계를 가정으로 놓고 신호와 잡음을 가장 잘 발라낼 수 있는 함수를 찾아내는데 초점을 두고 있다.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#ml-basic-elements",
    "href": "models_building.html#ml-basic-elements",
    "title": "\n14  모형 개발\n",
    "section": "",
    "text": "출력 : \\(y\\), 관심갖고 있는 결과변수\n입력 : \\(x\\), 설명/예측 변수\n\n\n\\(y\\)의 변동성을 설명하는 목적의 모형을 구축하는 경우 \\(x\\)는 설명변수\n\n\\(y\\)의 변동성을 예측하는 목적의 모형을 구축하는 경우 \\(x\\)는 예측변수\n\n\n가설: : \\(g: x \\rightarrow y\\), \\(x\\)는 \\(y\\)에 영향을 주는 인과관계가 존재한다.\n목적함수 : \\(f: x \\rightarrow y\\), \\(y\\)와 \\(x\\)를 연관시켜주는 함수\n데이터: \\((x_1 , y_1 ), (x_2 , y_2 ), \\dots, (x_n , y_n )\\)\n\n오차: \\(\\epsilon\\), \\(f: x \\rightarrow y\\)으로 설명되지 않는 부분\n\n\n\n\n기계학습 도해",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#대-기계학습-원리",
    "href": "models_building.html#대-기계학습-원리",
    "title": "\n14  모형 개발\n",
    "section": "\n14.2 3대 기계학습 원리",
    "text": "14.2 3대 기계학습 원리\n기계학습 알고리즘 개발자가 데이터를 학습시켜 기계학습 알고리즘을 뽑아내는 과정에 3대 기계학습 원리가 적용된다. 1\n\n\n오컴의 면도날(Occam’s Razor): 사고 절약의 원리(Principle of Parsimony)라고도 불리며, 같은 현상을 설명하는 두가지 모형이 있다면, 단순한 모형을 선택한다.\n표집 편향(Sampling Bias): 모집단을 대표성의 원리에 따라 표본을 추출하지 못할 때, 기계학습 알고리즘도 편향된 표본을 학습하여 결과를 왜곡시킨다.\n데이터 염탐 편향(Data Snooping Bias): 데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 된다.\n\n\n\n3대 기계학습 원리\n\n\n오컴의 면도날\n\n동일한 조건이면 더 단순한 것을 선택하는 것으로, 가장 큰 이유는 갖고 있는 데이터를 벗어나 새로운 데이터를 갖게 될 경우 학습시킨 기계학습 알고리즘이 더 좋은 성능을 보인다는 것이다. 결국 수많은 가능한 모형중에서 하나를 선택하는 기준이 된다.\n\nAn explanation of the data should be made as simple as possible, but no simpler – Albert Einstein\n\n\n표집 편향\n\n1948년 미국 대통령선거에서 트루먼이 듀이 후보를 물리치고 대통령이 된 것은 알려진 사실이다. 하지만, 대부분의 여론조사에서 듀이의 승리를 예상했지만, 사실은 그 반대로 나타났다. 그 당시 여론조사를 전화기를 사용하였는데, 문제는 전화기가 부유층이 많이 소유하고 있어 미국 대통령선거 모집단을 대표하는 대표성에 문제가 있어 왜곡된 결과가 도출된 것이다.\n상업적으로 개인금융의 신용카드발급, 신용평가에도 동일한 문제가 발생한다. 사실 수익성은 저신용자가 높아 이를 살펴보면, 신용평가에 사용될 데이터는 저신용자는 카드를 발급받을 수 없어 데이터베이스에는 표집편향된 고객정보만 존재하는 것을 어렵지 않게 볼 수 있다.\n\nIf the data is sampled in a biased way, learning will produce a similarly biased outome.\n\n\n데이터 염탐 편향\n\n데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 하지만, 현실적으로 현업에서 작업하는 사람들이 흔히 범하는 실수다. 동일한 데이터에 대해 갖가지 기계학습 알고리즘을 적용해서 가장 좋은 성능이 나오는 알고리즘을 선정한다. 문제는 데이터가 바뀌면 어떨까? 아마 기대했던 성능이 나오지 못할 가능성이 크다.\n\nIf you torture the data long enough, it will confess",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#모형-개발과정",
    "href": "models_building.html#모형-개발과정",
    "title": "\n14  모형 개발\n",
    "section": "\n14.3 모형 개발과정",
    "text": "14.3 모형 개발과정\n통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다. 다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고, 지난한 과정이 될 수도 있다.\n\n데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 팩키지와 소통이 될 수 있는 데이터형태)가 되도록 준비한다.\n변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도 적용한다.\n변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를 계산한다. 특히 변수간 상관관계가 \\(r &gt; 0.9\\) 혹은 근처인 경우 변수를 빼거나 다른 방법을 강구한다.\n동일한 척도로 회귀계수를 추정하고 평가하려는 경우, scale() 함수로 척도로 표준화한다.\n모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다. 만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로 개발한다.\n\n\nplot() 함수를 사용해서 이상점이 있는지, 비선형관계를 잘 잡아냈는지 시각적으로 확인한다.\n다양한 모형을 적합시키고 R^2 와 RMSE, 정확도 등 모형평가 결과가 가장 좋은 것을 선정한다.\n절약성의 원리(principle of parsimony)를 필히 준수하여 가장 간결한 모형이 되도록 노력한다.2\n\n\n\n최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를 넣어 마무리한다.\n\n\n\n\n\n\n\n키보드로 통계모형 표현법\n\n\n\n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n\n주효과에 대해 변수를 입력으로 넣을 +를 사용한다.\n교호작용을 변수간에 표현할 때 :을 사용한다. 예를 들어 x*y는 x+y+x:z와 같다.\n모든 변수를 표기할 때 .을 사용한다.\n종속변수와 예측변수를 구분할 때 ~을 사용한다. y ~ .은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n특정변수를 제거할 때는 -를 사용한다. y ~ . -x는 모든 예측변수를 사용하고, 특정 변수 x를 제거한다는 의미다.\n상수항을 제거할 때는 -1을 사용한다.\n\n\n\n\n\n\n\n\nR 공식구문\n수학적 표현\n설명\n\n\n\ny~x\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시키는 1차 선형회귀식\n\n\ny~x -1\n\\(y_i = \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시 절편 없는 1차 선형회귀식\n\n\ny~x+z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i\\)\n\nx와 z를 y에 적합시키는 1차 선형회귀식\n\n\ny~x:z\n\\(y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z 교호작용 항을 y에 적합시키는 1차 선형회귀식\n\n\ny~x*z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z, 교호작용항을 y에 적합시키는 1차 선형회귀식",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#과대적합-사례",
    "href": "models_building.html#과대적합-사례",
    "title": "\n14  모형 개발\n",
    "section": "\n14.4 과대적합 사례",
    "text": "14.4 과대적합 사례\n\\(y=x^2 + \\epsilon\\) 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n\ntidyr, modelr, ggplot2 팩키지를 불러와서 환경을 설정한다.\n\n\\(y=x^2 + \\epsilon\\), 오차는 \\(N(0, 0.25)\\)을 따르는 모형을 생성하고, df 데이터프레임에 결과를 저장한다.\n\npoly_fit_model 함수를 통해 7차 다항식으로 적합시킨다.\n적합결과를 ggplot을 통해 시각화한다.\n\n1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수 있다.\n\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(modelr)\n\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model &lt;- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx &lt;- seq(-1, 1, length=20)\ny &lt;- true_model(x)\ndf &lt;- tibble(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model &lt;- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\n# fitted_mod &lt;- poly_fit_model(df, 10)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\nfitted_tbl &lt;- tibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(x = list(seq(-1, 1, length=20))) |&gt;\n  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, \n                                         prediction = .y, \n                                         .name_repair = \"unique\"))) |&gt; \n  select(차수, fitted_df) |&gt; \n  mutate(차수 = as.factor(차수)) |&gt;   \n  unnest(fitted_df) \n\n\ndf %&gt;% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))\n\n\n\n\n\n\n\n다항식 차수를 달리하여 데이터에 1~10차 다항식을 적합시킨 후에 오차를 계산했다. 1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그 이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록 낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가 필요한지는 의문이다.\n\ntibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(true_y = map(데이터, 2)) |&gt;\n  # 참값과 모형예측값\n  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |&gt; \n  # 오차\n  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |&gt; \n  mutate(차수 = factor(차수)) |&gt; \n  mutate(color = c(\"gray30\", \"blue\", rep(\"gray30\", 8))) |&gt; \n  # 시각화\n  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs( x = \"차수\",\n          y = \"RMSE 오차\",\n          title = \"다항식 차수를 달리하여 계산한 RMSE 오차\") +\n    scale_fill_manual(values = c(\"blue\", \"gray30\", rep(\"blue\", 8))) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nAbu-Mostafa, Yaser S, Malik Magdon-Ismail, 와/과 Hsuan-Tien Lin. 2012. Learning from data. Vol 4. AMLBook New York.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#footnotes",
    "href": "models_building.html#footnotes",
    "title": "\n14  모형 개발\n",
    "section": "",
    "text": "Caltech MOOC, Yaser Abu-Mostafa, Introductory Machine Learning, 2012↩︎\n간결성 원칙으로도 번역되며, 오캄의 면도날(“Occam’s razor”)라는 이름으로도 알려져 있으며, 연구나 문제 해결의 맥락에서 가장 단순한 설명이나 가설을 우선적으로 고려해야 한다는 의미로, 두 개 이상의 설명이 관찰된 현상을 동등하게 설명할 수 있을 때, 더 적은 가정이 필요한 또는 더 단순한 설명을 선호해야 한다는 것이다.↩︎",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_many.html",
    "href": "models_many.html",
    "title": "15  많은 회귀모형",
    "section": "",
    "text": "15.1 기대수명 데이터\ngapminder 데이터를 가지고 회귀모형을 구축하고 모형을 활용하여 종속변수(기대수명, lifeExp)가 늘어나지 못한 국가를 뽑아내고 이를 시각적으로 확인해보자.\n# 0. 환경설정 -----\n# 데이터\nlibrary(gapminder)\n\n# Tidyverse\nlibrary(tidyverse)\n\n# 모형\nlibrary(broom)\n\n# 1. 데이터 -----\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#model-reg-viz-data",
    "href": "models_many.html#model-reg-viz-data",
    "title": "15  많은 회귀모형",
    "section": "",
    "text": "15.1.1 기대수명 회귀분석\n대륙과 국가를 그룹으로 잡아 회귀분석을 각각에 대해서 돌리고 나서, 모형 결과값을 데이터와 모형이 함께 위치하도록 티블(tibble)에 저장시켜 놓은다. 그리고 나서, 주요한 회귀모형 성능지표인 결정계수(\\(R^2\\))를 기준으로 정렬시킨다.\n\n# 2. 모형 -----\ncountry_model &lt;- function(df)\n  lm(lifeExp ~ year, data=df)\n\nby_country &lt;- gapminder %&gt;% \n  group_by(country, continent) %&gt;% \n  nest() %&gt;% \n  mutate(model = map(data, country_model),\n         model_glance = map(model, glance),\n         rsquare = map_dbl(model_glance, ~.$r.squared)) |&gt; \n  ungroup()\n\nby_country %&gt;% \n  arrange(rsquare)\n\n# A tibble: 142 × 6\n   country          continent data              model  model_glance      rsquare\n   &lt;fct&gt;            &lt;fct&gt;     &lt;list&gt;            &lt;list&gt; &lt;list&gt;              &lt;dbl&gt;\n 1 Rwanda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0172\n 2 Botswana         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0340\n 3 Zimbabwe         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0562\n 4 Zambia           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0598\n 5 Swaziland        Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0682\n 6 Lesotho          Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0849\n 7 Cote d'Ivoire    Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.283 \n 8 South Africa     Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.312 \n 9 Uganda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.342 \n10 Congo, Dem. Rep. Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.348 \n# ℹ 132 more rows\n\n\n\n15.1.2 회귀모형 시각화\n데이터셋 by_country를 이용하여 각 나라별로 회귀 모델의 $ R^2 $ (결정 계수) 값을 기반으로 “적합국”과 “비적합국”을 분류하고, 그 결과를 시각화한다. $ R^2 $ 값이 큰 나라 5개와 작은 나라 5개를 추출하여 기대수명 변화를 그래프로 대조하여 국가별 차이를 명확히 한다.\n\n\n최대 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 큰 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_max_countries에 저장한다.\n\n\n\n최소 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 작은 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_min_countries에 저장한다.\n\n\n\n데이터 필터링 및 시각화:\n\n총 10개 국가에 해당하는 데이터만 by_country에서 추출한다.\n추출된 데이터에서 나라명, 대륙명, $ R^2 $ 값, 그리고 원데이터를 추출한다.\n결과를 $ R^2 $ 값의 내림차순으로 정렬한다.\n데이터를 정리하여 각 나라의 연도별 기대수명을 나타내는 선그래프를 생성하고, 그래프에서는 $ R^2 $ 값이 높은 국가들을 “발전된국가”로, 낮은 값을 가진 국가들을 “개발국”으로 분류하여 색상을 달리하여 시각화할 재료로 준비한다.\n\n\n\n최종적으로는 각 국가별로 연도에 따른 기대수명 변화를 보여주는 그래프를 생성한다. $ R^2 $ 값이 높은 국가들은 빨간색으로, 낮은 국가들은 파란색으로 표시한고, 마지막으로 회귀모형이 얼마나 잘 적합되었는지에 따라 각 국가의 기대수명 변화 패턴을 비교한다. 파란색으로 회귀모형이 잘 적합된 경우에도 서로 다른 패턴이 확인된다. 즉, 선진국과 개발도상국 모두 제2차 세계대전 이후 기대수명이 증대했으나 선진국은 높은 기대수명에서 개도국은 낮은 기대수명에서 시작해서 모두 기대수명이 높아진 것이 눈에 띈다. 하지만, 빨간색으로 회귀계수가 낮은 나라는 기대수명이 높아지다가 특정 사건으로 인해 기대수명이 제자리로 돌아온 이후 다시 기대수명이 높아지는 추세를 보여 비선형적 관계를 보여주고 있어 회귀계수가 전반적으로 낮게 나타났다.\n\nrsquare_max_countries &lt;- by_country |&gt; \n  slice_max(order_by = rsquare, n = 5)  |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nrsquare_min_countries &lt;- by_country |&gt; \n  slice_min(order_by = rsquare, n = 5) |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nby_country |&gt; \n  filter(country %in% rsquare_max_countries | country %in% rsquare_min_countries) |&gt; \n  select(country, continent, rsquare, data) |&gt; \n  arrange(desc(rsquare)) |&gt; \n  unnest(data) |&gt; \n  mutate(country = factor(country, levels = c(rsquare_max_countries, rsquare_min_countries))) |&gt; \n  mutate(class = if_else(country %in% rsquare_max_countries, \"발전된국가\", \"개발국\")) |&gt; \n  ggplot(aes(x = year, y = lifeExp, color = class)) +\n    geom_line() +\n    facet_wrap(~country, nrow = 2) +\n    scale_color_manual(values = c(\"red\", \"blue\")) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    labs(title =\"회귀모형 기대수명 적합국과 비적합국\",\n         x = \"\",\n         y = \"기대수명\",\n         caption = \"자료출처: gapminder\")",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#모형식별",
    "href": "models_many.html#모형식별",
    "title": "15  많은 회귀모형",
    "section": "\n15.2 모형식별",
    "text": "15.2 모형식별\npurrr 팩키지를 활용하여 원본 모형 데이터와 모형을 하나의 데이터프레임(tibble)에 담을 수가 있다. 즉, 6가지 서로 다른 회귀모형을 일괄적으로 적합시키고 가장 AIC 값이 적은 회귀모형을 선택하는 코드를 다음과 같이 작성한다. 1\n\n\nreg_models: 다양한 회귀모형을 정의한다.\n\nmutate(map()): 정의한 회귀모형 각각을 적합시키고 모형성능 지표를 추출한다.\nAIC 기준으로 가장 낮은 모형을 선정한다.\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n## 데이터셋 준비 -----\ngapminder &lt;- gapminder %&gt;%\n  set_names(colnames(.) %&gt;% tolower())\n\n## 다양한 회귀모형 -----\nreg_models &lt;- list(\n  `01_pop` = 'lifeexp ~ pop',\n  `02_gdppercap` = 'lifeexp ~ gdppercap',\n  `03_simple` = 'lifeexp ~ pop + gdppercap',\n  `04_medium` = 'lifeexp ~ pop + gdppercap + continent + year',\n  `05_more`   = 'lifeexp ~ pop + gdppercap + country + year',\n  `06_full`   = 'lifeexp ~ pop + gdppercap + year*country')\n\nmodel_tbl &lt;- tibble(reg_formula = reg_models) %&gt;%\n  mutate(model_name = names(reg_formula)) %&gt;% \n  select(model_name, reg_formula) %&gt;% \n  mutate(reg_formula = map(reg_formula, as.formula))\n\nmodel_tbl\n\n# A tibble: 6 × 2\n  model_name   reg_formula \n  &lt;chr&gt;        &lt;named list&gt;\n1 01_pop       &lt;formula&gt;   \n2 02_gdppercap &lt;formula&gt;   \n3 03_simple    &lt;formula&gt;   \n4 04_medium    &lt;formula&gt;   \n5 05_more      &lt;formula&gt;   \n6 06_full      &lt;formula&gt;   \n\n## 회귀모형 적합 및 모형 성능 지표 -----\nmodel_tbl &lt;- model_tbl %&gt;%\n  mutate(fit = map(reg_formula, ~lm(., data = gapminder))) %&gt;% \n  mutate(model_glance = map(fit, broom::glance),\n         rsquare      = map_dbl(model_glance, ~.$r.squared),\n         AIC          = map_dbl(model_glance, ~.$AIC)) %&gt;% \n  arrange(AIC)\n\nmodel_tbl\n\n# A tibble: 6 × 6\n  model_name   reg_formula  fit          model_glance      rsquare    AIC\n  &lt;chr&gt;        &lt;named list&gt; &lt;named list&gt; &lt;named list&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 06_full      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.976    7752.\n2 05_more      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.932    9268.\n3 04_medium    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.717   11420.\n4 03_simple    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.347   12836.\n5 02_gdppercap &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.341   12850.\n6 01_pop       &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.00422 13553.\n\n\n\n15.2.1 교차검증 CV\n\n데이터를 10조각내서 교차검정을 통해 RMSE가 가장 작은 회귀모형이 어떤 것인지 살펴보자. cross_df() 함수로 교차검증 splits 데이터와 모형을 준비한다. 다음으로 analysis() 함수로 교차검증 데이터에 대해서 회귀모형 각각을 적합시키고, assessment() 함수로 적합시킨 모형에 대해 모형성능을 살펴본다. 마지막으로 RMSE 회귀모형 성능지표를 통해 모형선택을 한다.\n\n## 교차검정 -----\nvalid_tbl &lt;- gapminder %&gt;%\n  rsample::vfold_cv(10)\n\ncv_tbl &lt;- list(test_training = list(valid_tbl), \n               model_name = model_tbl$model_name)  \n  \ncv_tbl &lt;- tidyr::expand_grid(test_training = list(valid_tbl), \n                             model_name = model_tbl$model_name)\n\ncv_tbl &lt;- cv_tbl %&gt;%\n  mutate(model_number = row_number()) %&gt;%  # Manually creating the model_number column\n  left_join(model_tbl %&gt;% select(model_name, reg_formula), by = \"model_name\") %&gt;% \n  unnest(cols = c(test_training))\n\ncv_tbl\n\n# A tibble: 60 × 5\n   splits             id     model_name model_number reg_formula \n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;   \n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;   \n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;   \n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;   \n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;   \n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;   \n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;   \n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;   \n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;   \n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;   \n# ℹ 50 more rows\n\n## 교차검정 analysis, assessment -----\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)))) %&gt;%\n  mutate(RMSE = map2_dbl(fit, splits, ~modelr::rmse(.x, rsample::assessment(.y))))\n\ncv_fit_tbl\n\n# A tibble: 60 × 7\n   splits             id     model_name model_number reg_formula  fit       RMSE\n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt; &lt;named &gt; &lt;dbl&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.82\n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.39\n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;    &lt;lm&gt;      1.94\n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.28\n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.57\n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.25\n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.82\n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.58\n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.04\n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.75\n# ℹ 50 more rows\n\n## 시각화 -----\ncv_fit_tbl %&gt;%\n  ggplot(aes(RMSE, fill = model_name)) +\n  geom_density(alpha = 0.75) +\n  labs(x = \"RMSE\", title = \"gapminder 회귀모형별 교차검정 분포\")\n\n\n\n\n\n\n\n\n15.2.2 병렬처리 - furrr\n\nparallel::detectCores()을 통해 전체 코어 숫자를 확인하고 이를 병렬처리를 통해 교차검증에 따른 시간을 대폭 절감시킨다. 이를 위해서 future 팩키지를 사용하고 절약되는 시간을 측정하기 위해서 tictoc 팩키지를 동원한다.\n\nlibrary(furrr)\nlibrary(tictoc)\n\nplan(multisession, workers = parallel::detectCores() - 1)\n\n\n\npurrr 순차처리\n\n## purrr 순차처리 -----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y))))\n\ntoc()\n\n1.199 sec elapsed\n\n\n\nfurrr 병렬처리\n\n## furrr 병렬처리 ----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = future_map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)), .progress=TRUE)) \n\ntoc()\n\n1.64 sec elapsed",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#footnotes",
    "href": "models_many.html#footnotes",
    "title": "15  많은 회귀모형",
    "section": "",
    "text": "DAN OVANDO(FEBRUARY 20, 2018), “DATA WRANGLING AND MODEL FITTING USING PURRR”↩︎",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_case_study.html",
    "href": "models_case_study.html",
    "title": "16  사례: 시군 인구증가",
    "section": "",
    "text": "16.1 인구변동이 많은 시군",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun",
    "href": "models_case_study.html#r-population-by-sigun",
    "title": "16  사례: 시군 인구증가",
    "section": "",
    "text": "16.1.1 데이터 가져오기 및 데이터 전처리\n인구변동이 많은 시군 통계 분석을 위해 필요한 팩키지를 불러 읽어온다. 통계청 KOSIS에서 다운로드 받은 파일을 data 폴더 아래 저장하고 나서, 전처리 작업을 수행한다. 서울특별시를 포함한 광역시에 포함된 군은 데이터 분석에 제외할 것이기 때문에 stringr 정규표현식 기능을 활용하여 깔끔하게 향후 데이터분석을 위한 데이터프레임으로 정리한다.\n\n# 0. 환경설정 -----------\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggpubr)\nextrafont::loadfonts()\n\n# 1. 데이터 가져오기 ---------\n\nkor_dat &lt;- read_excel(\"data/korea-pop-zipf-law.xlsx\", sheet=\"데이터\", skip=1) \n\nkor_dat &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` !=\"전국\") %&gt;% \n  filter(`인구현황별` ==\"전체인구(A)\")\n\n# 2. 데이터 전처리 ---------\n## 2.1. 시군 뽑아내기 -------\n\nsigungu_v &lt;- kor_dat %&gt;% count(`소재지(시군구)별`) %&gt;% \n  pull(`소재지(시군구)별`)\n\nsigungu_v &lt;- sigungu_v[str_detect(sigungu_v, \"시$|군$\")]\n\n## 2.2. 시군 데이터 전처리 -------\n\nkor_df &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` %in% sigungu_v) %&gt;% \n  filter(!is.na(`2016 년`)) %&gt;% group_by(`소재지(시군구)별`) %&gt;% \n  summarise_if(is.numeric, mean) %&gt;% \n  rename(시군 = `소재지(시군구)별`)\n\n## 2.3. 시각화 데이터 변환  -------\nkor_lng_df &lt;- kor_df %&gt;% \n  gather(연도, 인구수, -시군) %&gt;% \n  mutate(연도 = as.numeric(str_extract(연도, \"[0-9]+\")))",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-problem",
    "href": "models_case_study.html#r-population-by-sigun-problem",
    "title": "16  사례: 시군 인구증가",
    "section": "\n16.2 문제점",
    "text": "16.2 문제점\n대한민국 시군이 166 이기 때문에 인구변동이 많은 시군을 추출하기 위해서 서울시는 천만명 근처이고 가장 작은 울릉군은 2016년 기준 10,001 명이라 편차가 매우 크다. 따라서, 이를 시각화를 하게 되면 문제점이 한눈에 파악된다.\n\n# 3. 시각화 ----------------\n## 3.1. 시군별 연도별 인구수 변화\nkor_lng_df %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=시군, group=시군)) +\n    geom_point() +\n    geom_line() +\n    theme_pubclean(base_family=\"NanumGothic\") +\n    scale_y_log10(labels=scales::comma) +\n    scale_x_date(date_labels = \"%Y\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\")",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-solution",
    "href": "models_case_study.html#r-population-by-sigun-solution",
    "title": "16  사례: 시군 인구증가",
    "section": "\n16.3 FP 통한 문제해결",
    "text": "16.3 FP 통한 문제해결\n이러한 문제점에 대해 가장 많이 활용되는 기법이 자료구조로 티블(tibble)을 도입하고, 데이터 분석을 위한 방법으로 함수형 프로그래밍을 조합하는 것이다.\n\n16.3.1 티블 자료구조\n데이터프레임을 기존 폭넓은(wide) 형태를 긴(long) 형태로 변환하고 이를 nest()를 적용시키면 함수형 프로그램을 적용시킬 수 있는 자료구조가 된다. 더불어, 선형회귀모형을 각 시군별로 적용시킬 예정이라 회귀모형 함수도 생성시켜 둔다.\n그리고 나서 전체 시군별로 연도별 인구변화를 회귀분석으로 수행하여 수행결과를 broom 팩키지의 tidy, glance, augment 함수를 활용하여 데이터와 모형분석결과를 결합시킨다.\n\n# 4. 인구변화 심한 시군 추출 ----------------\n## 4.1. 데이터 \nkor_sigun_tb &lt;- kor_lng_df %&gt;% group_by(시군) %&gt;% \n  nest()\n\n## 4.2. 모형 - 선형회귀모형\nsigun_model &lt;- function(df) {\n  lm(인구수 ~ 연도, data=df)\n}\n\n## 4.3. 데이터 + 모형 결합\nkor_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  mutate(model = map(data, sigun_model)) %&gt;% \n  mutate(\n    tidy    = map(model, broom::tidy),\n    glance  = map(model, broom::glance),\n    결정계수 = glance %&gt;% map_dbl(\"r.squared\"),\n    augment = map(model, broom::augment)\n  )\n\nDT::datatable(kor_sigun_tb)\n\n\n\n\n\n\n16.3.2 급성장 시군 - 결정계수(\\(R^2\\))\n회귀분석 결정계수(\\(R^2\\))기준 인구가 연도별로 높은 상관관계를 갖는 시군과 그렇지 않는 상위 하위 5개 시군을 뽑아 시각화 해보자.\n\n## 4.4. 고성장 및 정체 시군 추출\n\nkor_high_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"고성장\")\n\nkor_low_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(-5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"저성장\")\n\nkor_growth_sigun_tb &lt;- bind_rows(kor_high_growth_sigun_tb, kor_low_growth_sigun_tb)\n\n## 4.5. 고성장 및 정체 시군 시각화\n\nkor_growth_sigun_tb %&gt;% \n  mutate(구분 = factor(구분),\n         시군 = fct_reorder(시군, 결정계수)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=시군)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_log10(labels=scales::comma) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~시군, nrow=2, scale=\"free\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n16.3.3 급성장 시군 - 회귀계수\n회귀분석 결정계수(\\(R^2\\))기준이 아닌 회귀계수(\\(\\beta_1\\))를 기준으로 상위 5개, 하위 5개 시군을 뽑아 연도별 인구변화를 시각화해보자.\n\n# 5. 인구 증가 및 인구감소 ----------------\nkor_sigun_reg_df &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0)\n\ntop10_plus_sigun &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(5, 증감) %&gt;% \n  pull(시군)\n\ntop10_minus_sigun &lt;-kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(-5, 증감) %&gt;% \n  pull(시군)\n\nkor_lng_df %&gt;% \n  left_join(kor_sigun_reg_df, by=\"시군\") %&gt;% \n  filter(시군 %in% c(top10_plus_sigun, top10_minus_sigun)) %&gt;%\n  mutate(구분 = ifelse(시군 %in% top10_plus_sigun, \"성장\", \"역성장\"),\n         시군 = fct_reorder(시군, -증감)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=구분)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_continuous(labels=scales::comma) +\n  scale_x_date(date_labels = \"%Y\") +\n  theme(legend.position = \"none\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  facet_wrap(~시군, scale=\"free\", nrow=2)",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "\n17  함수\n",
    "section": "",
    "text": "17.1 함수 기본 지식\n함수는 입력값(x)를 넣어 어떤 작업(f)을 수행한 결과를 반환(y) 과정으로 이해할 수 있는데, 인자로 다양한 값을 함수에 넣을 수 있고, 물론 함수가 뭔가 유용한 작업을 수행하기 위한 전제조건을 만족시키는지 확인하는 과정을 assert 개념을 넣어 확인하고 기술된 작업을 수행한 후에 출력값을 변환시키게 된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#all-about-function",
    "href": "functions.html#all-about-function",
    "title": "\n17  함수\n",
    "section": "",
    "text": "데이터 과학 함수 개념",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-use-function",
    "href": "functions.html#how-to-use-function",
    "title": "\n17  함수\n",
    "section": "\n17.2 함수 사용법",
    "text": "17.2 함수 사용법\n본격적으로 함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 과학 대표 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 자세히 살펴보자.\n\n17.2.1 R 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n\n[[1]]\n[1] 10\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 21\n\n[[4]]\n[1] 2.333333\n\n\n\n17.2.2 파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n\n[10, 4, 21, 2.3333333333333335]\n\n\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 다른 사람이 작성한 함수를 사용하기 위해서 먼저 함수명을 알아야 하고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n\nfunction (x, na.rm = FALSE) \nNULL\n\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야만 표준편차를 계산할 수 있다. 예를 들어, 데이터프레임(penguins)의 변수 하나(bill_length_mm)를 지정하여 전달하고 na.rm = TRUE도 명세하여 인자로 전달한다. 인자값이 기본디폴트 값으로 설정된 경우 타이핑을 줄일 수 있고, 결측값이 포함된 경우에 따라서 다른 인자를 넣어 전달하는 방식으로 함수를 사용한다.\n\nlibrary(palmerpenguins)\n\nsd(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 5.459584",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#convert-scripts-to-function",
    "href": "functions.html#convert-scripts-to-function",
    "title": "\n17  함수\n",
    "section": "\n17.3 스크립트 → 함수",
    "text": "17.3 스크립트 → 함수\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다.\n\nR 함수 템플릿을 제작한다.\n\n함수명 &lt;- function() { }\n\n\n스크립트를 함수 몸통에 복사하여 붙인다.\n반복작업되는 인자를 찾아내 이를 인자로 넣어둔다.\n인자값과 연동되는 부분을 찾아 맞춰준다.\n함수명을 적절한 동사를 갖춘 이름으로 작명한다.\n\nreturn이 불필요하기 때문에 R 언어 특성을 반영하여 필요한 경우 제거한다.\n\n\n17.3.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해 본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n\n[1] 6\n\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1, 2, 3, 4, 5, 6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n\n[1] 3\n\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  face &lt;- sample(dice, size=num_try)\n  return(face) # 불필요함.\n}\n\ndraw_dice(3)\n\n[1] 5 4 2",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#why-write-function",
    "href": "functions.html#why-write-function",
    "title": "\n17  함수\n",
    "section": "\n17.4 왜 함수가 필요한가?",
    "text": "17.4 왜 함수가 필요한가?\n왜 함수가 필요한지를 데이터를 분석할 때 자주 나오는 변수 정규화 사례를 바탕으로 살펴보자. 데이터프레임에 담긴 변수의 측도가 상이하여 변수를 상대적으로 비교하기 위해 측도를 재조정하여 표준화할 필요가 있다. 변수에서 평균을 빼고 표준편차로 나누는 정규화도 있지만, 최대값에서 최소값을 빼서 분모에 두고 분자에 최소값을 빼서 나누면 모든 변수가 0–1 사이 값으로 척도가 조정된다.\n\\[ f(x)_{\\text{척도조정}} = \\frac{x-min(x)}{max(x)-min(x)} \\]\n\ndf &lt;- data.frame(a=c(1,2,3,4,5),\n                         b=c(10,20,30,40,50),\n                         c=c(7,8,6,1,3),\n                         d=c(5,4,6,5,2))\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) /\n        (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) /\n        (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\ndf        \n\n     a         b         c    d\n1 0.00  0.000000 0.8571429 0.75\n2 0.25 -1.111111 1.0000000 0.50\n3 0.50 -2.222222 0.7142857 1.00\n4 0.75 -3.333333 0.0000000 0.75\n5 1.00 -4.444444 0.2857143 0.00\n\n\n상기 R 코드는 측도를 모두 맞춰서 변수 4개(a, b, c, d)를 비교하거나 향후 분석을 위한 것이다. 하지만, 읽어야 하는 코드중복이 심하고 길어 코드를 작성한 개발자의 의도 가 본의 아니게 숨겨져 있다. 작성한 R 코드에 실수한 것이 있는 경우, 다음 프로그램 실행에서 버그(특히, 구문론이 아닌 의미론적 버그)가 숨겨지게 된다. 상기 코드가 작성되는 과정을 살펴보면 본의 아니게 의도가 숨겨진다는 의미가 어떤 것인지 명확해진다.\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) 코드를 작성한 후, 정상적으로 돌아가는지 확인한다.\n1번 코드가 잘 동작하게 되면 다음 복사하여 붙여넣기 신공을 사용하여 다른 칼럼 작업을 확장해 나간다. df$b, df$c, df$d를 생성하게 된다.\n즉, 복사해서 붙여넣은 것을 변수명을 편집해서 df$b, df$c, df$d 변수를 순차적으로 생성해 낸다.\n\n\n\n\n\n\n\n해들리 위캠 어록\n\n\n\n\n중복은 의도를 숨기게 되고, 복사하여 붙여넣기 두번하면 함수를 작성할 시점이 되었다. (Duplication hides the intent. If you have copied-and-pasted twice, it is time to write a function.)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#funciton-component",
    "href": "functions.html#funciton-component",
    "title": "\n17  함수\n",
    "section": "\n17.5 함수 구성요소",
    "text": "17.5 함수 구성요소\n\n17.5.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n\n17.5.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다.\n다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n\n[1] 15\n\nsum_numbers(na_vector)\n\nError in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nError in library(assertive): 'assertive'이라고 불리는 패키지가 없습니다\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n\nError in assert_is_numeric(vec): 함수 \"assert_is_numeric\"를 찾을 수 없습니다\n\n\n\n17.5.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\n\ncat(\"절편: \", inter, \"\\n기울기: \", slope)\n\n절편:  37.88458 \n기울기:  -2.87579\n\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 나누어 할당하는 것도 가능하다. 각 붓꽃마다 0~1 사이 난수를 생성하여 할당한다. 그리고 난수값이 0.2 이상이면 훈련, 그렇지 않으면 시험 데이터로 구분한다.\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\ncat(\"총 관측점: \", nrow(iris), \"\\n훈련: \", nrow(train), \"\\n시험: \", nrow(test))\n\n총 관측점:  150 \n훈련:  126 \n시험:  24\n\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n\n$intercept\n(Intercept) \n   37.88458 \n\n$beta\n     cyl \n-2.87579",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#time-to-write-function",
    "href": "functions.html#time-to-write-function",
    "title": "\n17  함수\n",
    "section": "\n17.6 함수를 작성하는 시점",
    "text": "17.6 함수를 작성하는 시점\n복사해서 붙여넣는 것을 두번 하게 되면, 함수를 작성할 시점이다. 중복을 제거하는 한 방법이 함수를 작성하는 것이고, 함수를 작성하게 되면 의도가 명확해진다. 함수명을 rescale로 붙이고 이를 실행하게 되면, 의도가 명확하게 드러나게 되고, 복사해서 붙여넣게 되면서 생겨나는 중복과 반복에 의한 실수를 줄일 수 있게 되고, 향후 코드를 갱신할 때도 도움이 된다.\n\nrescale &lt;- function(x){\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a &lt;- rescale(df$a)\ndf$b &lt;- rescale(df$b)\ndf$c &lt;- rescale(df$c)\ndf$d &lt;- rescale(df$d)\n\nrescale() 함수를 사용해서 복사하여 붙여넣는 중복을 크게 줄였으나, 여전히 함수명을 반복해서 복사하여 붙여넣기를 통해 코드를 작성했다. 함수형 프로그래밍을 사용하는 것으로 함수명을 반복적으로 사용하는 것조차도 피할 수 있다.\n\nlibrary(purrr)\ndf &lt;- map_df(df, rescale)\ndf\n\n# A tibble: 5 × 4\n      a     b     c     d\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0     1    0.857  0.75\n2  0.25  0.75 1      0.5 \n3  0.5   0.5  0.714  1   \n4  0.75  0.25 0      0.75\n5  1     0    0.286  0   \n\n\n함수를 사용하지 않고 복사하여 붙여넣기 방식으로 코드를 작성한 경우 의도하지 않은 실수가 있어 함수를 도입하여 작성한 코드와 결과가 다른 것이 존재한다. 코드를 읽어 찾아보거나 실행한 후 결과를 통해 버그를 찾아보는 것도 함수의 의미와 중요성을 파악하는데 도움이 된다.\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것.\n\n한걸음 더 들어가 구체적으로 좋은 함수는 다음과 같은 특성을 지니고 있다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 일관된 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-write-function",
    "href": "functions.html#how-to-write-function",
    "title": "\n17  함수\n",
    "section": "\n17.7 사례: rescale 함수",
    "text": "17.7 사례: rescale 함수\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만드는 과정을 통해 앞서 학습한 사례를 실습해 보자.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#function-is-argument",
    "href": "functions.html#function-is-argument",
    "title": "\n17  함수\n",
    "section": "\n17.8 사례: 요약통계 함수",
    "text": "17.8 사례: 요약통계 함수\n데이터를 분석할 때 가장 먼저 수행하는 작업이 요약통계를 통해 데이터를 이해하는 것이다. 이미 훌륭한 요약통계 패키지와 함수가 있지만, 친숙한 개념을 함수로 다시 제작함으로써 함수에 대한 이해를 높일 수 있다.\n요약통계 기능을 먼저 구현한 다음에 중복 제거하여 요약통계 기능 함수를 제작해보자. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성하는 경우, 숫자 1,2,3 만 차이날 뿐 다른 부분은 동일하기 때문에 함수 코드에 중복이 심하게 관찰된다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n여기서 요약통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘겨 요약통계함수를 간략하고 가독성 높게 작성할 수 있다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 가정해보자.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n\n[1] 0.5000000 0.5000000 0.7142857 0.7500000\n\ncol_summary(df, fun = mean)\n\n[1] 0.5000000 0.5000000 0.5714286 0.6000000\n\ncol_summary(df, fun = sd)\n\n[1] 0.3952847 0.3952847 0.4164966 0.3791438",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html",
    "href": "functions_ds.html",
    "title": "\n18  데이터 과학 함수\n",
    "section": "",
    "text": "18.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n\n# A tibble: 11 × 5\n   species island    sex    year body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n 2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n 3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n 4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n 5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n 6 Adelie  Dream     &lt;NA&gt;   2007        2975\n 7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n 8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n 9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#역사",
    "href": "functions_ds.html#역사",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.2 역사",
    "text": "18.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다.\n\n데이터 분석에서 빈도수가 높은 작업을 Base R과 dplyr 패키지를 사용한 사례를 다음과 같이 비교하면 S언어에서 현재까지 이뤄낸 발전이 가시적으로 다가온다.\n\n\n\n\n\n\n\n작업\nBase R\ndplyr\n\n\n\n행 필터링 (Filter)\nsubset(data, condition)\ndata %&gt;% filter(condition)\n\n\n특정 칼럼 선택 (Select)\ndata[, c(\"col1\", \"col2\")]\ndata %&gt;% select(col1, col2)\n\n\n그룹별 집계작업\naggregate(. ~ grouping_var, data, FUN = mean)\ndata %&gt;% group_by(grouping_var) %&gt;% summarize(new_col = mean(col_name))\n\n\n죠인(Join)\nmerge(data1, data2, by = \"key_column\")\ndata1 %&gt;% inner_join(data2, by = \"key_column\")\n\n\n칼럼 추가\ntransform(data, new_col = some_func(existing_col))\ndata %&gt;% mutate(new_col = some_func(existing_col))\n\n\n행 결합\nrbind(data1, data2)\nbind_rows(data1, data2)\n\n\n칼럼 결합\ncbind(data1, data2)\nbind_cols(data1, data2)\n\n\n정렬\ndata[order(data$col_name), ]\ndata %&gt;% arrange(col_name)\n\n\n\n\n18.2.1 attach() 함수\n데이터프레임에 attach() 함수를 사용하면 데이터프레임을 구성하는 칼럼이 벡터로 작업환경에서 바로 접근하여 작업을 수행할 수 있다. penguins 데이터프레임을 attach()한 결과 bill_depth_mm 벡터가 작업환경에서 바로 접근하여 평균값을 계산할 수 있게 되었다. 작업을 완료한 후에 detach() 를 사용해서 작업환경에서 제거한다.\n\nlibrary(palmerpenguins)\n\nbase::attach(penguins)\n\nls(pos = which(search() == \"penguins\")[1])\n\n[1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n[4] \"flipper_length_mm\" \"island\"            \"sex\"              \n[7] \"species\"           \"year\"             \n\nmean(bill_depth_mm, na.rm = TRUE)\n\n[1] 17.15117\n\ndetach(penguins)\n\n\n18.2.2 with() 함수\nattach() 함수는 편리한 장점이 있지만, 데이터프레임 변수명과 함수명, 또 다른 작업에서 나온 객체명과 충돌이 발생할 경우 전혀 생각하지 못한 문제가 발생할 수 있다. 따라서, 격리를 통해 문제를 단순화하는 것이 필요하다. 이를 위해서 with() 함수를 사용하게 되면 데이터프레임에 속한 칼럼명을 명시하지 않더라도 간결하게 데이터 분석 작업을 이어나갈 수 있다.\n\nlibrary(palmerpenguins)\n\nwith(data = penguins,\n     expr = mean(bill_depth_mm, na.rm = TRUE))\n\n[1] 17.15117\n\n\n\n18.2.3 aggregate() 함수\nBase R 에서 지원되는 aggregate() 함수를 사용해서 동일한 결과를 얻을 수 있다. aggregate() 함수는 with()와 지향점은 유사하지만 구현방식에서 다소 차이가 난다.\n\naggregate(bill_depth_mm ~ 1, \n          data = penguins, \n          FUN = mean, \n          na.rm = TRUE)\n\n  bill_depth_mm\n1      17.15117",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#데이터-마스킹",
    "href": "functions_ds.html#데이터-마스킹",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.3 데이터 마스킹",
    "text": "18.3 데이터 마스킹\nR에서의 데이터 마스킹(Data Masking)은 tidyverse 생태계에서 데이터 문법을 담당하는 dplyr 패키지에서 핵심적인 개념이다. 데이터 마스킹을 사용하면 데이터프레임 칼럼을 $, [[ ]]를 사용하지 않고도 칼럼명으로 직접 참조할 수 있어, 데이터를 조작하고 변환할 때 훨씬 직관적이고 가독성 높은 코드를 작성할 수 있다.\npenguins 데이터프레임의 species를 데이터 마스킹 없이 조작하려면 penguins$species 혹은 penguins[['species']]와 같이 구문을 작성해야 하지만 데이터 마스킹을 사용하면 species 만으로 충분한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# 데이터 마스킹을 사용하여 펭귄종(species)이 \"Adelie\"만 추출한다.\npenguins %&gt;% filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\ndplyr 함수의 데이터 마스킹은 비표준 평가 (Non-standard evaluation, NSE)라는 개념에 기반을 두는데, 표현식을 캡처하고 난 후 바로 바로 실행되지 않고 제공된 데이터의 맥락 내에서 평가가 이루어진다. 데이터 마스킹은 강력하며 깔끔한 구문을 제공하지만, 칼럼명과 충돌할 수 있는 환경의 변수 이름이 있을 때 예기치 않은 방식으로 동작한다. 모호한 상황이 발생할 때 항상 다음과 같은 방식으로 .data$column_name 함으로써 데이터 마스킹 재정의(Overriding)를 통해 명확히 한다.\n\nspecies &lt;- \"Chinstrap\"\n\npenguins %&gt;% \n  filter(.data$species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n\n항목\n데이터 마스킹\nTidy Evaluation\n\n\n\n정의\n- 데이터프레임 칼럼명을 직접적인 변수처럼 다룰 수 있는 능력.  - $나 [[ ]] 없이 칼럼 참조를 단순화.\n- R 메타프로그래밍을 위한 프레임워크, 특히 tidyverse 에서 사용.  - 다양한 맥락에서 표현식을 캡쳐하고 평가하는 도구 제공.\n\n\n사용 사례\n- dplyr 함수에서 직접 데이터 조작.  - 코드 가독성 향상.\n- 따옴표 없는 표현식으로 사용자 정의 함수 생성.  - 표현식을 프로그래밍 방식으로 구성 및 평가.  - 표현식 평가 맥락 제어.\n\n\n구현\n- 기본적으로 tidy evaluation 메커니즘을 사용하여 구현됨.\n- rlang 패키지 enquo(), quo(), !! 등을 사용.  - 표현식과 그 환경을 캡쳐하기 위해 쿼저(Quosure) 의존.\n\n\n복잡성\n- 최종 사용자를 위해 간소화.  - 기본적인 복잡성을 추상화.\n- R 메타프로그래밍 이해 필요.  - 고급 사용자에게 더 많은 유연성 제공.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#깔끔한-평가",
    "href": "functions_ds.html#깔끔한-평가",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.4 깔끔한 평가",
    "text": "18.4 깔끔한 평가\n깔끔한 평가(Tidy evaluation)은 R tidyverse 프레임워크로, 특히 비표준 평가 (Non-Standard Evaluation, NSE)와 관련하여 tidyverse 함수로 프로그래밍하는 방법을 표준화했다. NSE는 R 함수가 표준과는 다른 맥락에서 표현식을 평가할 때 발생한다.\n\n준인용(Quasiquotation): enquo() 함수를 사용하여 표현식을 캡처하고 !!를 사용하여 표현식의 인용제거(Unquoting)을 가능케 한다.\nPronouns (대명사): .data 대명사는 데이터프레임의 칼럼명을 명시적으로 참조하는데 사용되어 모호성을 제거한다.\n함수: enquo()는 표현식을 캡처하고, quo_name()은 표현식을 문자열로 변환하며, !!는 표현식 인용제거 또는 주입작업을 수행한다.\n\n예를 들어, dplyr 패키지 filter 및 select와 같은 동사를 사용하지만 함수에 칼럼명을 작성하려는 경우, 인수로 전달될 때 이러한 동사가 어떤 칼럼을 참조하는지 명확히 하기 위해 깔끔한 평가(tidy evaluation)가 사용된다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_and_select &lt;- function(data, col_name, threshold) {\n  \n  # 칼럼명 문자열을 기호로 변환\n  col_sym &lt;- sym(col_name)\n  \n  # 준인용(quasiquotation)을 사용해서 칼럼 표현식을 캡쳐\n  col_expr &lt;- enquo(col_sym)\n  \n  # !! 연산자를 이용하여 인용제거(unquote)하고 표현식을 주입\n  data %&gt;% \n    filter(!!col_expr &gt; threshold) %&gt;% \n    select(!!col_expr)\n}\n\nfilter_and_select(penguins, \"bill_length_mm\", 55)\n\n# A tibble: 5 × 1\n  bill_length_mm\n           &lt;dbl&gt;\n1           59.6\n2           55.9\n3           55.1\n4           58  \n5           55.8\n\n\nsym() → enquo() → !!(뱅뱅) 구현방식이 Defuse-and-Inject 패턴으로 내부 동작방식은 동일하지만 사용자 구문은 { 칼럼명 }으로 깔끔해졌다.\n\nfilter_and_select_latest &lt;- function(data, col_name, threshold) {\n  \n  data %&gt;% \n    filter({{ col_name }} &gt; threshold) %&gt;% \n    select({{ col_name }})\n  \n}\n\nfilter_and_select_latest(penguins, bill_length_mm, 55)\n\n# A tibble: 5 × 1\n  bill_length_mm\n           &lt;dbl&gt;\n1           59.6\n2           55.9\n3           55.1\n4           58  \n5           55.8\n\n\n\n데이터 마스킹: 변수를 사용하여 계산하는 arrange(), filter(), summarize() 등 함수에 사용.\nTidy-selection: 변수를 선택하는 select(), relocate(), rename()과 같은 함수에 사용.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그룹별-평균",
    "href": "functions_ds.html#그룹별-평균",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.5 그룹별 평균",
    "text": "18.5 그룹별 평균\n\n데이터프레임 요약통계량을 계산하는 코드를 작성해보자. 펭귄종(species) 별로 부리길이를 계산하는 코드를 작성해보자.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(부리길이_평균 = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species   부리길이_평균\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie             38.8\n2 Chinstrap          48.8\n3 Gentoo             47.5\n\n\n이번에는 그룹변수와 데이터프레임 칼럼명을 달리하여 그룹별로 평균을 계사하는 함수를 작성하여 코드로 작성해보자.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by(group_varname) |&gt; \n    summarise(부리길이_평균 = mean(varname, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_varname` is not found.\n\n\n상기 코드가 동작하지 않는 이유는 함수에 tidyverse 코드를 함수에 단순히 전달해서 넣기 때문에 발생했다. 다음과 같이 칼러명을 포용(embracing)하는 방식으로 { 칼럼명 }과 같이 함수에 사용되는 데이터프레임 변수명을 명시적으로 작성할 경우 문제가 해결된다.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by( {{ group_varname }} ) |&gt; \n    summarise(부리길이_평균 = mean( {{ varname }}, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n\n# A tibble: 3 × 2\n  species   부리길이_평균\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie             38.8\n2 Chinstrap          48.8\n3 Gentoo             47.5",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그래프",
    "href": "functions_ds.html#그래프",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.6 그래프",
    "text": "18.6 그래프\n\n데이터프레임에서 범주형 변수를 하나 선택하여 빈도수를 시각화하는 스크립트를 다음과 같이 작성한다.\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  count(island) |&gt; \n  ggplot(aes(x=island, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n범주형 변수를 막대그래프로 시각화하는 함수를 제작해보자.\n\ndraw_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    count( {{ varname }} ) |&gt; \n    ggplot(aes(x = {{ varname }}, y = n )) + \n      geom_col()\n}\n\npenguins |&gt; draw_bar_chart(year)\n\n\n\n\n\n\n\n함수내에서 새로운 변수 이름을 생성하는 경우 := 연산자를 사용해야 한다. 깔끔한 평가(tidy evaluation)에서 = 와 동일한 역할을 수행하는 것이 := 이기 때문이다. 예를 들어, 펭귄이 서식하고 있는 섬을 기준으로 빈도수를 내림차순 막대그래프를 작성할 경우, 함수 내부에서 범주형 변수를 다시 재정의해야 하는데 이 경우 := 연산자의 도입이 필요하다.\n\nlibrary(forcats)\n\norder_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    mutate({{ varname }} :=  fct_rev(fct_infreq( {{ varname }} ))) |&gt;\n    ggplot(aes(y = {{ varname }} )) + \n      geom_bar()\n}\n\npenguins |&gt; order_bar_chart(island)\n\n\n\n\n\n\n\n그래프를 작성할 때 거의 항상 등장하는 문제가 그래프 x-축, y-축 라벨을 붙이고 그래프 제목, 범례 등 텍스트를 넣어야 한다. 이런 경우 stringr, glue 패키지의 다양한 함수를 깔끔한 평가(tidy evaluation)에서 지원하는 기능이 rlang 패키지 englue() 함수다.\n\ndraw_bar_chart &lt;- function(dataframe, varname, penguin_species) {\n  \n  title_label &lt;- rlang::englue(\"남극에 서식하고 있는 펭귄 종 ({{ varname }}) {penguin_species} 빈도수\")\n  \n  dataframe |&gt; \n    filter({{ varname }} == penguin_species) |&gt; \n    count( island ) |&gt; \n    ggplot(aes(x = island, y = n )) + \n      geom_col() +\n      labs(title = title_label)\n}\n\npenguins |&gt; draw_bar_chart(species, \"Adelie\")",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#역사-1",
    "href": "functions_ds.html#역사-1",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.7 역사",
    "text": "18.7 역사\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html",
    "href": "functions_purrr.html",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "19.1 왜 함수형 프로그래밍인가?\n데이터 분석을 아주 추상화해서 간략하게 얘기한다면 데이터프레임을 함수에 넣어 새로운 데이터프레임으로 만들어 내는 것이다.\n데이터 분석, 데이터 전처리, 변수 선택, 모형 개발이 한번에 해결되는 것이 아니라서, 데이터프레임을 함수에 넣어 상태가 변경된 데이터프레임이 생성되고, 이를 다시 함수에 넣어 또다른 변경된 상태 데이터프레임을 얻게 되는 과정을 쭉 반복해 나간다.\n따라서… 데이터 분석에는 함수형 프로그래밍 패러다임을 활용하고, 툴/패키지 개발에는 객체지향 프로그래밍 패러다임 사용이 권장된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#why-functional-programming",
    "href": "functions_purrr.html#why-functional-programming",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "함수로 이해하는 데이터 분석\n\n\n\n\n데이터 분석 작업흐름\n\n\n\n\n데이터 분석과 툴/패키지 도구 개발",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton",
    "href": "functions_purrr.html#functional-programming-newton",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.2 뉴튼 방법(Newton’s Method)\n",
    "text": "19.2 뉴튼 방법(Newton’s Method)\n\n뉴튼-랩슨 알고리즘으로도 알려진 뉴튼(Newton Method) 방법은 컴퓨터를 사용해서 수치해석 방법으로 실함수의 근을 찾아내는 방법이다.\n특정 함수 \\(f\\) 의 근을 찾을 경우, 함수 미분값 \\(f'\\), 초기값 \\(x_0\\)가 주어지면 근사적 근에 가까운 값은 다음과 같이 정의된다.\n\\[x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)}\\]\n이 과정을 반복하게 되면 오차가 매우 적게 근의 값에 도달하게 된다.\n\\[x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\]\n기하적으로 보면, 파란 선은 함수 \\(f\\) 이고, \\(f\\)를 미분한 \\(f'\\) 빨간 선은 뉴턴방법을 활용하여 근을 구해가는 과정을 시각적으로 보여주고 있다. \\(x_{n-1}\\) 보다 \\(x_n\\)이, \\(x_n\\) 보다 \\(x_{n+1}\\)이 함수 \\(f\\) 근에 더 가깝게 접근해 나가는 것이 확인된다.\n\n\n뉴튼 방법 도식화",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton-method",
    "href": "functions_purrr.html#functional-programming-newton-method",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.3 뉴튼 방법 R 코드 1\n",
    "text": "19.3 뉴튼 방법 R 코드 1\n\n뉴튼 방법을 R코들 구현하면 다음과 같이 612의 제곱근 값을 수치적으로 컴퓨터를 활용하여 구할 수 있다. while같은 루프를 활용하여 반복적으로 해를 구하는 것도 가능하지만 재귀를 활용하여 해를 구하는 방법이 코드를 작성하고 읽는 개발자 관점에서는 훨씬 더 편리하고 권장된다.\n하지만, 속도는 while 루프를 사용하는 것이 R에서는 득이 많다. 이유는 오랜 세월에 걸쳐 최적화 과정을 거쳐 진화했기 때문이다.\n\n\nwhile 루프를 사용한 방법\n\nfind_root &lt;- function(guess, init, eps = 10^(-10)){\n    while(abs(init**2 - guess) &gt; eps){\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"현재 값: \", init, \"\\n\")\n    }\n    return(init)\n}\n\nfind_root(612, 10)\n\n현재 값:  35.6 \n현재 값:  26.39551 \n현재 값:  24.79064 \n현재 값:  24.73869 \n현재 값:  24.73863 \n현재 값:  24.73863 \n\n\n[1] 24.73863\n\n\n\n\n재귀를 사용한 방법\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"재귀방법 현재 값: \", init, \"\\n\")\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\nfind_root_recur(612, 10)\n\n재귀방법 현재 값:  35.6 \n재귀방법 현재 값:  26.39551 \n재귀방법 현재 값:  24.79064 \n재귀방법 현재 값:  24.73869 \n재귀방법 현재 값:  24.73863 \n재귀방법 현재 값:  24.73863 \n\n\n[1] 24.73863",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#map-reduce-apply",
    "href": "functions_purrr.html#map-reduce-apply",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.4 Map(), Reduce()와 *apply() 함수",
    "text": "19.4 Map(), Reduce()와 *apply() 함수\n함수를 인자로 받는 함수를 고차함수(High-order function)라고 부른다. 대표적으로 Map(), Reduce()가 있다. 숫자 하나가 아닌 벡터에 대한 제곱근을 구하기 위해서 Map 함수를 사용한다. 2 3\n\n# 제곱근 함수 -------------------------------------------\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\n# 벡터에 대한 제곱근 계산 \n\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nMap(find_root_recur, numbers, init=1, eps = 10^-10)\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 8\n\n[[6]]\n[1] 9\n\n\n숫자 하나를 받는 함수가 아니라, 벡터를 인자로 받아 제곱근을 계산하는 함수를 작성할 경우 함수 내부에서 함수를 인자로 받을 수 있도록 Map 함수를 활용한다.\n\n# `Map` 벡터 제곱근 계산\n\nfind_vec_root_recur &lt;- function(numbers, init, eps = 10^(-10)){\n    return(Map(find_root_recur, numbers, init, eps))\n}\n\nnumbers_z &lt;- c(9, 16, 25, 49, 121)\nfind_vec_root_recur(numbers_z, init=1, eps=10^(-10))\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 11\n\n\n이러한 패턴이 많이 활용되어 *apply 함수가 있어, 이전에 많이 사용했을 것이다. 벡터를 인자로 먼저 넣고, 함수명을 두번째 인자로 넣고, 함수에 들어갈 매개변수를 순서대로 쭉 나열하여 lapply, sapply 함수에 넣는다.\n\n# `lapply` 활용 제급근 계산\n\nlapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 11\n\nsapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n\n[1]  3  4  5  7 11\n\n\nReduce 함수도 삶을 편안하게 할 수 있는, 루프를 회피하는 또다른 방법이다. 이름에서 알 수 있듯이 numbers_z 벡터 원소 각각에 대해 해당 연산작업 +, %%을 수행시킨다. %%는 나머지 연산자로 기본디폴트 설정으로 \\(\\frac{10}{7}\\)로 몫 대신에 나머지 3을 우선 계산하고, 그 다음으로 \\(\\frac{3}{5}\\)로 최종 나머지 3을 순차적으로 계산하여 결과를 도출한다.\n\n# Reduce ----------------------------------------------\nnumbers_z\n\n[1]   9  16  25  49 121\n\nReduce(`+`, numbers_z)\n\n[1] 220\n\nnumbers_z &lt;- c(10,7,5)\nReduce(`%%`, numbers_z)\n\n[1] 3",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr",
    "href": "functions_purrr.html#functional-programming-purrr",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.5 purrr 팩키지",
    "text": "19.5 purrr 팩키지\n*apply 계열 함수는 각각의 자료형에 맞춰 기억하기가 쉽지 않아, 매번 도움말을 찾아 확인하고 코딩을 해야하는 번거러움이 많다. 데이터 분석을 함수형 프로그래밍 패러다임으로 실행하도록 purrr 팩키지가 개발되었다. 이를 통해 데이터 분석 작업이 수월하게 되어 저녁이 있는 삶이 길어질 것으로 기대된다.\n\n19.5.1 purrr 헬로월드\npurrr 팩키지를 불러와서 map_dbl() 함수에 구문에 맞게 작성하면 동일한 결과를 깔끔하게 얻을 수 있다. 즉,\n\n\nmap_dbl(): 벡터, 데이터프레임, 리스트에 대해 함수를 원소별로 적용시켜 결과를 double 숫자형으로 출력시킨다.\n\nnumbers: 함수를 각 원소별로 적용시킬 벡터 입력값\n\nfind_root_recur: 앞서 작성한 뉴톤 방법으로 제곱근을 수치적으로 구하는 사용자 정의함수.\n\ninit=1, eps = 10^-10: 뉴톤 방법을 구현한 사용자 정의함수에 필요한 초기값.\n\n\nlibrary(purrr)\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nmap_dbl(numbers, find_root_recur, init=1, eps = 10^-10)\n\n[1] 4 5 6 7 8 9",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-read-iris",
    "href": "functions_purrr.html#functional-programming-purrr-read-iris",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.6 병렬 데이터 분석",
    "text": "19.6 병렬 데이터 분석\n구글 검색을 통해서 쉽게 iris(붓꽃) 데이터를 구할 수 있다. 이를 불러와서 각 종별로 setosa versicolor, virginica로 나눠 로컬 .csv 파일로 저장하고 나서 이를 다시 불러오는 사례를 함수형 프로그래밍으로 구현해본다.\n\n\n붓꽃 데이터 불러오기\n\n먼저 iris.csv 파일을 R로 불러와서 각 종별로 나눠서 iris_종명.csv 파일형식으로 저장시킨다.\n\nlibrary(tidyverse)\niris_df &lt;- read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\")\n\niris_species &lt;- iris_df %&gt;% \n  count(species) %&gt;% pull(species)\n\nfor(i in 1:nrow(iris_df)) {\n  tmp_df &lt;- iris_df %&gt;% \n    filter(species == iris_species[i])\n  species_name &lt;- iris_species[i]\n  tmp_df %&gt;% write_csv(paste0(\"data/iris_\", species_name, \".csv\"))\n}\n\nSys.glob(\"data/iris_*.csv\")\n\n[1] \"data/iris_NA.csv\"         \"data/iris_setosa.csv\"    \n[3] \"data/iris_versicolor.csv\" \"data/iris_virginica.csv\" \n\n\n로컬 파일 iris_종명.csv 형식으로 저장된 데이터를 함수형 프로그래밍을 통해 불러와서 분석작업을 수행해보자. map() 함수를 사용해서 각 종별로 데이터를 깔끔하게 불러왔다.\niris_filename 벡터에 iris_종명.csv과 경로명이 포함된 문자열을 저장시켜 놓고 read_csv() 함수를 각 벡터 원소에 적용시켜 출력값으로 리스트 iris_list 객체를 생성시켰다.\n\niris_filename &lt;- c(\"data/iris_setosa.csv\", \"data/iris_versicolor.csv\", \"data/iris_virginica.csv\")\n\niris_list &lt;- map(iris_filename, read_csv) %&gt;% \n  set_names(iris_species)\n\niris_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name       value              \n  &lt;chr&gt;      &lt;list&gt;             \n1 setosa     &lt;spc_tbl_ [50 × 5]&gt;\n2 versicolor &lt;spc_tbl_ [50 × 5]&gt;\n3 virginica  &lt;spc_tbl_ [50 × 5]&gt;\n\n\niris_list 각 원소는 데이터프레임이라 summary 함수를 사용해서 기술 통계량을 구할 수도 있다. 물론 cor() 함수를 사용해서 iris_list의 각 원소를 지정하는 .x 여기서는 종별 데이터프레임에서 변수 두개를 추출하여 sepal_length, sepal_width 이 둘간의 스피커만 상관계수를 계산하는데 출력값이 double 연속형이라 map_dbl로 저정하여 작업시킨다.\n\nmap(iris_list, summary)\n\n$setosa\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n 1st Qu.:4.800   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.200  \n Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n Mean   :5.006   Mean   :3.418   Mean   :1.464   Mean   :0.244  \n 3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$versicolor\n  sepal_length    sepal_width     petal_length   petal_width   \n Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000  \n 1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200  \n Median :5.900   Median :2.800   Median :4.35   Median :1.300  \n Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326  \n 3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500  \n Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$virginica\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n 1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n 3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nmap_dbl(iris_list, ~cor(.x$sepal_length, .x$sepal_width, method = \"spearman\"))\n\n    setosa versicolor  virginica \n 0.7686085  0.5176060  0.4265165",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-random-number",
    "href": "functions_purrr.html#functional-programming-purrr-random-number",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.7 표본추출",
    "text": "19.7 표본추출\n서로 다른 난수를 생성시키는 방법을 살펴보자. 정규분포를 가정하고 평균과 표준편차를 달리하는 모수를 지정하고 난수갯수도 숫자를 달리하여 난수를 생성시킨다.\n\n19.7.1 \\(\\mu\\) 평균 변화\n정규분포에서 난수를 10개 추출하는데 표준편차는 1로 고정시키고, 평균만 달리한다. 평균만 달리하기 때문에 map() 함수를 그대로 사용한다. 즉, 입력값으로 평균만 달리하는 리스트를 입력값으로 넣는다.\n\n## 평균을 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\n\nsim_mu_name &lt;- paste0(\"mu: \", normal_mean)\n\nsim_mu_list &lt;- map(normal_mean, ~ data.frame(mean = .x, \n                            random_number = rnorm(mean=.x, sd=1, n=10))) %&gt;% \n  set_names(sim_mu_name)\n\nmap_dbl(sim_mu_list, ~mean(.x$random_number))\n\n    mu: 1     mu: 5    mu: 10 \n 1.027545  5.016667 10.271774 \n\nsim_mu_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name   value        \n  &lt;chr&gt;  &lt;list&gt;       \n1 mu: 1  &lt;df [10 × 2]&gt;\n2 mu: 5  &lt;df [10 × 2]&gt;\n3 mu: 10 &lt;df [10 × 2]&gt;\n\n\n\n19.7.2 \\(\\mu\\) 평균, \\(\\sigma\\) 표준편차\n난수갯수만 고정시키고 평균과 표준편차를 달리하여 난수를 정규분포에서 추출한다. 입력값으로 평균과 표준편차 두개가 되기 때문에 map2() 함수를 사용한다.\n\n## 평균과 표준편차를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\n\nsim_mu_sd_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd)\n\nsim_mu_sd_list &lt;- map2(normal_mean, normal_sd, \n                        ~ data.frame(mean = .x, sd = .y,\n                            random_number = rnorm(mean=.x, sd=.y, n=10))) %&gt;% \n  set_names(sim_mu_sd_name)\n\nmap_dbl(sim_mu_sd_list, ~sd(.x$random_number))\n\nmu: 1,  sd: 10  mu: 5,  sd: 5 mu: 10,  sd: 1 \n      9.093706       3.150940       1.170196 \n\nsim_mu_sd_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name           value        \n  &lt;chr&gt;          &lt;list&gt;       \n1 mu: 1,  sd: 10 &lt;df [10 × 3]&gt;\n2 mu: 5,  sd: 5  &lt;df [10 × 3]&gt;\n3 mu: 10,  sd: 1 &lt;df [10 × 3]&gt;\n\n\n\n19.7.3 \\(\\mu\\), \\(\\sigma\\), 표본크기\n\\(\\mu\\) 평균, \\(\\sigma\\) 표준편차, 표본크기를 모두 다르게 지정하여 난수를 추출한다. 이런 경우 pmap() 함수를 사용하고 입력 리스트가 다수라 이를 normal_list로 한번더 감싸서 이름이 붙은 리스트(named list)형태로 넣어주고, 이를 function() 함수의 내부 인수로 사용한다.\n\n## 평균, 표준편차, 표본크기를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\nnormal_size &lt;- list(10,20,30)\n\nsim_mu_sd_size_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd,\n                              \"  size: \", normal_size)\n\nnormal_list &lt;- list(normal_mean=normal_mean, normal_sd=normal_sd, normal_size=normal_size)\n\nsim_mu_sd_size_list &lt;- pmap(normal_list,\n                            function(normal_mean, normal_sd, normal_size)\n                        data.frame(mean=normal_mean, sd = normal_sd, size = normal_size,\n                            random_number = rnorm(mean=normal_mean, sd=normal_sd, n=normal_size))) %&gt;% \n  set_names(sim_mu_sd_size_name)\n\nmap_dbl(sim_mu_sd_size_list, ~length(.x$random_number))\n\nmu: 1,  sd: 10  size: 10  mu: 5,  sd: 5  size: 20 mu: 10,  sd: 1  size: 30 \n                      10                       20                       30 \n\nsim_mu_sd_size_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name                     value        \n  &lt;chr&gt;                    &lt;list&gt;       \n1 mu: 1,  sd: 10  size: 10 &lt;df [10 × 4]&gt;\n2 mu: 5,  sd: 5  size: 20  &lt;df [20 × 4]&gt;\n3 mu: 10,  sd: 1  size: 30 &lt;df [30 × 4]&gt;",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-ggplot",
    "href": "functions_purrr.html#functional-programming-purrr-ggplot",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.8 ggplot 시각화",
    "text": "19.8 ggplot 시각화\nlist-column을 활용하여 티블(tibble) 데이터프레임에 담아서 시각화를 진행해도 되고, 다른 방법으로 리스트에 담아서 이를 한장에 찍는 것도 가능하다. 4\n\nlibrary(gapminder)\n\n## 데이터 -----\nthree_country &lt;-  c(\"Korea, Rep.\", \"Japan\", \"China\")\n\ngapminder_tbl &lt;- gapminder %&gt;% \n  filter(str_detect(continent, \"Asia\")) %&gt;% \n  group_by(continent, country) %&gt;% \n  nest() %&gt;% \n  select(-continent) %&gt;% \n  filter(country %in% three_country )\n\n## 티블 데이터 시각화 -----\ngapminder_plot_tbl &lt;- gapminder_tbl %&gt;% \n  mutate(graph = map2(data, country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y)))\n\ngapminder_plot_tbl\n\n# A tibble: 3 × 4\n# Groups:   continent, country [3]\n  continent country     data              graph \n  &lt;fct&gt;     &lt;fct&gt;       &lt;list&gt;            &lt;list&gt;\n1 Asia      China       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n2 Asia      Japan       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n3 Asia      Korea, Rep. &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n\n## 리스트 데이터 시각화 -----\ngapminder_plot &lt;- map2(gapminder_tbl$data , three_country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y))\n\nwalk(gapminder_plot, print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 리스트 데이터 시각화 - 한장에 찍기 -----\ncowplot::plot_grid(plotlist = gapminder_plot)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-theory-practice",
    "href": "functions_purrr.html#fp-theory-practice",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.9 함수형 프로그래밍 이론과 실제",
    "text": "19.9 함수형 프로그래밍 이론과 실제\n함수는 다음과 같이 될 수도 있어 함수형 프로그래밍 언어가 된다. 5\n\n함수의 인자\n함수로 반환\n리스트에 저장\n변수에 저장\n무명함수\n조작할 수 있다.\n\nFirstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do anything with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.\n\n\n\n\n\n\nJohn Chambers 창시자가 말하는 R 계산의 기본원칙\n\n\n\n\n존재하는 모든 것은 객체다. (Everything that exists is an object.)\n일어나는 모든 것은 함수호출이다. (Everything that happens is a function call.)\n\n\nlibrary(tidyverse)\nclass(`%&gt;%`)\n\n[1] \"function\"\n\nclass(`$`)\n\n[1] \"function\"\n\nclass(`&lt;-`)\n\n[1] \"function\"\n\nclass(`+`)\n\n[1] \"function\"",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#pure-vs-impure-function",
    "href": "functions_purrr.html#pure-vs-impure-function",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.10 순수한 함수 vs 불순한 함수",
    "text": "19.10 순수한 함수 vs 불순한 함수\n순수한 함수(pure function)는 입력값에만 출력값이 의존하게 되는 특성과 부수효과(side-effect)를 갖지 않는 반면 순수하지 않은 함수(impure function)는 환경에 의존하며 부수효과도 갖는다.\n\n\n순수한 함수(pure function)\n\nmin(1:100)\n\n[1] 1\n\nmean(1:100)\n\n[1] 50.5\n\n\n\n순수하지 않은 함수(impure function)\n\nSys.time()\n\n[1] \"2024-01-28 21:49:56 KST\"\n\nrnorm(10)\n\n [1]  0.7124784 -1.5636042 -0.1669109 -0.4684979  0.5287181 -1.1670593\n [7]  0.1715563 -0.5630649 -0.2030090 -0.5322956\n\n# write_csv(\"data/sample.csv\")\n\n\n\n\n19.10.1 무명함수와 매퍼\n\\(\\lambda\\) (람다) 함수는 무명(anonymous) 함수는 함수명을 갖는 일반적인 함수와 비교하여 함수의 좋은 점은 그대로 누리면서 함수가 많아 함수명으로 메모리가 난잡하게 지져분해지는 것을 막을 수 있다.\n무명함수로 기능르 구현한 후에 매퍼(mapper)를 사용해서 as_mapper() 명칭을 부여하여 함수처럼 사용하는 것도 가능하다. 매퍼(mapper)를 사용하는 이유를 다음과 같이 정리할 수 있다.\n\n간결함(Concise)\n가독성(Easy to read)\n재사용성(Reusable)\n\n정치인 페이스북 페이지에서 팬수를 추출한다. 그리고 이를 이름이 부은 리스트(named list)로 일자별 팬수 추이를 리스트로 준비한다. 그리고 나서 안철수, 문재인, 심상정 세 후보에 대한 최고 팬수증가를 무명함수로 계산한다.\n\nlibrary(tidyverse)\n## 데이터프레임을 리스트로 변환\nahn_df  &lt;- read_csv(\"data/fb_ahn.csv\")  %&gt;% rename(fans = ahn_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nmoon_df &lt;- read_csv(\"data/fb_moon.csv\") %&gt;% rename(fans = moon_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nsim_df  &lt;- read_csv(\"data/fb_sim.csv\")  %&gt;% rename(fans = sim_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\n\nconvert_to_list &lt;- function(df) {\n  df_fans_v &lt;- df$fans %&gt;% \n    set_names(df$fdate)\n  return(df_fans_v)\n}\n\nahn_v  &lt;- convert_to_list(ahn_df)\nmoon_v &lt;- convert_to_list(moon_df)\nsim_v  &lt;- convert_to_list(sim_df)\n\nfans_lst &lt;- list(ahn_fans  = ahn_v,\n                 moon_fans = moon_v,\n                 sim_fans  = sim_v)\n\nlistviewer::jsonedit(fans_lst)\n\n\n\n\n## 무명함수 테스트\nmap_dbl(fans_lst, ~max(.x))\n\n ahn_fans moon_fans  sim_fans \n      796      1464      2029 \n\n\nrlang_lambda_function 무명함수로 increase_1000_fans 작성해서 일별 팬수 증가가 1000명 이상인 경우 keep() 함수를 사용해서 각 후보별로 추출할 수 있다. discard() 함수를 사용해서 반대로 버려버릴 수도 있다.\n\nincrease_1000_fans &lt;- as_mapper( ~.x &gt; 1000)\n\nmap(fans_lst, ~keep(.x, increase_1000_fans))\n\n$ahn_fans\nnamed numeric(0)\n\n$moon_fans\n2017-03-28 2017-04-18 2017-04-20 \n      1464       1310       1093 \n\n$sim_fans\n2017-03-12 2017-03-13 2017-04-14 2017-04-19 2017-04-20 2017-04-21 2017-04-24 \n      1301       1079       1070       1441       1190       1025       1948 \n2017-04-25 \n      2029 \n\n\n술어논리(predicate logic)은 조건을 테스트하여 참(TRUE), 거짓(FALSE)을 반환시킨다. every, some을 사용하여 팬수가 증가한 날이 매일 1,000명이 증가했는지, 전부는 아니고 일부 특정한 날에 1,000명이 증가했는지 파악할 수 있다.\n\n## 세후보 팬수가 매일 모두 1000명 이상 증가했나요?\nmap(fans_lst, ~every(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] FALSE\n\n$sim_fans\n[1] FALSE\n\n## 세후보 팬수가 전체는 아니고 일부 특정한 날에 1000명 이상 증가했나요?\nmap(fans_lst, ~some(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] TRUE\n\n$sim_fans\n[1] TRUE\n\n\n\n19.10.2 고차 함수(High order function)\n고차 함수(High order function)는 함수의 인자로 함수를 받아 함수로 반환시키는 함수를 지칭한다. high_order_fun 함수는 함수를 인자(func)로 받아 함수를 반환시키는 고차함수다. 평균 함수(mean)를 인자로 넣어 출력값으로 mean_na() 함수를 새롭게 생성시킨다. NA가 포함된 벡터를 넣어 평균값을 계산하게 된다.\n\nhigh_order_fun &lt;- function(func){\n  function(...){\n    func(..., na.rm = TRUE)\n  }\n}\n\nmean_na &lt;- high_order_fun(mean)\nmean_na( c(NA, 1:10) )\n\n[1] 5.5\n\n\n벡터가 입력값으로 들어가서 벡터가 출력값으로 나오는 보통 함수(Regular Function)외에 고차함수는 3가지 유형이 있다.\n\n벡터 → 함수: 함수공장(Function Factory)\n함수 → 벡터: Functional - for루프를 purrr 팩키지 map() 함수로 대체\n함수 → 함수: 함수연산자(Function Operator) - Functional과 함께 사용될 경우 adverbs로서 강력한 기능을 발휘\n\n\n\n고차함수 유형\n\n\n19.10.3 부사 - safely, possibly,…\npurrr 팩키지의 대표적인 부사(adverbs)에는 possibly()와 safely()가 있다. 그외에도 silently(), surely() 등 다른 부사도 있으니 필요한 경우 purrr 팩키지 문서를 참조한다.\nsafely(mean)은 동사 함수(mean())를 받아 부사 safely()로 “부사 + 동사”로 기능이 추가된 부사 동사를 반환시킨다. 따라서, NA가 추가된 벡터를 넣을 경우 $result와 $error를 원소로 갖는 리스트를 반환시킨다.\n\nmean_safe &lt;- safely(mean)\nclass(mean_safe)\n\n[1] \"function\"\n\nmean_safe(c(NA, 1:10))\n\n$result\n[1] NA\n\n$error\nNULL\n\n\n이를 활용하여 오류처리작업을 간결하게 수행시킬 수 있다. $result와 $error을 원소로 갖는 리스트를 반환시키기 때문에 오류와 결과값을 추출하여 후속작업을 수행하여 디버깅하는데 유용하게 활용할 수 있다.\n\ntest_lst &lt;- list(\"NA\", 1,2,3,4,5)\nlog_safe &lt;- safely(log)\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"result\")\n\n[[1]]\nNULL\n\n[[2]]\n[1] 0\n\n[[3]]\n[1] 0.6931472\n\n[[4]]\n[1] 1.098612\n\n[[5]]\n[1] 1.386294\n\n[[6]]\n[1] 1.609438\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"error\")\n\n[[1]]\n&lt;simpleError in .Primitive(\"log\")(x, base): 수학함수에 숫자가 아닌 인자가 전달되었습니다&gt;\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n\n반면에 possibly()는 결과와 otherwise 값을 반환시켜서 오류가 발생되면 중단되는 것이 아니라 오류가 있다는 사실을 알고 예외처리시킨 후에 쭉 정상진행시킨다.\n\nmax_possibly &lt;- possibly(sum, otherwise = \"watch out\")\n\nmax_possibly(c(1:10))\n\n[1] 55\n\nmax_possibly(c(NA, 1:10))\n\n[1] NA\n\nmax_possibly(c(\"NA\", 1:10))\n\n[1] \"watch out\"\n\n\npossibly()는 부울 논리값, NA, 문자열, 숫자를 반환시킬 수 있다.\ntranspose()와 결합하여 safely(), possibly() 결과를 변형시킬 수도 있다.\n\nmap(test_lst, log_safe) %&gt;% length()\n\n[1] 6\n\nmap(test_lst, log_safe) %&gt;% transpose() %&gt;% length()\n\n[1] 2\n\n\ncompact()를 사용해서 NULL을 제거하는데, 앞서 possibly()의 인자로 otherwise=를 지정하는 경우 otherwise=NULL와 같이 정의해서 예외처리로 NULL을 만들어 내고 compact()로 정상처리된 데이터만 얻는 작업흐름을 갖춘다.\n\nnull_lst &lt;- list(1, NULL, 3, 4, NULL, 6, 7, NA)\ncompact(null_lst)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 7\n\n[[6]]\n[1] NA\n\npossibly_log &lt;- possibly(log, otherwise = NULL)\nmap(null_lst, possibly_log) %&gt;% compact()\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1.098612\n\n[[3]]\n[1] 1.386294\n\n[[4]]\n[1] 1.791759\n\n[[5]]\n[1] 1.94591\n\n[[6]]\n[1] NA",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-clean-code",
    "href": "functions_purrr.html#fp-clean-code",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.11 깨끗한 코드",
    "text": "19.11 깨끗한 코드\nround_mean() 함수를 compose() 함수를 사용해서 mean() 함수로 평균을 구한 후에 round()함수로 반올림하는 코드를 다음과 같이 쉽게 작성할 수 있다. 6\n\nround_mean &lt;- compose(round, mean)\nround_mean(1:10)\n\n[1] 6\n\n\n두번째 사례로 전형적인 데이터 분석 사례로 lm() → anova() → tidy()를 통해 한방에 선형회귀 모형 산출물을 깨끗한 코드로 작성하는 사례를 살펴보자.\nmtcars 데이터셋에서 연비 예측에 변수 두개를 넣고 일반적인 lm() 선형예측모형 제작방식과 동일하게 인자를 넣는다.\n\nclean_lm &lt;- compose(broom::tidy, anova, lm)\nclean_lm(mpg ~ hp + wt, data=mtcars)\n\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 hp            1  678. 678.       101.   5.99e-11\n2 wt            1  253. 253.        37.6  1.12e- 6\n3 Residuals    29  195.   6.73      NA   NA       \n\n\ncompose()를 통해 함수를 조합하는 경우 함수의 인자를 함께 전달해야될 경우가 있다. 이와 같은 경우 partial()을 사용해서 인자를 넘기는 함수를 제작하여 compose()에 넣어준다.\n\nrobust_round_mean &lt;- compose(\n  partial(round, digits=1),\n  partial(mean, na.rm=TRUE))\nrobust_round_mean(c(NA, 1:10))\n\n[1] 5.5\n\n\n리스트 칼럼(list-column)과 결합하여 모형에서 나온 데이터 분석결과를 깔끔하게 코드로 제작해보자. 먼저 lm을 돌려 모형 요약하는 함수 summary를 통해 r.squared값을 추출하는 함수를 summary_lm으로 제작한다.\n그리고 나서 nest() 함수로 리스트 칼럼(list-column)을 만들고 두개의 집단 수동/자동을 나타내는 am 변수를 그룹으로 삼아 두 집단에 속한 수동/자동 데이터에 대한 선형 회귀모형을 적합시키고 나서 “r.squared”값을 추출하여 이를 티블 데이터프레임에 저장시킨다.\n\nsummary_lm &lt;- compose(summary, lm) \n\nmtcars %&gt;%\n  group_by(am) %&gt;%\n  nest() %&gt;%\n  mutate(lm_mod = map(data, ~ summary_lm(mpg ~ hp + wt, data = .x)),\n         r_squared = map(lm_mod, \"r.squared\")) %&gt;%\n  unnest(r_squared)\n\n# A tibble: 2 × 4\n# Groups:   am [2]\n     am data               lm_mod     r_squared\n  &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n1     1 &lt;tibble [13 × 10]&gt; &lt;smmry.lm&gt;     0.837\n2     0 &lt;tibble [19 × 10]&gt; &lt;smmry.lm&gt;     0.768",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#footnotes",
    "href": "functions_purrr.html#footnotes",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "Bruno Rodrigues(2016), “Functional programming and unit testing for data munging with R”, LeanPub, 2016-12-23↩︎\npurrr tutorial: Lessons and Examples↩︎\npurrr tutorial GitHub Webpage↩︎\nVery statisticious (August 20, 2018), “Automating exploratory plots with ggplot2 and purrr”↩︎\nAdvanced R, “Introduction”↩︎\nColin Fay, “A Crazy Little Thing Called {purrr} - Part 5: code optimization”↩︎",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "\n20  쿼토\n",
    "section": "",
    "text": "20.1 쿼토설치",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#쿼토설치",
    "href": "quarto.html#쿼토설치",
    "title": "\n20  쿼토\n",
    "section": "",
    "text": "20.1.1 설치방법\nquarto 웹사이트에서 Quarto CLI 엔진을 설치한다. 통합개발도구(IDE)를 설치한다. Quarto CLI를 지원하는 IDE는 VS Code, RStudio, Jupyter, VIM/Emacs 와 같은 텍스트 편집기가 포함된다. IDE까지 설치를 했다면 literate programming 방식으로 마크다운과 프로그래밍 언어를 결합하여 출판을 위한 전문 문서 저작을 시작한다.\n\n\n20.1.2 윈도우 설치\nQuarto를 운영체제에 맞춰 설치한다. Quarto 는 기본적으로 CLI 라서 설치 후 제대로 설정이 되었는지는 환경설정에 경로를 등록해줘야 한다.\n\n\nQuarto 다운로드\nQuarto 설치\nQuarto CLI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n윈도우 시스템의 경우 quarto.exe가 아니고 quarto.cmd 라 이에 유의한다. 즉, 제어판 → 환경 변수 설정 … 에서 \"C:\\Users\\사용자명\\AppData\\Local\\Programs\\Quarto\\bin 디렉토리를 등록한 후 quarto.cmd 을 사용해서 출판한다. 쿼토 1.3 버전이 출시되면서 이런 문제는 해결되었다.\n\nSys.which(\"quarto\")\n                                                                  quarto \n\"C:\\\\Users\\\\STATKC~1\\\\AppData\\\\Local\\\\Programs\\\\Quarto\\\\bin\\\\quarto.cmd\"",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#쿼토-출판",
    "href": "quarto.html#쿼토-출판",
    "title": "\n20  쿼토\n",
    "section": "\n20.2 쿼토 출판",
    "text": "20.2 쿼토 출판\n\n20.2.1 출판 플랫폼\n데이터 사이언스 저작물을 제작하게 되면 그 다음 단계로 출판을 해야하는데 다양한 문서를 모아 프로젝트로 담아 Quarto Pub에 전자출판한다. 다른 출판 플랫폼으로 netlify, GitHub Pages, RStudio Connect가 많이 사용된다.\n\n\n20.2.2 Quarto Pub 출판 1\n\nQuarto Pub 웹사이트에 출판하는 방식은 Quarto CLI 를 사용한다. 필히 RStudio 내부 Terminal을 사용해서 Quarto Pub으로 출판한다.\n\nquarto.cmd publish quarto-pub\n? Authorize (Y/n) › \n❯ In order to publish to Quarto Pub you need to\n  authorize your account. Please be sure you are\n  logged into the correct Quarto Pub account in \n  your default web browser, then press Enter or \n  'Y' to authorize.\n\n첫번째 출판하게 되면 인증작업을 수행하고 나면 _publish.yml 파일이 하나 생성된다.\n\n- source: project\n  quarto-pub:\n    - id: 1fa3ab1f-c010-453a-aaf2-f462bd074a66\n      url: 'https://quartopub.com/sites/statkclee/quarto-ds'\n\n이제 모든 준비가 되었기 때문에 다음 명령어로 작성한 출판 문서를 포함한 웹사이트를 로컬에서 미리 확인 한 후에 Quarto Pub으로 전자출판한다. 윈도우에서는 RStudio 내부 Terminal CLI를 사용하는 것을 권장한다.\n\nquarto preview\nquarto publish quarto-pub",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#문서-컴파일러-1",
    "href": "quarto.html#문서-컴파일러-1",
    "title": "\n20  쿼토\n",
    "section": "\n20.3 문서 컴파일러 2\n",
    "text": "20.3 문서 컴파일러 2\n\nQuarto 는 Pandoc에 기반한 오픈소스 과학기술 출판시스템이다. 하지만 특정 언어에 종속되지 않고 R, 파이썬, 쥴리아, 자바스크립트(Observable JS) 를 지원하고 있으며 이를 통해 다음 출판 저작물 작성이 가능하다.\n\n크게 세가지 부분에 대해 출판시스템에 대한 고민이 필요하다.\n\n콘텐츠(Content): 저작과 관련된 문서 내용\n디자인(Design): 출판 결과물에 대한 외양(Look and Feel)\n형식(Format): 출판물 최종 산출물\n\nQuarto 는 Literate Programming System 으로 다양한 언어를 지원하고 다양한 출판결과물을 연결시키는 핵심 엔진으로 Pandoc 을 사용한다.\n\n\n\n\n\n\n\nComputations\n문서 저작\n출력물\n\n\nPython, R, Julia, Observable JS\nPandoc, 마크다운 (Markdown)\n문서, 웹사이트, PPT, 책, 블로그등\n\n\n좀더 구체적으로 전문적인 출판을 위해서 문서저작에 다양한 기능과 함께 출판 산출물을 지원한다.\n\n문서저작(pandoc): 마크다운, 수식, 인용, 서지관리, 콜아웃(callout), 고급 layout 등\n출판산출물: 고품질 기사(article), 보고서, PPT, 웹사이트, 블로그, (HTML, PDF, MS 워드, ePub 등) 전자책",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#single-sourcing-출판저작",
    "href": "quarto.html#single-sourcing-출판저작",
    "title": "\n20  쿼토\n",
    "section": "\n20.4 Single Sourcing 출판저작",
    "text": "20.4 Single Sourcing 출판저작\n데이터 사이언스 출판저작에 다소 차이는 있지만 출판에 대한 대체적인 방식은 유사할 것으로 보인다. 즉, Single Sourcing 을 콘텐츠 저작, 디자인, 최종 출판물 관리까지 일원화되어 자동화되어 체계적으로 관리된다면 중복되는 낭비는 물론 재현가능성도 높여 과학기술 출판저작물로 가장 이상적으로 간주되고 있다.\n\n\n문제점\n개념\nSingle Sourcing Multi-Use\n\n\n\n\n\n\n\n\n그림 20.1: 문제점\n\n\n\n\n\n\n\n\n\n그림 20.2: Single Sourcing 개념\n\n\n\n\n\n\n\n\n\n그림 20.3: Single Sourcing Multi-Use",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#작업흐름",
    "href": "quarto.html#작업흐름",
    "title": "\n20  쿼토\n",
    "section": "\n20.5 작업흐름",
    "text": "20.5 작업흐름\n기존 R .Rmd, 파이썬 .ipynb 확장자를 갖는 작업흐름이 .qmd 파일로 단일화되는 것이 가장 큰 특징이다. 따라서 마크다운으로 콘텐츠를 작성하고 프로그래밍 코드를 R, 파이썬, 자바스크립트, 쥴리아 로 작성하게 되면 자동으로 계산을 수행하고 결과물을 마크다운으로 변환시키기 때문에 후속 작업을 신경쓰지 않고 원하는 결과물을 얻을 수 있는 장점이 있다.\n\n\nR (.Rmd)\n파이썬 (주피터)\nQuarto - R\nQuarto - 파이썬",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#주요-기능",
    "href": "quarto.html#주요-기능",
    "title": "\n20  쿼토\n",
    "section": "\n20.6 주요 기능",
    "text": "20.6 주요 기능\n\n\nFeature\nR Markdown\nQuarto\n\n\n\nBasic Formats\n\nhtml_document\npdf_document\nword_document\n\n\nhtml\npdf\ndocx\n\n\n\nBeamer\n\nbeamer_presentation\n\n\nbeamer\n\n\n\nPowerPoint\n\npowerpoint_presentation\n\n\npptx\n\n\n\nHTML Slides\n\nxaringan\nrevealjs\n\n\nrevealjs\n\n\n\nAdvanced Layout\n\ntufte\ndistill\n\n\nQuarto Article Layout\n\n\n\nCross References\n\nhtml_document2\npdf_document2\nword_document2\n\n\nQuarto Crossrefs\n\n\n\nWebsites & Blogs\n\nblogdown\ndistill\n\n\nQuarto Websites\nQuarto Blogs\n\n\n\nBooks\n\nbookdown\n\nQuarto Books\n\n\nInteractivity\nShiny Documents\nQuarto Interactive Documents\n\n\nPaged HTML\npagedown\nSummer 2022\n\n\nJournal Articles\nrticles\nSummer 2022\n\n\nDashboards\n\nflexdashboard |\nFall 2022",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#위지윅-vs-위지윔",
    "href": "quarto.html#위지윅-vs-위지윔",
    "title": "\n20  쿼토\n",
    "section": "\n20.7 위지윅 vs 위지윔",
    "text": "20.7 위지윅 vs 위지윔\n신속하고 빠르게 누구나 짧은 학습을 통해서 문서를 저작하고 출판할 수 있는 방식은 아래한글 혹은 MS워드 워드프로세서를 사용하는데 이는 위지위그(WYSIWYG: What You See Is What You Get, “보는 대로 얻는다”)에 기초한 것으로 문서 편집 과정에서 화면에 포맷된 낱말, 문장이 출력물과 동일하게 나오는 방식을 말한다. 위지윅의 대척점에 있는 것이 위지윔(WYSIWYM, What You See Is What You Mean)으로 대표적인 것인 \\(\\LaTeX\\) 으로 구조화된 방식으로 문서를 작성하면 컴파일을 통해서 최종 문서가 미려한 출판가능한 PDF, PS, DVI 등 확장자를 갖는 출판결과물을 얻을 수 있다.\n\n\n20.7.1 블로그 저작 소프트웨어\n개인용 컴퓨터가 보급되면서 아래한글과 같은 워드 프로세서를 사용해서 저작을 하는 것이 일반화되었지만 곧이어 인터넷이 보급되면서 웹에 문서를 저작하는 것이 이제는 더욱 중요하게 되었다. 전문 개발자가 아닌 일반인이 HTML, CSS, JavaScript를 학습하여 웹에 문서를 제작하고 출판하는 것은 난이도가 있다보니 워드프레스와 티스토리 같은 위지위그 패러다임을 채택한 저작도구가 사용되고 있으나 상대적으로 HTML, CSS, JavaScript을 조합한 방식과 비교하여 고급스러운 면과 함께 정교함에 있어 아쉬움이 있는 것도 사실이다.\n\n\n워드프레스와 티스토리\nHTML + CSS + 자바스크립트",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#쿼토-저작",
    "href": "quarto.html#쿼토-저작",
    "title": "\n20  쿼토\n",
    "section": "\n20.8 쿼토 저작",
    "text": "20.8 쿼토 저작\nQuarto 는 10년전부터 시작된 knitr 경험을 많이 녹여냈고 위지윔 패러다임에 기초를 하고 있다고 볼 수 있다. RStudio를 IDE로 Quarto CLI와 함께 출판물을 저작한다면 편집기에 있는 Visual 모드가 있어 위지윅 패러다임도 문서저작에 사용이 가능하다. 특히, R, 파이썬, SQL, 자바스크립트 등 컴퓨팅 엔진을 달리하여 문서에 그래프, 표, 인터랙티브 결과물도 함께 담을 수 있는 것은 커다란 장점이다.\n\nQuarto 저작은 크게 3가지 구성요소로 되어 있다.\n\n메타데이터: YAML\n텍스트: 마크다운\n코드: knitr, jupyter\n\n\n상기 구성요소를 조합하게 되면 다양한 데이터 사이언스 웹사이트를 비롯한 출판물을 제작하게 된다.\n\n\nQuarto 문서 구성요소\n\n\n20.8.1 YAML\n메타데이터는 YAML인데 GNU처럼 “Yet Another Markup Language” 혹은 “YAML Ain’t Markup Language”을 줄인단어다.\n\n\n키값\n출력옵션\n상세 출력옵션\n\n\n\n---\nkey: value\n---\n\n\n---\nformat: something\n---\n. . .\n---\nformat: pdf\n---\n---\nformat: pdf\n---\n---\nformat: revealjs\n---\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n왜 YAML이 필요하게 된 것인가? YAML은 단순히 KEY: Value 에 불과한데 CLI를 이해하게 되면 왜 YAML을 사용하는 것이 유용한지 이해할 수 있다. 먼저 간단한 CLI 명령어를 YAML로 변환해보자.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html\n\n\n\n\n---\nformat: pdf\n---\n\n\n한단계 더 들어가서 좀더 많은 선택옵션을 넣어 고급 기능을 넣는 사례를 살펴본다.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html -M code fold:true\n\n\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n20.8.2 마크다운\n데이터 과학 문서 웹사이트에 “마크다운 기초”, “고급 마크다운”, “R 마크다운 실무” 를 참조한다.\n\n20.8.3 코드\nR, 파이썬, SQL, 자바스크립트 등 버그 없이 정상 동작하는 프로그램을 작성하여 포함시킨다.\n\n20.8.4 YAML 코드편집\nRStudio, VSCode IDE는 탭-자동완성(tab-completion)을 제공한다. 즉, 첫단어를 타이핑하고 탭을 연결하여 키보드를 치게되면 연관 명령어가 나와 선택하면 된다. 혹은 Ctrl + space 단축키를 치게되면 전체 명령어가 나온다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "\n20  쿼토\n",
    "section": "",
    "text": "Quarto Pub↩︎\nAdam Hyde (Aug 16, 2021), “Single Source Publishing - A investigation of what Single Source Publishing is and how this ‘holy grail’ can be achieved.”↩︎",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "map.html",
    "href": "map.html",
    "title": "\n21  지도 공간정보\n",
    "section": "",
    "text": "21.1 도구의 진화\nGIS, 좌표계, 파일 형식, 계층(Layer), 교차분석 등 주요개념을 바탕으로 도구 진화과정을 살펴보자. 초기 PC가 보급되면서 1980년대 GIS 시스템은 사일로 형태 고가 시스템이었으며, 1990년대에는 1980년대 개발된 GIS 시스템간 연결이 시작되면서 스파게티 현상이 심해지면서 새로운 애플리케이션과 파일 형식이 도입되는데 필요 이상의 낭비가 심해지는 구조가 되었다.\n2000년대 GDAL(Geospatial Data Abstraction Layer)이 등장하여 개발자가 각 파일 형식별로 서로 다른 드라이버를 작성하는 대신 GDAL 클라이언트 드라이버만 개발하면 되기 때문에 일대 혁신이 일어났다. 2010년대는 인공위성을 통한 대량의 데이터가 넘쳐나며 이를 저장하기 위해서 클라우드 서비스가 우후죽순처럼 생겨났고 각기 다른 파편화된 진화과정이 일어났다. 2020년대 넘어서면서 1990년대와 유사한 상황이 재현되었고, 이를 해결하고 새로운 전환점을 만들기 위해 OpenEO가 제시되며 사용자와 데이터 센터 사이에서 새로운 표준으로 자리잡게 되면서 R, 파이썬 등 데이터 과학 프로그래밍 언어를 통한 공간정보 가치 창출이 용이해졌다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#도구의-진화",
    "href": "map.html#도구의-진화",
    "title": "\n21  지도 공간정보\n",
    "section": "",
    "text": "GIS 도구 진화과정",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#gis-생태계",
    "href": "map.html#gis-생태계",
    "title": "\n21  지도 공간정보\n",
    "section": "\n21.2 GIS 생태계",
    "text": "21.2 GIS 생태계\n인간은 오랜 역사 동안 다양한 도구를 개발하고 사용해 왔으며, GIS에서도 도구는 단순한 계산부터 복잡한 데이터 분석, 시각화를 통한 의사결정지원, 최근 거대언어모형(LLM) 인공지능까지 다양한 형태로 기여하고 있다.\n현재 직면한 문제를 해결하는데 과거 도구를 사용하는 것은 마치 철기시대에 석기시대 도구를 사용하는 것과 다름이 없다. 따라서, 데이터를 통해 가치를 만드는 데이터 과학자 입장에서 도구의 선택은 매우 중요할 수 밖에 없다.\n\n\n\n\n\n그림 21.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\n\n\n현재 R 공간정보 생태계는 공간정보 도구 진화과정을 몇차례 경험한 후에 그림 21.2 처럼 자리를 잡았다. 결론부터 들어가면 sf 패키지가 생태계의 중심으로 자리 잡았으며, 사용자와 개발자는 sf 패키지를 통해 상당수 공간정보 데이터 문제를 해결할 수 있게 되었다. 하지만, sf 패키지는 C/C++ 라이브러리에 크게 의존성하기 때문에 각 라이브러리를 살펴보는 것은 향후 GIS 개발자와 분석가로 현업에서 활약하는데 큰 도움이 될 것으로 보인다.\nGDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2는 C/C++ 라이브러리를 개발, 유지보수, 지원하는 커뮤니티가 존재하는 반면 R, 파이썬, 줄리아(Julia), 자바스크립트는 대화형 인터페이스를 통해 라이브러리를 활용하는 커뮤니티도 존재한다. (Pebesma 와/과 Bivand 2023)\n\n\nGDAL (Geospatial Data Abstraction Library) 라이브러리는 공간 데이터 처리에 있어 멀티툴의 대명사인 스위스군 칼(SAK, Swiss Army Knife)이라는 별명을 갖고 있고 100개가 넘는 다른 라이브러리와 연결되어 다양한 공간 데이터를 불러오고, 처리하고, 내보내는 기능을 제공한다.\n\nPROJ는 지도 투영 및 데이터 변환을 위한 라이브러리로, 하나의 좌표계에서 다른 좌표계로 좌표를 변환할 때 유용하다.\n\nGEOS (Geometry Engine Open Source)와 s2geometry 라이브러리는 기하학 연산에 사용하며, 길이, 면적, 거리를 측정하거나 연산작업에 사용되며 \\(R^2\\)로 표기되며 GEOS는 평평한 2차원 공간에, \\(S^2\\)로 표기되며 s2geometry는 구형 공간에 사용한다.\n\nNetCDF는 파일 형식이며 C 라이브러리로, 어떤 차원 배열도 정의할 수 있으며 특히 (기후) 모형개발 커뮤니티에서 널리 사용된다. Udunits2는 측정 단위의 데이터베이스 및 소프트웨어 라이브러리로, 단위의 변환과 파생 단위, 사용자 정의 단위를 처리한다. liblwgeom은 PostGIS 구성 요소로, GDAL, GEOS에서 누락된 몇 가지 루틴을 포함한다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#사례",
    "href": "map.html#사례",
    "title": "\n21  지도 공간정보\n",
    "section": "\n21.3 사례",
    "text": "21.3 사례\nsf 패키지가 추상화된 함수를 제공하기 때문에 R 공간정보 생태계에서 GDAL 라이브러리를 GIS 분석가가 직접적으로 다룰 일은 없다. 하지만, GIS 도구를 활용하여 2023년 7월 기준 시도별 인구수를 대한민국 지도위에 도식화하기 위해서는 R 공간정보 생태계를 구성하는 다양한 도구가 꼭 필요하다. 먼저, 지도 데이터를 다루기 위해 sf 패키지를 사용하고 통계청(KOSIS) 데이터를 처리하기 위해 tidyverse 패키지 도구를 활용하고 오픈지도 개발자가 공개한 geojson 파일을 결합하여 인구수를 시도 수준에서 시도별 인구수 색상을 달리하여 지도위에 시각화한다. 작성된 코드는 R로 작성되었지만 파이썬 진영에도 공간정보 생태계도 유사한 도구가 준비되어 있어 각자 사용하기 좋은 도구를 가지고 의미있는 결과물을 만들어내고 있다.\n\nlibrary(sf)\nlibrary(tidyverse)\nsf_use_s2(FALSE)\n\n## 지도\nkorea_map &lt;- read_sf(\"data/HangJeongDong_ver20230401.geojson\")\n\nsido_map &lt;- korea_map |&gt; \n  group_by(sidonm) |&gt; \n  summarise(geometry = sf::st_union(geometry))\n\n## 23년 7월 인구수(KOSIS) 행정구역별, 성별 인구수\npop_tbl &lt;- read_csv(\"data/행정구역_시군구_별__성별_인구수_20230831223248.csv\",\n         locale=locale(encoding=\"euc-kr\"), skip = 1) |&gt; \n  set_names(c(\"sidonm\", \"인구수\")) |&gt; \n  mutate(sidonm = if_else(sidonm == \"강원특별자치도\", \"강원도\", sidonm))\n\nsigo_gg &lt;- sido_map |&gt; \n  left_join(pop_tbl) |&gt; \n  ggplot() +\n    geom_sf(aes(geometry = geometry, fill = cut(인구수, 10)), show.legend = FALSE) +\n    ggrepel::geom_label_repel(aes(label = sidonm, geometry = geometry), \n                              size = 3, stat = \"sf_coordinates\") +\n    theme_void() +\n    scale_fill_brewer(palette = \"OrRd\")\n\nragg::agg_jpeg(\"images/GIS_tools.jpeg\",\n               width = 10, height = 7, units = \"in\", res = 600)\nsigo_gg\ndev.off()\n\n\n\nGIS 도구 활용 대한민국 시도별 인구수",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#마무리",
    "href": "map.html#마무리",
    "title": "\n21  지도 공간정보\n",
    "section": "\n21.4 마무리",
    "text": "21.4 마무리\n공간정보 개발자와 사용자가 개발하는 코드는 기계보다 사람 친화적으로 바뀌었으며, 효과적인 디버그와 신속한 개발을 위한 파이프 철학도 도입되어 생산성 향상이 비약적으로 높아졌고 도구의 추상화 수준도 대폭 향상되었다. IoT와 인공위성을 통해 엄청난 공간정보 데이터가 축적되고 있지만, GIS 도구가 꾸준히 발전하면서 이제 누구나 이러한 도구를 활용하여 공간정보 데이터를 통해 의미있는 산출물을 제작하고 도구도 개발할 수 있게 되었고, 다른 한편으로는 도구 없이 도구에 대한 이해없이 프롭테크를 논하는 것조차 의미없는 시대로 접어들고 있다.\n\n\n\n\nPebesma, Edzer, 와/과 Roger Bivand. 2023. Spatial Data Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, 와/과 Markus Neteler. 2016. “OpenEO: a GDAL for Earth Observation Analytics”. 2016년. https://r-spatial.org/2016/11/29/openeo.html.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법”. 프롭빅스(PROPBIX), 호 13 (9월). http://www.kahps.org/.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "langchain.html",
    "href": "langchain.html",
    "title": "\n23  랭체인\n",
    "section": "",
    "text": "23.1 허깅페이스\n파이썬과 R을 사용해 Hugging Face Hub의 대형 언어 모델(Large Language Model, LLM)을 활용한다. 파이썬에서는 필요한 라이브러리를 설치하고, R에서는 reticulate 라이브러리를 통해 파이썬 환경을 사용한다. 파이썬 코드에서 Hugging Face Hub에 접근하기 위한 API 토큰을 로드하고, HuggingFaceHub 클래스를 사용하여 특정 모델(‘tiiuae/falcon-7b-instruct’)에 질문을 하고, 모델의 답변을 출력한다.\nlibrary(reticulate)\n\nuse_condaenv(\"langchain\", required = TRUE)",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>랭체인</span>"
    ]
  },
  {
    "objectID": "langchain.html#허깅페이스",
    "href": "langchain.html#허깅페이스",
    "title": "\n23  랭체인\n",
    "section": "",
    "text": "pip install langchain_community, pip install dotenv, pip install huggingface_hub: 이 세 명령어는 파이썬 환경에서 필요한 패키지들을 설치한다. langchain_community는 언어 체인 커뮤니티 라이브러리, dotenv는 환경 변수를 관리하는 라이브러리, huggingface_hub는 Hugging Face Hub와 연동하는 데 사용되는 라이브러리다.\nR 코드 부분에서 library(reticulate)를 사용해 파이썬과 R 사이의 상호작용을 가능하게 하는 reticulate 라이브러리를 로드한다. use_condaenv(\"langchain\", required = TRUE)는 langchain이라는 이름의 Conda 환경을 사용하도록 지시한다. 이는 파이썬 코드를 R 환경에서 실행하기 위한 준비 단계다.\n파이썬 코드에서는 먼저 langchain_community.llms에서 HuggingFaceHub 클래스를, dotenv에서 load_dotenv 함수를 가져온다. 이후 os 모듈을 임포트한다. load_dotenv()를 호출하여 환경 변수를 로드한다. 이는 .env 파일에 저장된 환경 변수를 사용할 수 있게 한다.\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')는 환경 변수에서 ’HF_TOKEN’을 찾아 해당 토큰을 변수에 저장한다. 이 토큰은 Hugging Face Hub에 접근할 때 인증을 위해 사용된다.\nHuggingFaceHub 클래스의 인스턴스를 생성한다. 이 때 repo_id에는 사용할 Hugging Face 모델의 저장소 ID를, huggingfacehub_api_token에는 위에서 얻은 API 토큰을 넣는다.\n대형 언어 모델에 질문을 하기 위해 question 변수에 질문을 저장하고, llm.invoke(question)을 호출하여 모델에 질문을 전달하고 결과를 받는다.\n마지막으로 print(output)을 통해 얻은 결과를 출력한다. 이 코드는 Hugging Face Hub의 특정 모델을 사용하여 질문에 대한 답변을 얻는 과정을 보여준다.\n\n\n\npip install langchain_community\npip install dotenv\npip install huggingface_hub\n\n\nfrom langchain_community.llms import HuggingFaceHub\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')\n\nllm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', \n                     huggingfacehub_api_token = huggingfacehub_api_token)\n\nquestion = 'what is an Large Language Model in artificial intelligence?'\noutput = llm.invoke(question)\n\nprint(output)",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>랭체인</span>"
    ]
  },
  {
    "objectID": "code_interpreter.html",
    "href": "code_interpreter.html",
    "title": "24  데이터 사이언스",
    "section": "",
    "text": "24.1 프롬프트",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스</span>"
    ]
  },
  {
    "objectID": "code_interpreter.html#프롬프트",
    "href": "code_interpreter.html#프롬프트",
    "title": "24  데이터 사이언스",
    "section": "",
    "text": "데이터 전처리EDA통계모형기계학습성능 최적화시각화분석도구대쉬보드파이프라인모듈 개발\n\n\n\nPrompt: What are the best practices for preprocessing {topic} data using {programming_language_or_framework}?\n\n\nData Cleaning: Identify and address missing values, outliers, and duplicate records. Cleaning your data ensures that you’re working with accurate and reliable information.\nData Transformation: Normalize or standardize numerical features, encode categorical variables, and create new features if necessary. Data transformation enhances the quality of your dataset.\nFeature Engineering: Extract meaningful information from your data to improve the performance of machine learning models. Feature engineering involves creating new features or modifying existing ones to make them more informative.\nData Validation: Ensure data consistency and integrity by performing validation checks. Validate that your data adheres to predefined rules and constraints.\n\n\n\n\nPrompt: How can I perform exploratory data analysis on {topic} data using {programming_language_or_framework}?\n\n\nData Visualization: Create informative plots, charts, histograms, and scatterplots to visualize data distributions, relationships, and trends. Visualization helps in gaining initial insights into the data.\nStatistical Analysis: Compute summary statistics, such as mean, median, and standard deviation, to describe the central tendencies and variability of your data. Statistical tests can reveal relationships and dependencies.\nHypothesis Testing: Formulate hypotheses about your data and conduct statistical tests to validate or reject these hypotheses. Hypothesis testing is useful for making data-driven decisions.\nInteractive Exploration: Leverage libraries and tools available in {programming_language_or_framework} to perform interactive exploration. Interactive visualization and widgets allow for dynamic exploration of the data.\n\n\n\n\nPrompt: What are the most common statistical techniques to analyze {topic} data in {programming_language_or_framework}?\n\n\nRegression Analysis: Use regression techniques to model relationships between variables and make predictions. Linear regression, logistic regression, and polynomial regression are common types.\nClustering: Apply clustering algorithms, such as K-means or hierarchical clustering, to group similar data points together. Clustering helps in segmentation and pattern recognition.\nClassification: Perform classification tasks to categorize data into predefined classes or labels. Decision trees, support vector machines, and neural networks are frequently used for classification.\nTime Series Analysis: Analyze data with temporal components using time series analysis. This technique is essential for understanding trends and patterns over time.\n\n\n\n\nPrompt: Provide a step-by-step guide for implementing a machine learning model for {specific_task} using {programming_language_or_framework}.\n\n\nData Preparation: Begin by preprocessing and cleaning your data, ensuring that it’s in the right format for modeling.\nFeature Selection: Identify and select the most relevant features or variables for your model. Feature selection helps improve model performance and reduce complexity.\nModel Selection: Choose an appropriate machine learning algorithm or model for your task. Consider factors like data size, complexity, and interpretability.\nTraining and Evaluation: Train your chosen model on a portion of your data and evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\nDeployment: If the model performs satisfactorily, deploy it in your application or workflow for making predictions.\n\n\n\n\nPrompt: Explain how to optimize {topic} data analysis performance in {programming_language_or_framework} using best coding practices.\n\n\nVectorization: Take advantage of vectorized operations to perform calculations on entire arrays or datasets, which can significantly speed up computations.\nMemory Management: Efficiently manage memory resources, such as by releasing unnecessary objects or using data structures that minimize memory usage.\nParallel Processing: Utilize parallel computing techniques to distribute tasks across multiple cores or processors, thereby accelerating data processing.\nProfiling and Testing: Regularly profile your code to identify performance bottlenecks and optimize critical sections. Thoroughly test your code to ensure correctness and reliability.\n\n\n\n\nPrompt: Discuss the pros and cons of different data visualization techniques for {topic} data analysis in {programming_language_or_framework}.\n\n\nBar Charts and Histograms: These are effective for showing data distributions and comparing categories. They are excellent for visualizing frequency and count data.\nScatterplots: Ideal for displaying relationships between two continuous variables. Scatterplots help identify correlations and trends in data.\nHeatmaps: Useful for visualizing large datasets and identifying patterns in multidimensional data. They are especially valuable for displaying correlation matrices.\nInteractive Dashboards: Create user-friendly interactive dashboards that allow users to explore and interact with data. Dashboards can provide real-time insights and support decision-making.\n\n\n\n\nPrompt: Describe the process of building a custom data analysis tool for {topic} using {programming_language_or_framework}, including the necessary features and functionalities.\n\n\nData Import: Allow users to import data from various sources, such as CSV files, databases, or APIs.\nData Processing: Include preprocessing and transformation capabilities, enabling users to clean, filter, and manipulate data easily.\nVisualization: Incorporate interactive visualization components that help users explore and understand the data visually.\nExport and Reporting: Provide options for exporting analysis results, generating reports, and sharing findings with stakeholders.\n\n\n\n\nPrompt: Explain how to develop a user-friendly dashboard for visualizing and interacting with {topic} data analysis results using {programming_language_or_framework}.\n\n\nIntuitive Design: Create a visually appealing and intuitive design that makes it easy for users to navigate and understand the dashboard.\nInteractive Elements: Incorporate interactive elements, such as filters, sliders, and dropdowns, that allow users to customize their data views.\nReal-Time Updates: Enable real-time updates of data and visualizations to provide users with the latest information.\nAccessibility: Ensure that the dashboard is accessible to all users, including those with disabilities, by following accessibility guidelines.\n\n\n\n\nPrompt: Provide a step-by-step guide for creating a reusable data analysis pipeline for {topic} using {programming_language_or_framework}, covering data preprocessing, analysis, and visualization.\n\n\nData Ingestion: Develop a module for loading data from various sources, including files, databases, and APIs.\nPreprocessing: Create a preprocessing module that encompasses data cleaning, transformation, and feature engineering steps.\nAnalysis: Develop analysis modules that encapsulate statistical analyses, machine learning models, and hypothesis tests.\nVisualization: Implement visualization modules that generate informative charts and graphs for insights.\n\n\n\n\nPrompt: Discuss the key considerations when designing a scalable and modular data analysis tool for {topic} in {programming_language_or_framework}, including performance optimization and extensibility.\n\n\nPerformance Optimization: Optimize your code and algorithms for scalability to ensure that the tool can handle large datasets efficiently.\nModular Architecture: Design the tool with a modular architecture, making it easier to add new features, update existing ones, and maintain the codebase.\nExtensibility: Allow for the easy integration of additional data sources, analysis methods, and visualization techniques to accommodate changing needs.\nUser Collaboration: Implement features that enable collaboration among multiple users, such as data sharing, version control, and user permissions.",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스</span>"
    ]
  },
  {
    "objectID": "code_interpreter.html#llm-데이터-분석",
    "href": "code_interpreter.html#llm-데이터-분석",
    "title": "24  데이터 사이언스",
    "section": "24.2 LLM 데이터 분석",
    "text": "24.2 LLM 데이터 분석\n대형 언어 모델(LLMs)과 이미지 생성 모델(IGMs)을 기반으로 파이프라인을 사용하여 데이터 시각화 산출물 생성을 제시한 연구(Dibia 2023) 로 LIDA라는 새로운 도구를 통해 문법에 구애받지 않고 자연어를 통해 시각화 및 인포그래픽을 생성한다. LIDA는 데이터를 자연어 요약으로 변환하는 SUMMARIZER, 데이터를 기반으로 시각화 목표를 나열하는 GOAL EXPLORER, 시각화 코드를 생성하고 정제하며 실행하고 필터링하는 VISGENERATOR, IGM을 사용해 데이터 충실한 스타일화된 그래픽을 생성하는 INFOGRAPHER로 구성된다.\n\n\n\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models”. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 편집자： Danushka Bollegala, Ruihong Huang, 와/과 Alan Ritter, 113–26. Toronto, Canada: Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10 ChatGPT Prompts You Should Start Using Today”. medium.com, 12월. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스</span>"
    ]
  },
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n25  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n26 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "26  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "**7부** 챗GPT",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFisher, Ronald Aylmer. 1970. “Statistical Methods for Research\nWorkers.” In Breakthroughs in Statistics: Methodology and\nDistribution, 66–70. Springer.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric\nElguero, Didier Fontenille, François Renaud, Carlo Costantini, and\nFrédéric Thomas. 2010. “Beer Consumption Increases Human\nAttractiveness to Malaria Mosquitoes.” PloS One 5 (3):\ne9546.\n\n\nLindquist, Everet Franklin. 1940. “Statistical Analysis in\nEducational Research.”\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#책의-구성",
    "href": "index.html#책의-구성",
    "title": "챗GPT 데이터 과학",
    "section": "책의 구성",
    "text": "책의 구성",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 데이터 과학",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 4월 속초 영금정\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  들어가며",
    "section": "",
    "text": "graph LR\n    subgraph 이해하기\n        변환 --&gt; 시각화\n        시각화 --&gt; 모형\n        모형 --&gt; 변환\n    end\n    가져오기 --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n    \n    classDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\n    classDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\n    classDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n    \n    class 깔끔화,변환,모형,시각화,의사소통 modern\n    class 가져오기 emphasize\n    class 이해하기 subgraphStyle\n\n\n\n\n그림 1.1: 데이터 과학 작업흐름도",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>들어가며</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "4  텍스트 파일",
    "section": "",
    "text": "4.1 데이터 종류\ngraph TB\n    subgraph 가져오기[\"가져오기(Import)\"]\n        csv[CSV 파일] --&gt; 핸들러\n        스프레드쉬트 --&gt; 핸들러\n        데이터베이스 --&gt; 핸들러\n        웹 --&gt; 핸들러\n\n        핸들러 --&gt; 데이터프레임\n    end\n    \n    classDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\n    classDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\n    classDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n    \n    class csv,스프레드쉬트,데이터베이스,웹,핸들러 modern\n    class 데이터프레임 emphasize\n    class 가져오기 subgraphStyle\n\n\n\n\n그림 4.2: 다양한 데이터 종류",
    "crumbs": [
      "**2부** 가져오기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>텍스트 파일</span>"
    ]
  },
  {
    "objectID": "penguins.html",
    "href": "penguins.html",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "4 펭귄 데이터셋",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#펭귄-데이터-출현",
    "href": "penguins.html#펭귄-데이터-출현",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.1 펭귄 데이터 출현",
    "text": "4.1 펭귄 데이터 출현\n미국에서 “George Floyd”가 경찰에 의해 살해되면서 촉발된 “Black Lives Matter” 운동은 아프리카계 미국인을 향한 폭력과 제도적 인종주의에 반대하는 사회운동이다. 한국에서도 소수 정당인 정의당에서 여당 의원 176명 중 누가?…차별금지법 발의할 ’의인’을 구합니다로 기사로 낼 정도로 적극적으로 나서고 있다.\n데이터 과학에서 최근 R.A. Fisher의 과거 저술한 “The genetical theory of natural selection” (fisher1958genetical?) 우생학(Eugenics) 대한 관점이 논란이 되면서 R 데이터 과학의 첫 데이터셋으로 붓꽃 iris 데이터를 다른 데이터, 즉 펭귄 데이터로 대체하는 움직임이 활발히 전개되고 있다. palmerpenguins (penguin2020?) 데이터셋이 대안으로 많은 호응을 얻고 있다. (Levy2019?)",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#penguins-study",
    "href": "penguins.html#penguins-study",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.2 펭귄 공부",
    "text": "4.2 펭귄 공부\n팔머(Palmer) 펭귄은 3종이 있으며 자세한 내용은 다음 나무위키를 참조한다. 1\n\n\n젠투 펭귄(Gentoo Penguin): 머리에 모자처럼 둘러져 있는 하얀 털 때문에 알아보기가 쉽다. 암컷이 회색이 뒤에, 흰색이 앞에 있다. 펭귄들 중에 가장 빠른 시속 36km의 수영 실력을 자랑하며, 짝짓기 할 준비가 된 펭귄은 75-90cm까지도 자란다.\n\n아델리 펭귄(Adelie Penguin): 프랑스 탐험가인 뒤몽 뒤르빌(Dumont D’Urville) 부인의 이름을 따서 ’아델리’라 불리게 되었다. 각진 머리와 작은 부리 때문에 알아보기 쉽고, 다른 펭귄들과 마찬가지로 암수가 비슷하게 생겼지만 암컷이 조금 더 작다.\n\n턱끈 펭귄(Chinstrap Penguin): 언뜻 보면 아델리 펭귄과 매우 비슷하지만, 몸집이 조금 더 작고, 목에서 머리 쪽으로 이어지는 검은 털이 눈에 띈다. 어린 고삐 펭귄들은 회갈색 빛을 띄는 털을 가지고 있으며, 목 아래 부분은 더 하얗다. 무리를 지어 살아가며 일부일처제를 지키기 때문에 짝짓기 이후에도 부부로써 오랫동안 함께 살아간다.\n\n\n\n팔머 펭귄 3종 세트\n\n다음으로 iris 데이터와 마찬가지로 펭귄 3종을 구분하기 위한 변수로 조류의 부리에 있는 중앙 세로선의 융기를 지칭하는 능선(culmen) 길이(culmen length)와 깊이(culmen depth)를 이해하면 된다.\n\n\n팔머 펭귄 능선 변수",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#penguin-home",
    "href": "penguins.html#penguin-home",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.3 펭귄 서식지",
    "text": "4.3 펭귄 서식지\nleaflet 팩키지로 펭귄 서식지를 남극에서 특정한다. geocoding을 해야 하는데 구글에서 위치 정보를 구글링하면 https://latitude.to/에서 직접 위경도를 반환하여 준다. 이 정보를 근거로 하여 펭귄 서식지를 시각화한다.\n\n\n\n\n\n\n\n\n파머 연구소와 펭귄 서식지\n\n\n\n\n\n펭귄 3종\n\n\n\n\n\n\n\n아델리, 젠투, 턱끈 펭귄이 함께한 사진\n\n\n\n\n\n토르거센 섬에서 새끼를 키우는 아델리 펭귄\n\n\n\n\n\n비스코 지점 젠투 펭귄 서식지\n\n\n\n\n\n펭귄과 함께 현장에서 일하는 크리스틴 고먼 박사\n\n\n\n\n\n파머 펭귄 데이터셋\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(palmerpenguins)\n# library(tidygeocoder)\n\npenguins %&gt;% \n  count(island)\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\nisland_df &lt;- tribble(~\"address\", ~\"lat\", ~\"lng\",\n                     \"Torgersen Island antarctica\", -64.772819, -64.074325,\n                     \"Dream Island antarctica\", -64.725558, -64.225562,\n                     \"Biscoe Island antarctica\", -64.811565, -63.777947,\n                     \"Palmer Station\", -64.774312, -64.054213)\n\nisland_df %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addMarkers(lng=~lng, lat=~lat, \n                   popup = ~ as.character(paste0(\"&lt;strong&gt;\", paste0(\"명칭:\",`address`), \"&lt;/strong&gt;&lt;br&gt;\",\n                                                 \"-----------------------------------------------------------&lt;br&gt;\",\n                                                 \"&middot; latitude: \", `lat`, \"&lt;br&gt;\",\n                                                 \"&middot; longitude: \", `lng`, \"&lt;br&gt;\"\n                   )))",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#데이터-설치",
    "href": "penguins.html#데이터-설치",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.4 데이터 설치",
    "text": "4.4 데이터 설치\nremotes 팩키지 install_github() 함수로 펭귄 데이터를 설치한다.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"allisonhorst/palmerpenguins\")\n\ntidyverse 팩키지 glimpse() 함수로 펭귄 데이터를 일별한다.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#penguin-EDA-skimr",
    "href": "penguins.html#penguin-EDA-skimr",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.5 자료구조 일별",
    "text": "4.5 자료구조 일별\nskimr 팩키지를 사용해서 penguins 데이터프레임 자료구조를 일별한다. 이를 통해서 344개 펭귄 관측값이 있으며, 7개 칼럼으로 구성된 것을 확인할 수 있다. 또한, 범주형 변수가 3개, 숫자형 변수가 4개로 구성되어 있다. 그외 더 자세한 사항은 범주형, 숫자형 변수에 대한 요약 통계량을 참조한다.\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n데이터가 크지 않아 DT 팩키지를 통해 데이터 전반적인 내용을 살펴볼 수 있다.\n\npenguins %&gt;% \n  reactable::reactable()",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#penguin-EDA",
    "href": "penguins.html#penguin-EDA",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.6 탐색적 데이터 분석",
    "text": "4.6 탐색적 데이터 분석\npalmerpenguins 데이터셋 소개에 포함되어 있는 미국 팔머 연구소 (palmer station) 펭귄 물갈퀴(flipper) 길이와 체질량(body mass) 산점도를 그려보자.\n\nlibrary(tidyverse)\nlibrary(extrafont)\nloadfonts()\n\nmass_flipper &lt;- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  theme_minimal(base_family = \"NanumGothic\") +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"펭귄 크기\",\n       subtitle = \"남극 펭귄 3종 물갈퀴 길이와 체질량 관계\",\n       x = \"물갈퀴 길이 (mm)\",\n       y = \"체질량 (g)\",\n       color = \"펭귄 3종\",\n       shape = \"펭귄 3종\") +\n  theme(legend.position = c(0.2, 0.7),\n        legend.background = element_rect(fill = \"white\", color = NA),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\")\n\nmass_flipper",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#footnotes",
    "href": "penguins.html#footnotes",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "신발끈 여행사, 관광안내자료↩︎",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "penguins.html#펭귄-데이터-저장",
    "href": "penguins.html#펭귄-데이터-저장",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.7 펭귄 데이터 저장",
    "text": "4.7 펭귄 데이터 저장\n\n4.7.1 .csv 파일\n\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  drop_na() |&gt; \n  write_csv(\"data/penguins.csv\")\n\n\n4.7.2 .xlsx 엑셀 파일\n\nlibrary(writexl)\n\npenguins |&gt; \n  drop_na() |&gt; \n  write_xlsx(\"data/penguins.xlsx\")\n\n\n4.7.3 .sqlite 데이터베이스\n\nlibrary(DBI)\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), dbname = \"data/penguins.sqlite\")\n\n# 데이터프레임을 SQLite 테이블로 저장합니다. 'my_table'이라는 이름으로 저장됩니다.\n# 데이터베이스에 같은 이름의 테이블이 이미 존재한다면, append, overwrite 또는 fail 중 하나를 선택할 수 있습니다.\ndbWriteTable(con, \"penguin\", penguins |&gt; drop_na() , overwrite = TRUE)\n\n# 연결을 닫습니다.\ndbDisconnect(con)\n\n\n4.7.4 pins\n\n\nlibrary(pins)\n\nboard &lt;- board_folder(\"C:/Users/statkclee/OneDrive/pins\") \n\nmetadata &lt;- list(owner       = \"한국 R 사용자회\",\n                 deptartment = \"R&D\",\n                 URL         = \"https://r2bit.com\")\n\nboard  |&gt; pin_write(penguins |&gt; drop_na(), \n                    name        = \"penguins\",\n                    title       = \"펭귄 데이터셋\",\n                    description = \"남극 파머 연구소 서식 펭귄 데이터셋\",\n                    metadata    = metadata)\n\n\n4.7.5 구글시트\n\nlibrary(googlesheets4)\nlibrary(googledrive)\n\ngoogledrive::drive_auth()\n\ngs4_create(\n  name = \"penguins\",\n  sheets = list(\"penguins\" = penguins |&gt; drop_na())\n)\n\n#&gt; ✔ Creating new Sheet: penguins.\n#&gt; Waiting for authentication in browser...\n#&gt; Press Esc/Ctrl + C to abort\n#&gt; Authentication complete.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "data_structure.html",
    "href": "data_structure.html",
    "title": "4  자료구조",
    "section": "",
    "text": "4.1 자료형\n2.1 더하기 \"black\"이 말이 되지 않기 때문에 마지막 문장이 오류를 뱉어낼 것이라고 추측한다면, 제대로 이해하고 있는 것이다. 이미 자료형(data type)으로 불리는 프로그래밍의 중요한 개념에 대한 직관을 갖추고 있는 것이다. 데이터 자료형이 무엇인지 다음을 통해 물어보게 된다:\n다섯 가지 주요 자료형이 있다. 실수형(double), 정수형(integer), 복소수형(complex), 논리형(logical), 문자형(character).\n분석이 얼마나 복잡해지냐에 관계없이, R에서 모든 데이터는 이러한 기본 자료형 중 하나로 해석된다. 이러한 엄격함이 정말로 중요한 결과를 잉태하게 된다.\n사용자가 또다른 고양이에 대한 상세내용을 추가했고, 추가 정보는 data/feline-data_v2.csv 파일에 저장되어 있다.\n# file.show(\"data/feline-data-v2.csv\")\nfile.show(\"feline-data-v2.csv\")\n앞서와 마찬가지로 새로운 고양이 데이터를 불러와서 weight 칼럼에 데이터 자료형이 무엇인지 확인한다.\n이러면 안되는데, 고양이 몸무게가 더이상 실수형 자료형이 아니다! 앞서 수행했던 동일한 수학연산을 취하게 되면 문제에 봉착한다.\n무슨 일이 일어난 걸까? R이 csv 파일을 불러올 때, 칼럼에 모든 것이 동일한 자료형이 되는 것을 요구한다; 칼럼에 모든 원소가 실수형으로 인식되지 않으면, 칼럼에 어떤 원소도 실수형이 될 수 없다. 고양이 데이터를 불러온 테이블을 데이터프레임(data.frame)이라고 부르고, 자료구조(data structure)로 불리는 첫번째 사례가 된다. 즉, 자료구조는 기본 자료형에서 R이 생성할 줄 아는 구조가 된다.\nclass 함수를 호출해서 데이터프레임인지를 알 수 있다.\nR에서 데이터를 성공적으로 사용하려면 기본 자료구조가 무엇이고 어떻게 동작하는지 이해할 필요가 있다. 지금으로서는 추가된 마지막 줄을 제거하고 좀더 살펴보도록 하자:\nfeline-data.csv:\nRStudio로 다시 돌아와서,",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#자료형data-types",
    "href": "data_structure.html#자료형data-types",
    "title": "\n5  자료구조\n",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\ntabby,2.3 or 2.4,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#벡터와-자료형-강제변환type-coercion",
    "href": "data_structure.html#벡터와-자료형-강제변환type-coercion",
    "title": "\n5  자료구조\n",
    "section": "\n5.2 벡터와 자료형 강제변환(Type Coercion)",
    "text": "5.2 벡터와 자료형 강제변환(Type Coercion)\n강제변환을 보다 잘 이해하기 위해서, 또 다른 자료구조(벡터(vector))를 만나보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR에서 벡터는 본질적으로 무언가 순서를 갖는 리스트로, 특별한 성질을 갖는데 벡터에 모든 것은 동일한 자료형을 갖는다는 점이다. 자료형을 지정하지 않게 되면, 기본디폴트 설정은 논리형(logical)이 되거나; 자료형에 관계없이 공벡터를 선언할 수 있다. 자료가 벡터인지 다음과 같이 확인한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 명령어로부터 다소 암호스러운 출력결과가 나오는데 해당 벡터에서 발견된 기본 자료형은 이 경우 chr 문자; 벡터에 나와있는 숫자는 벡터의 인덱스로 이 경우 [1:3]; 그리고 벡터에 실제로 들어있는 몇가지 예로, 이 경우 빈 문자열이 된다. 유사하게 다음을 실행하게 되면,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncats$weight도 벡터임을 알 수 있다. R 데이터프레임으로 불러온 데이터 칼럼은 모두 벡터다. 이러한 연유로 칼럼에 있는 모든 원소는 동일한 자료형을 갖게 강제하는 이유가 된다.\n\n\n\n\n\n\n토론 주제\n\n\n\n왜 R에서는 데이터 칼럼에 무엇을 넣는지에 대해서 고집스럽게 주장을 할까? 이러한 점은 어떻게 우리에게 도움이 될까?\n\n\n\n\n\n\n토론 정리\n\n\n\n\n\n칼럼에 모든 것을 동일하게 둠으로써, 데이터에 관해서 단순한 가정을 할 수 있게 한다; 만약 칼럼의 첫번째 입력값이 숫자라면, 모든 입력값을 숫자로 해석할 수 있게 되고, 그렇게 함으로써 모든 것을 확인할 필요가 없게 된다. 깨끗한 데이터(clean data)라고 사람들이 회자할 때, 사람들이 의미하는 것이 이러한 일관성이다. 장기적으로 엄격한 일관성이 R에서 우리 삶을 풍요롭고 수월하게 만들 것이다.\n\n\n\n\n\n결합 함수(c())를 사용해서 벡터를 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n지금까지 학습한 것을 바탕으로, 다음 문장은 어떤 결과를 출력하게 될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이것을 자료형 강제변환(type coercion)라고 부른다. 이것이 많은 놀라움의 원천이고, 왜 기본 자료형에 대해서 인지하고 있어야 되는 이유가 되고, R이 해석하는 방식도 알아야 된다. R에서 혼합된 자료형(상기 예제는 숫자와 문자)의 경우 단하나의 벡터로 변환시킬 때, 모든 자료를 동일한 자료형으로 강제 변환시킨다. 다음을 생각해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n강제 변환규칙은 다음과 같이 적용된다: 논리형 -&gt; 정수형 -&gt; 숫자형 -&gt; 복소수형 -&gt; 문자형. 여기서 -&gt; 표현은 다음으로 변환된다로 읽힌다. 이런 자동 변환규칙에 거슬러 자료형을 강제로 변환시키려면 as. 함수를 사용한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR이 기본 자료형을 다른 자료형으로 강제 변환할 때, 놀라운 일이 생겨난다! 자료형 강제변환에 대한 핵심사항은 차치하고 중요한 점은 다음과 같다. 만약 내 데이터가 생각했던 것과 다르게 보인다면, 자료형 강제변환이 원인일 가능성이 높다. 벡터와 데이터프레임의 칼럼 자료형이 동일하도록 확실히 하라. 그렇지 않으면 끔찍한 놀라운 경험을 하게 될 것이다!\n하지만, 경우에 따라서는 자료형 강제변환이 매우 유용할 수도 있다! 예를 들어, cats 데이터프레임 likes_string 칼럼은 숫자형이지만, 1과, 0이 실제로 TRUE와 FALSE를 표현한다는 것을 알고 있다. 이 경우 두 상태(TRUE 혹은 FALSE)를 갖는 논리형 자료형을 사용해야 한다. as.logical 함수를 사용해서 칼럼을 논리형(logical)으로 ‘강제변환(coerce)’ 시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n결합 함수(c())는 기존 벡터에 무언가 추가하는 역할을 수행한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n숫자 순열도 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n벡터에 관해 궁금한 점도 물어볼 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n마지막으로, 벡터의 각 원소에 명칭을 부여하는 것도 가능하다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n1 부터 26까지 숫자를 갖는 벡터를 생성하면서 시작해 보자. 생성한 벡터에 2를 곱해서 다시 자신에게 할당한다. 벡터에 A 부터 Z까지 이름을 부여한다. (힌트: LETTERS라는 내장벡터가 있다.)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#데이터프레임data-frames",
    "href": "data_structure.html#데이터프레임data-frames",
    "title": "\n5  자료구조\n",
    "section": "\n5.3 데이터프레임(Data Frames)",
    "text": "5.3 데이터프레임(Data Frames)\n데이터프레임의 칼럼이 벡터라고 앞에서 언급했다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n말이 된다. 다음은 어떤가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#요인",
    "href": "data_structure.html#요인",
    "title": "4  자료구조",
    "section": "\n4.4 요인",
    "text": "4.4 요인\n또 다른 중요한 자료구조가 요인(factor)이다. 요인은 보통 문자 데이터처럼 생겼다. 하지만, 일반적으로 범주형 정보를 나타내는데 사용된다. 예를 들어, 연구중인 모든 고양이에 대한 색상을 문자열 벡터로 만들어보자:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n문자열 벡터를 요인형으로 바꾸면 다음과 같다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 R은 데이터에 3가지 가능한 범주가 있음을 파악하게 되었다. 하지만, 놀라운 것도 함께 수행했다; 문자열을 출력하는 대신에, 숫자가 대량으로 출도 되었다. R은 내부적으로 사람이 읽을 수 있는 범주를 숫자 인덱스로 치환시킨다. 이런 기능은 대다수 통계 계산에서 범주형 데이터를 숫자형으로 표현되는 기능을 활용하기 때문에 꼭 필요하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\ncats 데이터프레임에 요인형 칼럼이 있나요? 요인형 칼럼의 이름은 무엇인가요? ?read.csv 명령어를 사용해서 텍스트 칼럼을 요인형 대신에 문자형으로 그대로 유지시키는 방법을 찾아내세요; 그리고 나서 cat 데이터프레임의 요인이 실제로 문자벡터임을 확인하는 명령문을 작성하시오.\n\n\n\n\n\n\n해답\n\n\n\n\n\n해법으로 stringAsFactors 인자를 사용하면 된다.:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n또다른 해법은 colClasses 인자를 사용해서 칼럼을 좀더 면밀히 제어하는 것이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n주의: 도움말 파일이 이해하기 어렵다는 학생이 다수 있다; 도움말 파일을 이해하기 어렵다는 것이 일반적이라서, 확신하지는 못하더라도, 문맥에 기초하여 최대한 추측하도록 용기를 주도록 한다.\n\n\n\n\n\n모형 함수에서 기준 수준(baseline level)이 무엇인지 파악하는 것이 중요하다. 요인의 첫번째 범주로 가정하지만, 기본디폴트는 알파벳순으로 정해지게 되어 있다. 수준을 다음과 같이 지정해서 변경할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 경우 “control”를 1로, “case”를 2로 명시적으로 지정하도록 했다. 이러한 지정이 통계 모형 결과를 해석하는데 있어 매우 중요하다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#리스트-lists",
    "href": "data_structure.html#리스트-lists",
    "title": "\n5  자료구조\n",
    "section": "\n5.5 리스트 (Lists)",
    "text": "5.5 리스트 (Lists)\n데이터 과학자로서 알고 있어야 되는 또다른 자료구조가 리스트(list)다. 리스트는 다른 자료형과 비교하여 몇 가지 점에서 더 단순하다. 왜냐하면 원하는 무엇이든 넣을 수 있기 때문이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임에서 다소 놀라운 것을 이제 이해할 수 있다. 다음을 실행하게 되면 무슨 일이 발생될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임은 ‘내부적으로(under the hood)’ 리스트라는 것을 알 수 있다 - 이유는 데이터프레임이 실제로 벡터와 요인으로 구성된 리스트이기 때문이다. 벡터와 요인으로 뒤섞인 칼럼을 붙잡아 두려면, 데이터프레임은 모든 칼럼을 유사한 표에 담을 수 있는 벡터보다 더 유연할 필요가 있다. 다른 말로, data.frame은 모든 벡터가 동일한 길이를 갖는 특별한 리스트로 정의할 수 있다.\ncats 사례에서는 정수형, 숫자형, 논리형 변수로 구성된다. 이미 살펴봤듯이, 데이터프레임 각 칼럼은 벡터다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n각 행은 다른 변수의 관측점(observation)으로 그 자체로 데이터프레임이다. 따라서, 서로 다른 자료형을 갖는 원소로 구성된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n데이터프레임에서 변수와 관측점과 원소를 호출하는 미모하지만 다른 방식이 존재한다: - cats[1] - cats[[1]] - cats$coat - cats[\"coat\"] - cats[1, 1] - cats[, 1] - cats[1, ]\n상기 예제를 시도해보고, 각각이 반환하는 것을 설명해 본다. 힌트: typeof() 함수를 사용해서 각각의 경우에 반환되는 것을 꼼꼼히 살펴본다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임을 벡터로 구성된 리스트로 간주할 수 있다. 단일 꺾쇠 [1]은 리스트의 첫번째 원소를 리스트로 반환한다. 이번 경우, 데이터프레임의 첫번째 칼럼이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이중 꺾쇠 [[1]]은 리스트의 원소 내용물을 반환한다. 이번 경우, 리스트가 아닌 요인형 벡터로 첫번째 칼럼 내용물을 반환한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n명칭으로 항목을 끄집어내는데 $ 기호를 사용한다. _coat_가 데이터프레임의 첫번째 칼럼으로 요인형 벡터가 반환된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n[\"coat\"] 방식은 칼럼 인덱스를 명칭으로 바꾸고 동시에 꺾쇠를 사용한 경우다. 예제 1과 마찬가지로 반환되는 객체는 리스트가 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n단일 꺾쇠를 사용했는데 이번에는 행과 열의 좌표도 넣어 전달했다. 반환되는 객체는 첫번째 행, 첫번째 열에 교차하는 값이 된다. 반환되는 객체는 정수형이지만,요인형 벡터의 일부분이라 정수값과 연관된 라벨 “calico”도 함께 출력한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n앞선 예제와 마찬가지로 꺾쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 행좌표를 지정하지 않은 경우, R에서 결측값은 해당 칼럼 벡터의 모든 원소로 해석된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 한번, 꺽쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 칼럼 좌표가 지정되어 있지 않기 때문에, 첫번째 행의 모든 값을 포함하는 리스트가 반환된다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#행렬-matrices",
    "href": "data_structure.html#행렬-matrices",
    "title": "\n5  자료구조\n",
    "section": "\n5.6 행렬 (Matrices)",
    "text": "5.6 행렬 (Matrices)\n마지막으로 중요한 자료형이 행렬이다. 0으로 가득찬 행렬을 다음과 같이 선언한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다른 자료구조와 마찬가지로, 행렬에 질문을 다음과 같이 던질 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\nlength(matrix_example) 실행결과는 어떻게 나올까? 시도해보자. 생각한 것과 일치하는가? 왜 그런가/ 왜 그렇지 않는가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n행렬은 차원 속성이 추가된 벡터라서, length() 함수는 행렬의 전체 원소 갯수를 반환시킨다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n또다른 행렬을 만들어 보자. 이번에는 1:50 숫자를 담고 있는 칼럼이 5, 행이 10일 행렬이다. matrix() 함수로 칼럼기준으로 혹은 행기준으로 채울 수 있나요? 행과 열을 바꿔 변경할 수 있는 방법을 찾아보자. (힌트: matrix 도움말 문서를 참조한다!)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n이번 워크샵에서 다룬 각 섹션별 문자벡터를 포함하는 길이 2를 갖는 리스트를 생성하시오.\n\n자료형(Data types)\n자료구조(Data structures)\n\n지금까지 살펴본 자료형(data type)과 자료구조(data structure)를 명칭으로 갖는 문자 벡터를 채워넣는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n주목: 칠판이나 벽에 자료형과 자료구조를 모두 적어두는 것이 도움이 될 수 있다 - 워크샵 동안 참여자들에게 기본 자료형과 구조의 중요성을 상기할 수 있기 때문이다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n아래 행렬의 출력 결과를 생각해보자:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이 행렬을 작성하는 올바른 명령어는 다음 중 무엇일까? 직접 타이핑하기 전에 각 명령어를 살펴보고, 정답을 생각해보자. 다른 명령어는 어던 행렬을 만들어낼지도 생각해본다.\n\nmatrix(c(4, 1, 9, 5, 10, 7), nrow = 3)\nmatrix(c(4, 9, 10, 1, 5, 7), ncol = 2, byrow = TRUE)\nmatrix(c(4, 9, 10, 1, 5, 7), nrow = 2)\nmatrix(c(4, 1, 9, 5, 10, 7), ncol = 2, byrow = TRUE)\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#자료형",
    "href": "data_structure.html#자료형",
    "title": "4  자료구조",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\ntabby,2.3 or 2.4,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#벡터와-자료형-강제변환",
    "href": "data_structure.html#벡터와-자료형-강제변환",
    "title": "4  자료구조",
    "section": "\n4.2 벡터와 자료형 강제변환",
    "text": "4.2 벡터와 자료형 강제변환\n강제변환을 보다 잘 이해하기 위해서, 또 다른 자료구조(벡터(vector))를 만나보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR에서 벡터는 본질적으로 무언가 순서를 갖는 리스트로, 특별한 성질을 갖는데 벡터에 모든 것은 동일한 자료형을 갖는다는 점이다. 자료형을 지정하지 않게 되면, 기본디폴트 설정은 논리형(logical)이 되거나; 자료형에 관계없이 공벡터를 선언할 수 있다. 자료가 벡터인지 다음과 같이 확인한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 명령어로부터 다소 암호스러운 출력결과가 나오는데 해당 벡터에서 발견된 기본 자료형은 이 경우 chr 문자; 벡터에 나와있는 숫자는 벡터의 인덱스로 이 경우 [1:3]; 그리고 벡터에 실제로 들어있는 몇가지 예로, 이 경우 빈 문자열이 된다. 유사하게 다음을 실행하게 되면,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncats$weight도 벡터임을 알 수 있다. R 데이터프레임으로 불러온 데이터 칼럼은 모두 벡터다. 이러한 연유로 칼럼에 있는 모든 원소는 동일한 자료형을 갖게 강제하는 이유가 된다.\n\n\n\n\n\n\n토론 주제\n\n\n\n왜 R에서는 데이터 칼럼에 무엇을 넣는지에 대해서 고집스럽게 주장을 할까? 이러한 점은 어떻게 우리에게 도움이 될까?\n\n\n\n\n\n\n토론 정리\n\n\n\n\n\n칼럼에 모든 것을 동일하게 둠으로써, 데이터에 관해서 단순한 가정을 할 수 있게 한다; 만약 칼럼의 첫번째 입력값이 숫자라면, 모든 입력값을 숫자로 해석할 수 있게 되고, 그렇게 함으로써 모든 것을 확인할 필요가 없게 된다. 깨끗한 데이터(clean data)라고 사람들이 회자할 때, 사람들이 의미하는 것이 이러한 일관성이다. 장기적으로 엄격한 일관성이 R에서 우리 삶을 풍요롭고 수월하게 만들 것이다.\n\n\n\n\n\n결합 함수(c())를 사용해서 벡터를 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n지금까지 학습한 것을 바탕으로, 다음 문장은 어떤 결과를 출력하게 될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이것을 자료형 강제변환(type coercion)라고 부른다. 이것이 많은 놀라움의 원천이고, 왜 기본 자료형에 대해서 인지하고 있어야 되는 이유가 되고, R이 해석하는 방식도 알아야 된다. R에서 혼합된 자료형(상기 예제는 숫자와 문자)의 경우 단하나의 벡터로 변환시킬 때, 모든 자료를 동일한 자료형으로 강제 변환시킨다. 다음을 생각해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n강제 변환규칙은 다음과 같이 적용된다: 논리형 -&gt; 정수형 -&gt; 숫자형 -&gt; 복소수형 -&gt; 문자형. 여기서 -&gt; 표현은 다음으로 변환된다로 읽힌다. 이런 자동 변환규칙에 거슬러 자료형을 강제로 변환시키려면 as. 함수를 사용한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR이 기본 자료형을 다른 자료형으로 강제 변환할 때, 놀라운 일이 생겨난다! 자료형 강제변환에 대한 핵심사항은 차치하고 중요한 점은 다음과 같다. 만약 내 데이터가 생각했던 것과 다르게 보인다면, 자료형 강제변환이 원인일 가능성이 높다. 벡터와 데이터프레임의 칼럼 자료형이 동일하도록 확실히 하라. 그렇지 않으면 끔찍한 놀라운 경험을 하게 될 것이다!\n하지만, 경우에 따라서는 자료형 강제변환이 매우 유용할 수도 있다! 예를 들어, cats 데이터프레임 likes_string 칼럼은 숫자형이지만, 1과, 0이 실제로 TRUE와 FALSE를 표현한다는 것을 알고 있다. 이 경우 두 상태(TRUE 혹은 FALSE)를 갖는 논리형 자료형을 사용해야 한다. as.logical 함수를 사용해서 칼럼을 논리형(logical)으로 ‘강제변환(coerce)’ 시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n결합 함수(c())는 기존 벡터에 무언가 추가하는 역할을 수행한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n숫자 순열도 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n벡터에 관해 궁금한 점도 물어볼 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n마지막으로, 벡터의 각 원소에 명칭을 부여하는 것도 가능하다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n1 부터 26까지 숫자를 갖는 벡터를 생성하면서 시작해 보자. 생성한 벡터에 2를 곱해서 다시 자신에게 할당한다. 벡터에 A 부터 Z까지 이름을 부여한다. (힌트: LETTERS라는 내장벡터가 있다.)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#데이터프레임",
    "href": "data_structure.html#데이터프레임",
    "title": "4  자료구조",
    "section": "\n4.3 데이터프레임",
    "text": "4.3 데이터프레임\n데이터프레임의 칼럼이 벡터라고 앞에서 언급했다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n말이 된다. 다음은 어떤가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#리스트",
    "href": "data_structure.html#리스트",
    "title": "4  자료구조",
    "section": "\n4.5 리스트",
    "text": "4.5 리스트\n데이터 과학자로서 알고 있어야 되는 또다른 자료구조가 리스트(list)다. 리스트는 다른 자료형과 비교하여 몇 가지 점에서 더 단순하다. 왜냐하면 원하는 무엇이든 넣을 수 있기 때문이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임에서 다소 놀라운 것을 이제 이해할 수 있다. 다음을 실행하게 되면 무슨 일이 발생될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임은 ‘내부적으로(under the hood)’ 리스트라는 것을 알 수 있다 - 이유는 데이터프레임이 실제로 벡터와 요인으로 구성된 리스트이기 때문이다. 벡터와 요인으로 뒤섞인 칼럼을 붙잡아 두려면, 데이터프레임은 모든 칼럼을 유사한 표에 담을 수 있는 벡터보다 더 유연할 필요가 있다. 다른 말로, data.frame은 모든 벡터가 동일한 길이를 갖는 특별한 리스트로 정의할 수 있다.\ncats 사례에서는 정수형, 숫자형, 논리형 변수로 구성된다. 이미 살펴봤듯이, 데이터프레임 각 칼럼은 벡터다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n각 행은 다른 변수의 관측점(observation)으로 그 자체로 데이터프레임이다. 따라서, 서로 다른 자료형을 갖는 원소로 구성된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n데이터프레임에서 변수와 관측점과 원소를 호출하는 미모하지만 다른 방식이 존재한다: - cats[1] - cats[[1]] - cats$coat - cats[\"coat\"] - cats[1, 1] - cats[, 1] - cats[1, ]\n상기 예제를 시도해보고, 각각이 반환하는 것을 설명해 본다. 힌트: typeof() 함수를 사용해서 각각의 경우에 반환되는 것을 꼼꼼히 살펴본다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임을 벡터로 구성된 리스트로 간주할 수 있다. 단일 꺾쇠 [1]은 리스트의 첫번째 원소를 리스트로 반환한다. 이번 경우, 데이터프레임의 첫번째 칼럼이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이중 꺾쇠 [[1]]은 리스트의 원소 내용물을 반환한다. 이번 경우, 리스트가 아닌 요인형 벡터로 첫번째 칼럼 내용물을 반환한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n명칭으로 항목을 끄집어내는데 $ 기호를 사용한다. _coat_가 데이터프레임의 첫번째 칼럼으로 요인형 벡터가 반환된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n[\"coat\"] 방식은 칼럼 인덱스를 명칭으로 바꾸고 동시에 꺾쇠를 사용한 경우다. 예제 1과 마찬가지로 반환되는 객체는 리스트가 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n단일 꺾쇠를 사용했는데 이번에는 행과 열의 좌표도 넣어 전달했다. 반환되는 객체는 첫번째 행, 첫번째 열에 교차하는 값이 된다. 반환되는 객체는 정수형이지만,요인형 벡터의 일부분이라 정수값과 연관된 라벨 “calico”도 함께 출력한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n앞선 예제와 마찬가지로 꺾쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 행좌표를 지정하지 않은 경우, R에서 결측값은 해당 칼럼 벡터의 모든 원소로 해석된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 한번, 꺽쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 칼럼 좌표가 지정되어 있지 않기 때문에, 첫번째 행의 모든 값을 포함하는 리스트가 반환된다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#행렬",
    "href": "data_structure.html#행렬",
    "title": "4  자료구조",
    "section": "\n4.6 행렬",
    "text": "4.6 행렬\n마지막으로 중요한 자료형이 행렬이다. 0으로 가득찬 행렬을 다음과 같이 선언한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다른 자료구조와 마찬가지로, 행렬에 질문을 다음과 같이 던질 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\nlength(matrix_example) 실행결과는 어떻게 나올까? 시도해보자. 생각한 것과 일치하는가? 왜 그런가/ 왜 그렇지 않는가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n행렬은 차원 속성이 추가된 벡터라서, length() 함수는 행렬의 전체 원소 갯수를 반환시킨다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n또다른 행렬을 만들어 보자. 이번에는 1:50 숫자를 담고 있는 칼럼이 5, 행이 10일 행렬이다. matrix() 함수로 칼럼기준으로 혹은 행기준으로 채울 수 있나요? 행과 열을 바꿔 변경할 수 있는 방법을 찾아보자. (힌트: matrix 도움말 문서를 참조한다!)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n이번 워크샵에서 다룬 각 섹션별 문자벡터를 포함하는 길이 2를 갖는 리스트를 생성하시오.\n\n자료형\n자료구조\n\n지금까지 살펴본 자료형과 자료구조를 명칭으로 갖는 문자 벡터를 채워넣는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n주목: 칠판이나 벽에 자료형과 자료구조를 모두 적어두는 것이 도움이 될 수 있다 - 워크샵 동안 참여자들에게 기본 자료형과 구조의 중요성을 상기할 수 있기 때문이다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n아래 행렬의 출력 결과를 생각해보자:\n     [,1] [,2]\n[1,]    4    1\n[2,]    9    5\n[3,]   10    7\n이 행렬을 작성하는 올바른 명령어는 다음 중 무엇일까? 직접 타이핑하기 전에 각 명령어를 살펴보고, 정답을 생각해보자. 다른 명령어는 어떤 행렬을 만들어낼지도 생각해본다.\n\nmatrix(c(4, 1, 9, 5, 10, 7), nrow = 3)\nmatrix(c(4, 9, 10, 1, 5, 7), ncol = 2, byrow = TRUE)\nmatrix(c(4, 9, 10, 1, 5, 7), nrow = 2)\nmatrix(c(4, 1, 9, 5, 10, 7), ncol = 2, byrow = TRUE)\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "dataframe.html",
    "href": "dataframe.html",
    "title": "5  데이터프레임 탐색",
    "section": "",
    "text": "5.1 행과 열 추가\n데이터프레임의 칼럼은 벡터라는 것을 배웠다. 따라서, 데이터는 칼럼에서 자료형의 일관성을 유지해야 한다. 이를테면, 칼럼을 새로 추가하려면 벡터를 새로 만들어서 시작한다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#데이터프레임에-행과-열-추가",
    "href": "dataframe.html#데이터프레임에-행과-열-추가",
    "title": "5  데이터프레임 탐색",
    "section": "",
    "text": "R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nage를 칼럼으로 다음과 같이 추가한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임의 행의 개수와 다른 개수를 갖는 age 벡터를 추가하게 되면 추가되지 않고 오류가 발생된다는 점에 주의한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n정상 동작이 되지 않는 이유가 무엇일까? R은 테이블의 모든 행마다 신규 칼럼에서도 원소 하나가 있길 원한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래서, 정상 동작하려면 nrow(cats) = length(age)이 되어야 한다. cats 내용을 새로운 데이터프레임으로 덮어써보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 행을 추가하면 어떻게 될까? 이미 데이터프레임의 행이 리스트라는 사실을 알고 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#요인",
    "href": "dataframe.html#요인",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.2 요인",
    "text": "5.2 요인\n살펴볼 것이 하나 더 있다. 범주형 자료 처리를 위한 요인(factor)에서 각기 다른 값을 범주 수준(level)이라고 한다. 요인형 “coat” 변수는 수준이 3으로 구성된다. “black”, “calico”, “tabby”. R은 세 가지 수준 중 하나와 매칭되는 값만 받아들인다. 완전 새로운 값을 추가하게 되면, 추가되는 신규 값은 NA가 된다.\n경고 메시지를 통해서 coat 요인변수에 “tortoiseshell” 값을 추가하는데 성공하지 못했다고 알려준다. 하지만, 3.3 (숫자형), TRUE (논리형), 9 (숫자형)은 모두 weight, likes_string, age 변수에 성공적으로 추가된다. 왜냐하면 이 변수들이 요인형이 아니기 때문이다. “tortoiseshell”을 coat 요인변수에 성공적으로 추가하려면 요인의 수준(level)로 “tortoiseshell”을 추가하면 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n대안으로, 요인형 벡터를 문자형 벡터로 변환시키면 된다. 요인변수의 범주를 잃게 되지만, 요인 수준을 조심스럽게 다룰 필요 없이 칼럼에 추가하고자 하는 임의의 단어를 추가할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n\n\ncats$age 벡터에 7을 곱해서 human_age 벡터를 생성하자.\n\nhuman_age를 요인형으로 변환시키자.\n\nas.numeric() 함수를 사용해서 human_age 벡터를 다시 숫자형 벡터로 변환시킨다. 이제 7로 나눠서 원래 고양이 나이로 되돌리자. 무슨 일이 생겼는지 설명하자.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nhuman_age &lt;- cats$age * 7\n\nhuman_age &lt;- factor(human_age). as.factor(human_age) 도 잘 동작한다.\n\nas.numeric(human_age)을 실행하면 1 2 3 4 4이 된다.\n\n왜냐하면 요인형 변수는 정수형(여기서 1:4)으로 자료를 저장하기 때문이다. 정수 라벨과 연관된 값은 여기서 28, 35, 56, 63이다. 요인형 변수를 숫자형 벡터로 변환시키면 라벨이 아니라 그 밑단의 정수를 반환시킨다. 원래 숫자를 원하는 경우, human_age를 문자형 벡터로 변환시키고 나서 숫자형 벡터로 변환시키면 된다. (왜 이 방식은 정상 동작할까?) 실수로 숫자만 담긴 칼럼 어딘가에 문자가 포함된 csv 파일로 작업할 때 이런 일이 실제로 종종 일어난다. 데이터를 불러 읽어올 때 stringsAsFactors=FALSE 설정을 잊지 말자.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#행-제거",
    "href": "dataframe.html#행-제거",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.3 행 제거",
    "text": "5.3 행 제거\n이제 데이터프레임에 행과 열을 추가하는 방법을 알게 되었다. 하지만, 데이터프레임에 “tortoiseshell” 고양이를 처음으로 추가하면서 우연히 쓰레기 행을 추가시켰다. 데이터프레임에 문제가 되는 행을 마이너스 연산자를 사용해서 빼자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n-4, 다음에 아무것도 지정하지 않아서 4번째 행 전체를 제거함에 주목한다. 주목: 벡터 내부에 행 다수를 넣어 한번에 행을 제거할 수도 있다: cats[c(-4,-5), ] 대안으로, NA 값을 갖는 모든 행을 제거시킨다. 출력결과를 cats에 다시 대입하여 변경사항이 데이터프레임에 영구히 남도록 조치한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#칼럼-제거",
    "href": "dataframe.html#칼럼-제거",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.4 칼럼 제거",
    "text": "5.4 칼럼 제거\n데이터프레임의 칼럼도 제거할 수 있다. “age” 칼럼을 제거하고자 한다면 어떻게 해야 할까? 변수명과 변수 인덱스, 두가지 방식으로 칼럼을 제거할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n,-4 앞에 아무것도 없는 것에 주목한다. 이는 모든 행을 간직한다는 의미를 갖는다. 대안으로, 색인명을 사용해서 컬럼을 제거할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#데이터프레임에-덧붙이기",
    "href": "dataframe.html#데이터프레임에-덧붙이기",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.5 데이터프레임에 덧붙이기",
    "text": "5.5 데이터프레임에 덧붙이기\n데이터프레임에 데이터를 추가할 때 기억할 것은 칼럼은 벡터, 행은 리스트라는 사실이다. rbind() 함수를 사용해서 데이터프레임 두 개를 본드로 붙이듯이 결합시킬 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n하지만 이제 행 이름이 불필요하게 복잡해졌다. 행 이름을 제거하면 R이 자동으로 순차적인 이름을 다시 붙여준다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n다음 구문을 사용해서 R 내부에서 직접 데이터프레임을 새로 만들 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다음 정보를 갖는 데이터프레임을 직접 제작해 보자.\n\n이름(first name)\n성(last name)\n좋아하는 숫자\n\nrbind를 사용해서 옆사람을 항목에 추가한다. 마지막으로 cbind() 함수를 사용해서 “지금이 커피 시간인가요?”라는 질문의 답을 칼럼으로 추가한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#현실적인-예제",
    "href": "dataframe.html#현실적인-예제",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.6 현실적인 예제",
    "text": "5.6 현실적인 예제\n지금까지 고양이 데이터를 가지고 데이터프레임 조작에 대한 기본적인 사항을 살펴봤다. 이제 학습한 기술을 사용해서 좀 더 현실적인 데이터셋을 다뤄보자. 앞에서 다운로드 받은 gapminder 데이터셋을 불러오자.\n\ngapminder &lt;- read.csv(\"data/gapminder_data.csv\")\n\n\n\n\n\n\n\n데이터셋 설명\n\n\n\n\n흔히 맞닥드리는 또다른 유형의 파일이 탭구분자를 갖는 파일(.tsv)이다. 탭을 구분자로 명세하는데, \"\\\\t\"을 사용하고, read.delim() 함수로 불러 읽어온다.\n파일을 download.file() 함수를 사용해서 인터넷으로부터 직접 본인 컴퓨터 폴더로 다운로드할 수 있다. read.csv() 함수를 실행해서 다운로드 받은 파일을 읽어온다. 예를 들어,\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n대안으로, read.csv() 함수 내부에 파일 경로를 웹 주소로 치환해서 인터넷에서 직접 파일을 불러올 수도 있다. 이런 경우 로컬 컴퓨터에 CSV 파일이 전혀 저장되지 않았다는 점을 주의한다. 예를 들어,\n\n\ngapminder &lt;- read.csv(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/main/episodes/data/gapminder_data.csv\")\n\n\n\nreadxl 패키지를 사용해서, 엑셀 스프레드시트를 평범한 텍스트로 변환하지 않고 직접 불러올 수도 있다.\n\n\n\ngapminder 데이터셋을 좀 더 살펴보자. 항상 가장 먼저 해야 되는 작업은 str 명령어로 데이터가 어떻게 생겼는지 확인하는 것이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ntypeof() 함수로 데이터프레임 칼럼 각각을 면밀히 조사할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임 차원에 대한 정보를 얻어낼 수도 있다. str(gapminder) 실행결과 gapminder 데이터프레임에 관측점 1704개, 변수 6개가 있음을 상기한다. 다음 코드 실행결과는 무엇일까? 그리고 왜 그렇게 되는가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n공정한 추측은 아마도 데이터프레임 길이가 행의 길이(1704)라고 보는 것이다. 하지만, 이번에는 다르다. 데이터프레임은 벡터와 요인으로 구성된 리스트라는 사실을 기억하라.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nlength() 함수가 6을 제시하는 이유는 gapminder가 6개 칼럼을 갖는 리스트로 만들어졌기 때문이다. 데이터셋에서 행과 열 숫자를 얻는 데 다음 함수를 던져보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n혹은 한번에 보려면 다음과 같이 한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n또한, 모든 칼럼의 칼럼명이 무엇인지 파악하고자 하면 다음과 같이 질문을 던진다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n현 단계에서, R이 제시하는 구조가 우리의 직관이나 예상과 부합되는지 묻어보는 것이 중요하다. 각 칼럼에 대한 기본 자료형은 이해가 되는가? 만약 납득이 가지 않는다면, 후속 작업에서 나쁜 놀라운 사실로 전환되기 전에 문제를 해결해야 한다. 문제를 해결하는 데, R이 데이터를 이해하는 방법과 데이터를 기록할 때 엄격한 일관성(strict consistency)의 중요성에 관해 학습한 것을 동원한다.\n자료형과 자료구조가 타당해 보이게 되면, 데이터를 제대로 파고들 시간이 되었다. gapminder 데이터의 처음 몇 줄을 살펴보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n데이터 마지막 몇 줄, 중간 몇 줄을 점검하는 것도 좋은 습관이다. 그런데 어떻게 점검할 수 있을까?\n중간 몇 줄을 찾아보는 것이 너무 어렵지는 않지만, 임의로 몇 줄을 추출할 수도 있다. 어떻게 할 수 있을까?\n\n\n\n\n\n\n해답\n\n\n\n\n\n마지막 몇 줄을 점검하려면, R에 내장된 함수가 있어서 상대적으로 간단하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터가 온전한지(혹은 관점에 따라 데이터가 온전하지 않은지)를 점검하는 데 몇 줄을 추출할 수 있을까?\n\n팁: 몇 가지 방법이 존재한다.\n\n중첩 함수(또 다른 함수에 인자로 전달되는 함수)를 사용한 해법도 있다. 새로운 개념처럼 들리지만, 사실 이미 사용하고 있다. my_dataframe[rows, cols] 명령어는 데이터프레임을 화면에 뿌려준다. 데이터프레임에 행이 얼마나 많은지 알지 못하는데 어떻게 마지막 행을 뽑아낼 수 있을까? R에 내장된 함수가 있다. (의사) 난수를 얻어보는 것은 어떨까? R은 난수추출 함수도 갖추고 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n분석 결과를 재현 가능하게 확실히 만들려면, 코드를 스크립트 파일에 저장해서 나중에 다시 볼 수 있어야 한다.\n\n\n\n\n\n\n도전과제\n\n\n\nfile -&gt; new file -&gt; R script로 가서, gapminder 데이터셋을 불러오는 R 스크립트를 작성한다. scripts/ 디렉토리에 저장하고 버전 제어 시스템에도 추가한다. 인자로 파일 경로명을 사용해서 source() 함수를 사용해서 스크립트를 실행하라. (혹은 RStudio “source” 버튼을 누른다)\n\n\n\n\n\n\n해답\n\n\n\n\n\nscripts/load-gapminder.R 파일에 담긴 내용물은 다음과 같다.\n\ndownload.file(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/main/episodes/data/gapminder_data.csv\", destfile = \"data/gapminder_data.csv\")\ngapminder &lt;- read.csv(file = \"data/gapminder_data.csv\")\n\n스크립트를 실행시키면 데이터를 gapminder 변수에 적재시킨다.\n\nsource(file = \"scripts/load-gapminder.R\")\n\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\nstr(gapminder) 출력결과를 다시 불러오자. 이번에는 gapminder 데이터에 대해 str() 함수가 출력하는 모든 것이 의미하는 바를 설명한다. 지금까지 학습한 요인, 리스트와 벡터 뿐만 아니라, colnames(), dim() 같은 함수도 동원한다. 이해하지 못한 부분이 있다면, 주변 동료와 상의한다!\n\n\n\n\n\n\n해답\n\n\n\n\n\ngapminder 객체는 다음 칼럼을 갖는 데이터프레임이다. - country와 continent 변수는 요인형 벡터 - year 변수는 정수형 벡터 - pop, lifeExp, gdpPercap 변수는 숫자형 벡터",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#행과-열-추가",
    "href": "dataframe.html#행과-열-추가",
    "title": "5  데이터프레임 탐색",
    "section": "",
    "text": "R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nage를 칼럼으로 다음과 같이 추가한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임의 행의 개수와 다른 개수를 갖는 age 벡터를 추가하게 되면 추가되지 않고 오류가 발생된다는 점에 주의한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n정상 동작이 되지 않는 이유가 무엇일까? R은 테이블의 모든 행마다 신규 칼럼에서도 원소 하나가 있길 원한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래서, 정상 동작하려면 nrow(cats) = length(age)이 되어야 한다. cats 내용을 새로운 데이터프레임으로 덮어써보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 행을 추가하면 어떻게 될까? 이미 데이터프레임의 행이 리스트라는 사실을 알고 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#덧붙이기",
    "href": "dataframe.html#덧붙이기",
    "title": "5  데이터프레임 탐색",
    "section": "\n5.5 덧붙이기",
    "text": "5.5 덧붙이기\n데이터프레임에 데이터를 추가할 때 기억할 것은 칼럼은 벡터, 행은 리스트라는 사실이다. rbind() 함수를 사용해서 데이터프레임 두 개를 본드로 붙이듯이 결합시킬 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n하지만 이제 행 이름이 불필요하게 복잡해졌다. 행 이름을 제거하면 R이 자동으로 순차적인 이름을 다시 붙여준다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n다음 구문을 사용해서 R 내부에서 직접 데이터프레임을 새로 만들 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다음 정보를 갖는 데이터프레임을 직접 제작해 보자.\n\n이름(first name)\n성(last name)\n좋아하는 숫자\n\nrbind를 사용해서 옆사람을 항목에 추가한다. 마지막으로 cbind() 함수를 사용해서 “지금이 커피 시간인가요?”라는 질문의 답을 칼럼으로 추가한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  }
]
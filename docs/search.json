[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "1 데이터 과학",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>챗GPT 데이터 과학</span>"
    ]
  },
  {
    "objectID": "index.html#교육",
    "href": "index.html#교육",
    "title": "챗GPT 데이터 과학",
    "section": "1.2 교육",
    "text": "1.2 교육\n\nTidyverse Hands-on",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>챗GPT 데이터 과학</span>"
    ]
  },
  {
    "objectID": "index.html#관련-링크",
    "href": "index.html#관련-링크",
    "title": "챗GPT 데이터 과학",
    "section": "1.3 관련 링크",
    "text": "1.3 관련 링크\n\nR사용자회\n페이스북\n유튜브\nGitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>챗GPT 데이터 과학</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n2  소개\n",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "tidyverse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>소개</span>"
    ]
  },
  {
    "objectID": "lang_r.html",
    "href": "lang_r.html",
    "title": "\n3  R 언어\n",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lang_py.html",
    "href": "lang_py.html",
    "title": "4  파이썬",
    "section": "",
    "text": "About this site\n\nprint(\"챗GPT 데이터 과학 세상에 오신 것을 환영합니다.\")\n\n챗GPT 데이터 과학 세상에 오신 것을 환영합니다."
  },
  {
    "objectID": "index.html#데이터-과학",
    "href": "index.html#데이터-과학",
    "title": "챗GPT 데이터 과학",
    "section": "0.1 데이터 과학",
    "text": "0.1 데이터 과학"
  },
  {
    "objectID": "index.html#참고",
    "href": "index.html#참고",
    "title": "챗GPT 데이터 과학",
    "section": "1.1 참고",
    "text": "1.1 참고",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>챗GPT 데이터 과학</span>"
    ]
  },
  {
    "objectID": "penguins.html",
    "href": "penguins.html",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "4 펭귄 데이터셋"
  },
  {
    "objectID": "penguins.html#펭귄-데이터-출현",
    "href": "penguins.html#펭귄-데이터-출현",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.1 펭귄 데이터 출현",
    "text": "4.1 펭귄 데이터 출현\n미국에서 “George Floyd”가 경찰에 의해 살해되면서 촉발된 “Black Lives Matter” 운동은 아프리카계 미국인을 향한 폭력과 제도적 인종주의에 반대하는 사회운동이다. 한국에서도 소수 정당인 정의당에서 여당 의원 176명 중 누가?…차별금지법 발의할 ’의인’을 구합니다로 기사로 낼 정도로 적극적으로 나서고 있다.\n데이터 과학에서 최근 R.A. Fisher의 과거 저술한 “The genetical theory of natural selection” [@fisher1958genetical] 우생학(Eugenics) 대한 관점이 논란이 되면서 R 데이터 과학의 첫 데이터셋으로 붓꽃 iris 데이터를 다른 데이터, 즉 펭귄 데이터로 대체하는 움직임이 활발히 전개되고 있다. palmerpenguins [@penguin2020] 데이터셋이 대안으로 많은 호응을 얻고 있다. [@AbdulMajedRaja2020, @Levy2019]"
  },
  {
    "objectID": "penguins.html#penguins-study",
    "href": "penguins.html#penguins-study",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.2 펭귄 공부",
    "text": "4.2 펭귄 공부\n팔머(Palmer) 펭귄은 3종이 있으며 자세한 내용은 다음 나무위키를 참조한다. 1\n\n\n젠투 펭귄(Gentoo Penguin): 머리에 모자처럼 둘러져 있는 하얀 털 때문에 알아보기가 쉽다. 암컷이 회색이 뒤에, 흰색이 앞에 있다. 펭귄들 중에 가장 빠른 시속 36km의 수영 실력을 자랑하며, 짝짓기 할 준비가 된 펭귄은 75-90cm까지도 자란다.\n\n아델리 펭귄(Adelie Penguin): 프랑스 탐험가인 뒤몽 뒤르빌(Dumont D’Urville) 부인의 이름을 따서 ’아델리’라 불리게 되었다. 각진 머리와 작은 부리 때문에 알아보기 쉽고, 다른 펭귄들과 마찬가지로 암수가 비슷하게 생겼지만 암컷이 조금 더 작다.\n\n턱끈 펭귄(Chinstrap Penguin): 언뜻 보면 아델리 펭귄과 매우 비슷하지만, 몸집이 조금 더 작고, 목에서 머리 쪽으로 이어지는 검은 털이 눈에 띈다. 어린 고삐 펭귄들은 회갈색 빛을 띄는 털을 가지고 있으며, 목 아래 부분은 더 하얗다. 무리를 지어 살아가며 일부일처제를 지키기 때문에 짝짓기 이후에도 부부로써 오랫동안 함께 살아간다.\n\n\n\n팔머 펭귄 3종 세트\n\n다음으로 iris 데이터와 마찬가지로 펭귄 3종을 구분하기 위한 변수로 조류의 부리에 있는 중앙 세로선의 융기를 지칭하는 능선(culmen) 길이(culmen length)와 깊이(culmen depth)를 이해하면 된다.\n\n\n팔머 펭귄 능선 변수"
  },
  {
    "objectID": "penguins.html#penguin-home",
    "href": "penguins.html#penguin-home",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.3 펭귄 서식지",
    "text": "4.3 펭귄 서식지\nleaflet 팩키지로 펭귄 서식지를 남극에서 특정한다. geocoding을 해야 하는데 구글에서 위치 정보를 구글링하면 https://latitude.to/에서 직접 위경도를 반환하여 준다. 이 정보를 근거로 하여 펭귄 서식지를 시각화한다.\n\n\n\n\n파머 연구소와 펭귄 서식지\n\n\n\n펭귄 3종\n\n\n\n\n\n아델리, 젠투, 턱끈 펭귄이 함께한 사진\n\n\n\n토르거센 섬에서 새끼를 키우는 아델리 펭귄\n\n\n\n비스코 지점 젠투 펭귄 서식지\n\n\n\n펭귄과 함께 현장에서 일하는 크리스틴 고먼 박사\n\n\n\n\n파머 펭귄 데이터셋\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(palmerpenguins)\n# library(tidygeocoder)\n\npenguins %&gt;% \n  count(island)\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\nisland_df &lt;- tribble(~\"address\", ~\"lat\", ~\"lng\",\n                     \"Torgersen Island antarctica\", -64.772819, -64.074325,\n                     \"Dream Island antarctica\", -64.725558, -64.225562,\n                     \"Biscoe Island antarctica\", -64.811565, -63.777947,\n                     \"Palmer Station\", -64.774312, -64.054213)\n\nisland_df %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addMarkers(lng=~lng, lat=~lat, \n                   popup = ~ as.character(paste0(\"&lt;strong&gt;\", paste0(\"명칭:\",`address`), \"&lt;/strong&gt;&lt;br&gt;\",\n                                                 \"-----------------------------------------------------------&lt;br&gt;\",\n                                                 \"&middot; latitude: \", `lat`, \"&lt;br&gt;\",\n                                                 \"&middot; longitude: \", `lng`, \"&lt;br&gt;\"\n                   )))"
  },
  {
    "objectID": "penguins.html#데이터-설치",
    "href": "penguins.html#데이터-설치",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.4 데이터 설치",
    "text": "4.4 데이터 설치\nremotes 팩키지 install_github() 함수로 펭귄 데이터를 설치한다.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"allisonhorst/palmerpenguins\")\n\ntidyverse 팩키지 glimpse() 함수로 펭귄 데이터를 일별한다.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "penguins.html#penguin-EDA-skimr",
    "href": "penguins.html#penguin-EDA-skimr",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.5 자료구조 일별",
    "text": "4.5 자료구조 일별\nskimr 팩키지를 사용해서 penguins 데이터프레임 자료구조를 일별한다. 이를 통해서 344개 펭귄 관측값이 있으며, 7개 칼럼으로 구성된 것을 확인할 수 있다. 또한, 범주형 변수가 3개, 숫자형 변수가 4개로 구성되어 있다. 그외 더 자세한 사항은 범주형, 숫자형 변수에 대한 요약 통계량을 참조한다.\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n데이터가 크지 않아 DT 팩키지를 통해 데이터 전반적인 내용을 살펴볼 수 있다.\n\npenguins %&gt;% \n  reactable::reactable()"
  },
  {
    "objectID": "penguins.html#penguin-EDA",
    "href": "penguins.html#penguin-EDA",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "\n4.6 탐색적 데이터 분석",
    "text": "4.6 탐색적 데이터 분석\npalmerpenguins 데이터셋 소개에 포함되어 있는 미국 팔머 연구소 (palmer station) 펭귄 물갈퀴(flipper) 길이와 체질량(body mass) 산점도를 그려보자.\n\nlibrary(tidyverse)\nlibrary(extrafont)\nloadfonts()\n\nmass_flipper &lt;- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  theme_minimal(base_family = \"NanumGothic\") +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"펭귄 크기\",\n       subtitle = \"남극 펭귄 3종 물갈퀴 길이와 체질량 관계\",\n       x = \"물갈퀴 길이 (mm)\",\n       y = \"체질량 (g)\",\n       color = \"펭귄 3종\",\n       shape = \"펭귄 3종\") +\n  theme(legend.position = c(0.2, 0.7),\n        legend.background = element_rect(fill = \"white\", color = NA),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\")\n\nmass_flipper"
  },
  {
    "objectID": "penguins.html#footnotes",
    "href": "penguins.html#footnotes",
    "title": "\n3  펭귄 데이터셋\n",
    "section": "",
    "text": "신발끈 여행사, 관광안내자료↩︎"
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "4  챗GPT 자연어",
    "section": "",
    "text": "5 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n6 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Notable.ai\n\n\n8 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기"
  },
  {
    "objectID": "lang_sql.html",
    "href": "lang_sql.html",
    "title": "7  파이썬",
    "section": "",
    "text": "About this site\n\nprint(\"챗GPT 데이터 과학 세상에 오신 것을 환영합니다.\")\n\n챗GPT 데이터 과학 세상에 오신 것을 환영합니다."
  },
  {
    "objectID": "simpson.html",
    "href": "simpson.html",
    "title": "8  심슨의 역설",
    "section": "",
    "text": "9 심슨의 역설 사례\n심슨의 역설 사례를 책페이지수와 책가격의 관계를 살펴보자. 데이터는 책 유형(하드커버, 페이퍼백)은 두가지가 있고, 페이지수와 책가격이 달러로 구성된 데이터프레임이다.\n심슨의 역설에 대해서 동아사이언스에서 2008년, 2013년 3회 심슨의 역설에 대한 기사를 실었는데 “명문대 남녀 합격생의 반전”, “오류를 잡아라! 확률 법정” 의 기사가 눈에 띈다."
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA",
    "href": "simpson.html#simpson-paradox-case-study-EDA",
    "title": "8  심슨의 역설",
    "section": "\n9.1 데이터 시각화",
    "text": "9.1 데이터 시각화\n이를 시각적으로 표현하면 관계가 음의 상관관계를 갖는 것을 알 수 있다.\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(extrafont)\nloadfonts()\n\nsimp_df &lt;- tribble(\n    ~book_type, ~num_pages, ~book_price,\n    \"hardcover\", 150, 27.43, \n    \"hardcover\", 225, 48.76, \n    \"hardcover\", 342, 50.25, \n    \"hardcover\", 185, 32.01, \n    \"paperback\", 475, 10.00, \n    \"paperback\", 834, 15.73, \n    \"paperback\", 1020, 20.00, \n    \"paperback\", 790, 17.89)\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE)"
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor",
    "title": "8  심슨의 역설",
    "section": "\n9.2 기술통계량",
    "text": "9.2 기술통계량\nnum_pages, book_price 두변수를 추출하여 상관계수를 도출한다. 그리고 나서, 책 유형에 따른 상관관계도 도출해 낸다. 먼저 책 유형에 관계없이 num_pages, book_price 상관관계는 -0.5949366으로 나름 강한 음의 상관계수가 관측된다.\n\nsimp_df %&gt;% \n    summarise(book_cor = cor(num_pages, book_price)) %&gt;% \n    pull()\n\n[1] -0.5949366\n\n\n이번에는 책 유형에 따른 상관계수는 어떤지 계산해 보자. 이 경우, 하드커버는 0.848, 페이퍼백은 0.956로 강한 양의 상관관계가 존재함이 확인된다.\n\nsimp_df %&gt;% \n    group_by(book_type) %&gt;% \n    summarise(book_cor = cor(num_pages, book_price))\n\n# A tibble: 2 × 2\n  book_type book_cor\n  &lt;chr&gt;        &lt;dbl&gt;\n1 hardcover    0.848\n2 paperback    0.956"
  },
  {
    "objectID": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "href": "simpson.html#simpson-paradox-case-study-EDA-cor-viz",
    "title": "8  심슨의 역설",
    "section": "\n9.3 상관관계 시각화",
    "text": "9.3 상관관계 시각화\n앞서 확인한 결과를 책 유형별로 나눠 상관계수를 시각화한다.\n\nsimp_df %&gt;% \n    ggplot(aes(x=num_pages, y=book_price, color=book_type)) +\n      geom_point(size=3) +\n      geom_smooth(method = \"lm\", se=FALSE) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"책페이지 수\", y=\"책가격($)\", title=\"심슨의 역설 사례\", color=\"책유형\" )+\n      theme(legend.position = \"top\")"
  },
  {
    "objectID": "simpson.html#simpson-paradox-berkeley",
    "href": "simpson.html#simpson-paradox-berkeley",
    "title": "8  심슨의 역설",
    "section": "\n10.1 UC 버클리 입학 3\n",
    "text": "10.1 UC 버클리 입학 3\n\n심슨의 역설관련 가장 유명한 사례는 1973년 UC 버클리 대학 입학데이터로 입학에 성차별이 존재하는지에 관한 데이터다.\n\n\n성별에 따른 입학률 비교\n\nlibrary(datasets)\nadmin_df &lt;- UCBAdmissions %&gt;% tbl_df\n\nadmin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n\n복사하여 붙여넣기\n\nadmin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    DT::datatable(options = list(scrollX = TRUE)) %&gt;% \n    DT::formatPercentage(c(\"Admitted_Pcnt\",\"Rejected_Pcnt\"), digits=1)\n\n\n\n\n\n\n\n\n기술통계량을 통해 살펴본 사항을 그래프로 시각화한다. 막대 그래프를 통해 남성 합격률이 여성보다 높은 것으로 나타나 성차별이 존재하는 것으로 파악되지만, 학과별로 놓고 보면 여성 합격률이 더 높거나 남성과 유사한 것으로 시각적으로 나타난다.\n\n# A barplot for overall admission percentage for each gender.\n\nadmit_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, width = 0.2, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"성별\", y=\"입학합격율\", title=\"버클리 전체 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_minimal(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") \n\nadmit_dept_g &lt;- admin_df %&gt;% \n    group_by(Gender, Admit, Dept) %&gt;% \n    dplyr::summarise(total = sum(n)) %&gt;% \n    spread(Admit, total) %&gt;% \n    mutate(Admitted_Pcnt = Admitted/ (Admitted+Rejected),\n           Rejected_Pcnt = Rejected/ (Admitted+Rejected)) %&gt;% \n    ggplot(aes(x = Gender, y = Admitted_Pcnt, fill=Gender)) +\n    geom_bar(stat = \"identity\") +\n    facet_grid(. ~ Dept) +\n    labs(x=\"성별\", y=\"\", title=\"버클리 학과별 입학합격률\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          legend.position = \"none\") \n\nplot_grid(admit_g, admit_dept_g, labels = \"\")"
  },
  {
    "objectID": "simpson.html#simpson-paradox-news-article-game-case",
    "href": "simpson.html#simpson-paradox-news-article-game-case",
    "title": "8  심슨의 역설",
    "section": "\n11.1 게임 업데이터 사례 4\n",
    "text": "11.1 게임 업데이터 사례 4\n\n예전에 모 게임에서 큰 규모의 업데이트를 한 후 게임 고객 동향을 분석한 적이 있습니다. 이 게임은 전체 게임 고객을 약 십 여가지 유형으로 분류하고 있는데, 크게 보면 게임 활동이 왕성하고 충성도가 높은 ‘진성’ 유형, 게임 활동이 그리 활발하지 않은 ‘라이트’ 유형, 자동 사냥 유저로 의심되는 ‘봇’ 유형 등이 있죠.\n이 게임의 업데이트 전/후 일별접속자수(DAU)와 유저당 결제금액(ARPU) 지표를 확인해 보니 아래와 같이 나왔습니다.\n\n일별 접속자수(DAU)가 크게 늘었지만 유저당 결재금액(ARPU)가 하락하여 뭔가 특단의 조치가 필요한 것으로 파악되지만, 이를 고객 집단을 반영하여 분석을 하게 되면 진성유저는 큰 차이가 없고, 크게 늘어난 유저가 봇이거나 Non-PU 유저라 봇을 비용으로 간주하여 제거하거나 Non-PU유저를 PU로 바꾸거나 PU 유저의 결재금액을 높이는 방향으로 사업적인 조치를 취하는 것이 바람직스러워 보인다."
  },
  {
    "objectID": "simpson.html#footnotes",
    "href": "simpson.html#footnotes",
    "title": "8  심슨의 역설",
    "section": "",
    "text": "Paul van der Laken (27 September 2017), “Simpson’s Paradox: Two HR examples with R code.”↩︎\n나무위키, “심슨의 역설”↩︎\nJohnny Hong(January 30, 2016), “A (very) brief introduction to ggplot2”↩︎\nNC소프트 (2017-07-05) “데이터 분석을 이용한 게임 고객 모델링 #4”↩︎"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "\n21  쿼토\n",
    "section": "",
    "text": "21.1 쿼토설치",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#윈도우-설치",
    "href": "quarto.html#윈도우-설치",
    "title": "\n9  쿼토\n",
    "section": "\n9.1 윈도우 설치",
    "text": "9.1 윈도우 설치\nQuarto를 운영체제에 맞춰 설치한다. Quarto 는 기본적으로 CLI 라서 설치 후 제대로 설정이 되었는지는 환경설정에 경로를 등록해줘야 한다.\n\n\nQuarto 다운로드\nQuarto 설치\nQuarto CLI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n윈도우 시스템의 경우 quarto.exe가 아니고 quarto.cmd 라 이에 유의한다. 즉, 제어판 → 환경 변수 설정 … 에서 \"C:\\Users\\사용자명\\AppData\\Local\\Programs\\Quarto\\bin 디렉토리를 등록한 후 quarto.cmd 을 사용해서 출판한다. 쿼토 1.3 버전이 출시되면서 이런 문제는 해결되었다.\n\nSys.which(\"quarto\")\n                                                                  quarto \n\"C:\\\\Users\\\\STATKC~1\\\\AppData\\\\Local\\\\Programs\\\\Quarto\\\\bin\\\\quarto.cmd\""
  },
  {
    "objectID": "quarto.html#설치방법",
    "href": "quarto.html#설치방법",
    "title": "\n9  쿼토\n",
    "section": "\n9.2 설치방법",
    "text": "9.2 설치방법\nquarto 웹사이트에서 Quarto CLI 엔진을 설치한다. 통합개발도구(IDE)를 설치한다. Quarto CLI를 지원하는 IDE는 VS Code, RStudio, Jupyter, VIM/Emacs 와 같은 텍스트 편집기가 포함된다. IDE까지 설치를 했다면 literate programming 방식으로 마크다운과 프로그래밍 언어를 결합하여 출판을 위한 전문 문서 저작을 시작한다."
  },
  {
    "objectID": "quarto.html#쿼토-출판",
    "href": "quarto.html#쿼토-출판",
    "title": "\n21  쿼토\n",
    "section": "\n21.2 쿼토 출판",
    "text": "21.2 쿼토 출판\n\n21.2.1 출판 플랫폼\n데이터 사이언스 저작물을 제작하게 되면 그 다음 단계로 출판을 해야하는데 다양한 문서를 모아 프로젝트로 담아 Quarto Pub에 전자출판한다. 다른 출판 플랫폼으로 netlify, GitHub Pages, RStudio Connect가 많이 사용된다.\n\n\n21.2.2 Quarto Pub 출판 1\n\nQuarto Pub 웹사이트에 출판하는 방식은 Quarto CLI 를 사용한다. 필히 RStudio 내부 Terminal을 사용해서 Quarto Pub으로 출판한다.\n\nquarto.cmd publish quarto-pub\n? Authorize (Y/n) › \n❯ In order to publish to Quarto Pub you need to\n  authorize your account. Please be sure you are\n  logged into the correct Quarto Pub account in \n  your default web browser, then press Enter or \n  'Y' to authorize.\n\n첫번째 출판하게 되면 인증작업을 수행하고 나면 _publish.yml 파일이 하나 생성된다.\n\n- source: project\n  quarto-pub:\n    - id: 1fa3ab1f-c010-453a-aaf2-f462bd074a66\n      url: 'https://quartopub.com/sites/statkclee/quarto-ds'\n\n이제 모든 준비가 되었기 때문에 다음 명령어로 작성한 출판 문서를 포함한 웹사이트를 로컬에서 미리 확인 한 후에 Quarto Pub으로 전자출판한다. 윈도우에서는 RStudio 내부 Terminal CLI를 사용하는 것을 권장한다.\n\nquarto preview\nquarto publish quarto-pub",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "\n21  쿼토\n",
    "section": "",
    "text": "Quarto Pub↩︎\nAdam Hyde (Aug 16, 2021), “Single Source Publishing - A investigation of what Single Source Publishing is and how this ‘holy grail’ can be achieved.”↩︎",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#쿼토설치",
    "href": "quarto.html#쿼토설치",
    "title": "\n21  쿼토\n",
    "section": "",
    "text": "21.1.1 설치방법\nquarto 웹사이트에서 Quarto CLI 엔진을 설치한다. 통합개발도구(IDE)를 설치한다. Quarto CLI를 지원하는 IDE는 VS Code, RStudio, Jupyter, VIM/Emacs 와 같은 텍스트 편집기가 포함된다. IDE까지 설치를 했다면 literate programming 방식으로 마크다운과 프로그래밍 언어를 결합하여 출판을 위한 전문 문서 저작을 시작한다.\n\n\n21.1.2 윈도우 설치\nQuarto를 운영체제에 맞춰 설치한다. Quarto 는 기본적으로 CLI 라서 설치 후 제대로 설정이 되었는지는 환경설정에 경로를 등록해줘야 한다.\n\n\nQuarto 다운로드\nQuarto 설치\nQuarto CLI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n윈도우 시스템의 경우 quarto.exe가 아니고 quarto.cmd 라 이에 유의한다. 즉, 제어판 → 환경 변수 설정 … 에서 \"C:\\Users\\사용자명\\AppData\\Local\\Programs\\Quarto\\bin 디렉토리를 등록한 후 quarto.cmd 을 사용해서 출판한다. 쿼토 1.3 버전이 출시되면서 이런 문제는 해결되었다.\n\nSys.which(\"quarto\")\n                                                                  quarto \n\"C:\\\\Users\\\\STATKC~1\\\\AppData\\\\Local\\\\Programs\\\\Quarto\\\\bin\\\\quarto.cmd\"",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#문서-컴파일러-1",
    "href": "quarto.html#문서-컴파일러-1",
    "title": "\n21  쿼토\n",
    "section": "\n21.3 문서 컴파일러 2\n",
    "text": "21.3 문서 컴파일러 2\n\nQuarto 는 Pandoc에 기반한 오픈소스 과학기술 출판시스템이다. 하지만 특정 언어에 종속되지 않고 R, 파이썬, 쥴리아, 자바스크립트(Observable JS) 를 지원하고 있으며 이를 통해 다음 출판 저작물 작성이 가능하다.\n\n크게 세가지 부분에 대해 출판시스템에 대한 고민이 필요하다.\n\n콘텐츠(Content): 저작과 관련된 문서 내용\n디자인(Design): 출판 결과물에 대한 외양(Look and Feel)\n형식(Format): 출판물 최종 산출물\n\nQuarto 는 Literate Programming System 으로 다양한 언어를 지원하고 다양한 출판결과물을 연결시키는 핵심 엔진으로 Pandoc 을 사용한다.\n\n\n\n\n\n\n\nComputations\n문서 저작\n출력물\n\n\nPython, R, Julia, Observable JS\nPandoc, 마크다운 (Markdown)\n문서, 웹사이트, PPT, 책, 블로그등\n\n\n좀더 구체적으로 전문적인 출판을 위해서 문서저작에 다양한 기능과 함께 출판 산출물을 지원한다.\n\n문서저작(pandoc): 마크다운, 수식, 인용, 서지관리, 콜아웃(callout), 고급 layout 등\n출판산출물: 고품질 기사(article), 보고서, PPT, 웹사이트, 블로그, (HTML, PDF, MS 워드, ePub 등) 전자책",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#single-sourcing-출판저작",
    "href": "quarto.html#single-sourcing-출판저작",
    "title": "\n21  쿼토\n",
    "section": "\n21.4 Single Sourcing 출판저작",
    "text": "21.4 Single Sourcing 출판저작\n데이터 사이언스 출판저작에 다소 차이는 있지만 출판에 대한 대체적인 방식은 유사할 것으로 보인다. 즉, Single Sourcing 을 콘텐츠 저작, 디자인, 최종 출판물 관리까지 일원화되어 자동화되어 체계적으로 관리된다면 중복되는 낭비는 물론 재현가능성도 높여 과학기술 출판저작물로 가장 이상적으로 간주되고 있다.\n\n\n문제점\n개념\nSingle Sourcing Multi-Use\n\n\n\n\n\n\n\n\n그림 21.1: 문제점\n\n\n\n\n\n\n\n\n\n그림 21.2: Single Sourcing 개념\n\n\n\n\n\n\n\n\n\n그림 21.3: Single Sourcing Multi-Use",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#작업흐름",
    "href": "quarto.html#작업흐름",
    "title": "\n21  쿼토\n",
    "section": "\n21.5 작업흐름",
    "text": "21.5 작업흐름\n기존 R .Rmd, 파이썬 .ipynb 확장자를 갖는 작업흐름이 .qmd 파일로 단일화되는 것이 가장 큰 특징이다. 따라서 마크다운으로 콘텐츠를 작성하고 프로그래밍 코드를 R, 파이썬, 자바스크립트, 쥴리아 로 작성하게 되면 자동으로 계산을 수행하고 결과물을 마크다운으로 변환시키기 때문에 후속 작업을 신경쓰지 않고 원하는 결과물을 얻을 수 있는 장점이 있다.\n\n\nR (.Rmd)\n파이썬 (주피터)\nQuarto - R\nQuarto - 파이썬",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#주요-기능",
    "href": "quarto.html#주요-기능",
    "title": "\n21  쿼토\n",
    "section": "\n21.6 주요 기능",
    "text": "21.6 주요 기능\n\n\nFeature\nR Markdown\nQuarto\n\n\n\nBasic Formats\n\nhtml_document\npdf_document\nword_document\n\n\nhtml\npdf\ndocx\n\n\n\nBeamer\n\nbeamer_presentation\n\n\nbeamer\n\n\n\nPowerPoint\n\npowerpoint_presentation\n\n\npptx\n\n\n\nHTML Slides\n\nxaringan\nrevealjs\n\n\nrevealjs\n\n\n\nAdvanced Layout\n\ntufte\ndistill\n\n\nQuarto Article Layout\n\n\n\nCross References\n\nhtml_document2\npdf_document2\nword_document2\n\n\nQuarto Crossrefs\n\n\n\nWebsites & Blogs\n\nblogdown\ndistill\n\n\nQuarto Websites\nQuarto Blogs\n\n\n\nBooks\n\nbookdown\n\nQuarto Books\n\n\nInteractivity\nShiny Documents\nQuarto Interactive Documents\n\n\nPaged HTML\npagedown\nSummer 2022\n\n\nJournal Articles\nrticles\nSummer 2022\n\n\nDashboards\n\nflexdashboard |\nFall 2022",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#위지윅-vs-위지윔",
    "href": "quarto.html#위지윅-vs-위지윔",
    "title": "\n21  쿼토\n",
    "section": "\n21.7 위지윅 vs 위지윔",
    "text": "21.7 위지윅 vs 위지윔\n신속하고 빠르게 누구나 짧은 학습을 통해서 문서를 저작하고 출판할 수 있는 방식은 아래한글 혹은 MS워드 워드프로세서를 사용하는데 이는 위지위그(WYSIWYG: What You See Is What You Get, “보는 대로 얻는다”)에 기초한 것으로 문서 편집 과정에서 화면에 포맷된 낱말, 문장이 출력물과 동일하게 나오는 방식을 말한다. 위지윅의 대척점에 있는 것이 위지윔(WYSIWYM, What You See Is What You Mean)으로 대표적인 것인 \\(\\LaTeX\\) 으로 구조화된 방식으로 문서를 작성하면 컴파일을 통해서 최종 문서가 미려한 출판가능한 PDF, PS, DVI 등 확장자를 갖는 출판결과물을 얻을 수 있다.\n\n\n21.7.1 블로그 저작 소프트웨어\n개인용 컴퓨터가 보급되면서 아래한글과 같은 워드 프로세서를 사용해서 저작을 하는 것이 일반화되었지만 곧이어 인터넷이 보급되면서 웹에 문서를 저작하는 것이 이제는 더욱 중요하게 되었다. 전문 개발자가 아닌 일반인이 HTML, CSS, JavaScript를 학습하여 웹에 문서를 제작하고 출판하는 것은 난이도가 있다보니 워드프레스와 티스토리 같은 위지위그 패러다임을 채택한 저작도구가 사용되고 있으나 상대적으로 HTML, CSS, JavaScript을 조합한 방식과 비교하여 고급스러운 면과 함께 정교함에 있어 아쉬움이 있는 것도 사실이다.\n\n\n워드프레스와 티스토리\nHTML + CSS + 자바스크립트",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "quarto.html#쿼토-저작",
    "href": "quarto.html#쿼토-저작",
    "title": "\n21  쿼토\n",
    "section": "\n21.8 쿼토 저작",
    "text": "21.8 쿼토 저작\nQuarto 는 10년전부터 시작된 knitr 경험을 많이 녹여냈고 위지윔 패러다임에 기초를 하고 있다고 볼 수 있다. RStudio를 IDE로 Quarto CLI와 함께 출판물을 저작한다면 편집기에 있는 Visual 모드가 있어 위지윅 패러다임도 문서저작에 사용이 가능하다. 특히, R, 파이썬, SQL, 자바스크립트 등 컴퓨팅 엔진을 달리하여 문서에 그래프, 표, 인터랙티브 결과물도 함께 담을 수 있는 것은 커다란 장점이다.\n\nQuarto 저작은 크게 3가지 구성요소로 되어 있다.\n\n메타데이터: YAML\n텍스트: 마크다운\n코드: knitr, jupyter\n\n\n상기 구성요소를 조합하게 되면 다양한 데이터 사이언스 웹사이트를 비롯한 출판물을 제작하게 된다.\n\n\nQuarto 문서 구성요소\n\n\n21.8.1 YAML\n메타데이터는 YAML인데 GNU처럼 “Yet Another Markup Language” 혹은 “YAML Ain’t Markup Language”을 줄인단어다.\n\n\n키값\n출력옵션\n상세 출력옵션\n\n\n\n---\nkey: value\n---\n\n\n---\nformat: something\n---\n. . .\n---\nformat: pdf\n---\n---\nformat: pdf\n---\n---\nformat: revealjs\n---\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n왜 YAML이 필요하게 된 것인가? YAML은 단순히 KEY: Value 에 불과한데 CLI를 이해하게 되면 왜 YAML을 사용하는 것이 유용한지 이해할 수 있다. 먼저 간단한 CLI 명령어를 YAML로 변환해보자.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html\n\n\n\n\n---\nformat: pdf\n---\n\n\n한단계 더 들어가서 좀더 많은 선택옵션을 넣어 고급 기능을 넣는 사례를 살펴본다.\n\n\n\n\nterminal\n\nquarto render document.qmd --to html -M code fold:true\n\n\n\n\n---\nformat: \n  html:\n    toc: true\n    code-fold: true\n---\n\n\n\n21.8.2 마크다운\n데이터 과학 문서 웹사이트에 “마크다운 기초”, “고급 마크다운”, “R 마크다운 실무” 를 참조한다.\n\n21.8.3 코드\nR, 파이썬, SQL, 자바스크립트 등 버그 없이 정상 동작하는 프로그램을 작성하여 포함시킨다.\n\n21.8.4 YAML 코드편집\nRStudio, VSCode IDE는 탭-자동완성(tab-completion)을 제공한다. 즉, 첫단어를 타이핑하고 탭을 연결하여 키보드를 치게되면 연관 명령어가 나와 선택하면 된다. 혹은 Ctrl + space 단축키를 치게되면 전체 명령어가 나온다.",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "9  통계",
    "section": "",
    "text": "통계학은 왕의 학문이라는 별명을 갖고 있으며, 왕(국가)이 기원전 3,050년 피라미드 건립을 위해 인구조사(센서스)를 했다는 기록이 최초로 남아있다. 근대 국가운영을 위해 인구, 출생, 사망, 실업자수, 세금 수입, 지출, 수입과 수출 등 자료가 필요하여 발전했으며 통계학(Statistics)는 라틴어 Status 정치국가(Political State) 를 의미한다.\n\n\n\n\ngraph LR\n\n  World[\"현실/가상&lt;br&gt;세계\"] --&gt; Data\n  Data[\"데이터\"] --&gt; Analysis[\"분석\"]\n  Analysis --&gt; Summary_Technique_Inference[\"요약기술&lt;br&gt;추론\"]\n  \n  style World fill:#f5d06c,stroke:#333,stroke-width:3px\n  style Data fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style Analysis fill:#c6def1,stroke:#333,stroke-width:3px\n  style Summary_Technique_Inference fill:#e1d5e7,stroke:#333,stroke-width:3px"
  },
  {
    "objectID": "statistics.html#통계-분야",
    "href": "statistics.html#통계-분야",
    "title": "\n9  통계\n",
    "section": "\n9.1 통계 분야",
    "text": "9.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다."
  },
  {
    "objectID": "statistics.html#기술통계",
    "href": "statistics.html#기술통계",
    "title": "\n9  통계\n",
    "section": "\n9.2 기술통계",
    "text": "9.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n\n\n\n9.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n\n# A tibble: 1 × 2\n  평균_체중 최빈종\n      &lt;dbl&gt; &lt;fct&gt; \n1     4202. Adelie\n\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\n\n펭귄 체중: 4201.754385964912\n\nprint(f'펭귄 최빈종: {mode_species}')\n\n펭귄 최빈종: Adelie\n\n\n\n\n\n\n9.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n\n\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  분산_체중 표준편차_체중\n      &lt;dbl&gt;         &lt;dbl&gt;\n1   643131.          802.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\n\n펭귄 체중 분산: 643131.0773267478\n\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n펭귄 체중 표준편차: 801.9545356980955\n\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n\n# A tibble: 3 × 2\n  species   빈도수\n  &lt;fct&gt;      &lt;int&gt;\n1 Adelie       152\n2 Gentoo       124\n3 Chinstrap     68\n\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies\n\n     species  빈도수\n0     Adelie  152\n1     Gentoo  124\n2  Chinstrap   68"
  },
  {
    "objectID": "statistics.html#가능성",
    "href": "statistics.html#가능성",
    "title": "\n9  통계\n",
    "section": "\n9.3 가능성",
    "text": "9.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n9.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"경남\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"전남\" \"충남\" \"충남\" \"인천\" \"전북\" \"전남\" \"충남\" \"경북\" \"광주\" \"부산\"\n[11] \"전북\" \"충남\" \"부산\" \"광주\" \"울산\" \"서울\" \"세종\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.05882353\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0535\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n광주\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n대전\n경기\n강원\n울산\n충남\n전남\n충남\n충남\n광주\n경북\n서울\n경기\n서울\n경기\n경북\n인천\n경기\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.11764705882352941\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0577\n\n\n\n\n\n\n9.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}  Pr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\  &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)  \\end{aligned}\\)\n\n9.3.2.1 넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n\n[1] 0.1300448\n\nmean(서건창 | 이정후)\n\n[1] 0.6098655\n\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n\n[1] 0.6098655\n\n\n두 선수가 동시에 안타를 칠 확률은 0.13이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.61이 된다.\n\n9.3.2.2 200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n\n[1] 0.11804\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n\n[1] 0.1189903\n\n\n\n9.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n\n9.3.3.1 두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\n\n9.3.3.2 R 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n\n[1] 5.0183\n\nmean(binom_df$y)\n\n[1] 10.0214\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n\n# A tibble: 1 × 1\n  mean_z\n   &lt;dbl&gt;\n1   15.0"
  },
  {
    "objectID": "statistics.html#footnotes",
    "href": "statistics.html#footnotes",
    "title": "\n9  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎"
  },
  {
    "objectID": "statistics.html#확률",
    "href": "statistics.html#확률",
    "title": "\n9  통계\n",
    "section": "\n9.4 확률",
    "text": "9.4 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"충북\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"대전\" \"대전\" \"울산\" \"세종\" \"인천\" \"대전\" \"세종\" \"세종\" \"충북\" \"강원\"\n[11] \"인천\" \"전북\" \"충남\" \"강원\" \"대구\" \"제주\" \"경기\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0605\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n울산\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n세종\n충남\n경북\n서울\n전북\n경기\n전남\n전남\n강원\n전북\n경기\n세종\n강원\n서울\n경기\n세종\n부산\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.0\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0624"
  },
  {
    "objectID": "statistics.html#분포",
    "href": "statistics.html#분포",
    "title": "\n9  통계\n",
    "section": "\n9.4 분포",
    "text": "9.4 분포"
  },
  {
    "objectID": "basic_stat.html#통계-분야",
    "href": "basic_stat.html#통계-분야",
    "title": "\n9  통계\n",
    "section": "\n9.1 통계 분야",
    "text": "9.1 통계 분야\n데이터가 주어지면 데이터를 기술하는 통계와 추론하는 두가지 영역으로 나눠진다. 데이터를 기술하는 통계를 기술통계(Descriptive Statistics), 데이터에서 추론하는 통계를 추론통계(Inferential Statistics)로 지칭한다."
  },
  {
    "objectID": "basic_stat.html#기술통계",
    "href": "basic_stat.html#기술통계",
    "title": "\n9  통계\n",
    "section": "\n9.2 기술통계",
    "text": "9.2 기술통계\n데이터가 수집되어 준비되면 먼저 수집된 데이터의 자료형에 맞춰 요약을 해야한다. 크게 보면 자료형은 범주형과 숫자형으로 나눠지고 숫자형과 범주형에 관계 없이 중심과 퍼짐을 요약하는 측도가 필요하다.\n\n\n\n\ngraph LR\n\n  DescirptiveStat[\"기술통계\"] --&gt; DataType[\"자료형\"]\n  DataType --&gt; CategoricalData[\"범주형\"]\n  DataType --&gt; NumericData[\"숫자형\"]\n  CategoricalData --&gt; MeasureCenter[\"중심\"]\n  NumericData --&gt; MeasureCenter[\"중심\"]\n  MeasureCenter --&gt; MeasureSpread[\"퍼짐\"]\n  \n  style DescirptiveStat fill:#f5d06c,stroke:#333,stroke-width:3px\n  style DataType fill:#f9d0c4,stroke:#333,stroke-width:3px\n  style CategoricalData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style NumericData  fill:#c6def1,stroke:#333,stroke-width:3px\n  style MeasureCenter fill:#e1d5e7,stroke:#333,stroke-width:3px\n  style MeasureSpread fill:#e1d5e7,stroke:#333,stroke-width:3px  \n\n\n\n\n\npalmerpenguins 패키지에 포함된 penguins 데이터셋에 숫자형과 범주형 변수가 포함되어 있어 이를 바탕으로 기술통계에 대한 사례를 살펴보자. dplyr 패키지 glimpse() 함수를 사용해서 penguins 데이터셋을 살펴보자. penguins 데이터셋은 총 8개의 열과 344개의 행으로 구성되어 있으며 다음과 같은 변수들이 포함되어 있다.\n\n\nspecies: 펭귄의 종을 나타내는 범주형 변수입니다.\n\nisland: 펭귄이 서식하는 섬의 이름을 나타내는 범주형 변수입니다.\n\nbill_length_mm: 펭귄의 부리 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbill_depth_mm: 펭귄의 부리 깊이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nflipper_length_mm: 펭귄의 플리퍼(날개) 길이를 밀리미터 단위로 나타내는 수치형 변수입니다.\n\nbody_mass_g: 펭귄의 체중을 그램 단위로 나타내는 수치형 변수입니다.\n\nsex: 펭귄의 성별을 나타내는 범주형 변수입니다.\n\nyear: 관찰 년도를 나타내는 수치형 변수입니다.\n\n\n\nR\n파이썬\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\npenguins = load_penguins()\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n\n\n\n9.2.1 중심 측도\n&lt;fct&gt; 변수는 범주형 자료형이고, &lt;dbl&gt;, &lt;int&gt; 변수는 숫자형 자료형을 나타내고 있다. 각 자료형에 맞춰 데이터를 요약해보자. 먼저 숫자형과 범주형 중심을 각각 나타내는 대표측도로 숫자형은 평균(mean() 함수), 범주형은 최빈수(mode() 함수)로 펭귄 체중의 평균과 펭귄 중 최빈종을 각각 계산해보자. R에 최빈값에 대한 내장함수가 없어 사용자 정의함수를 작성해서 별도 계산한다.\n\n\nR\n파이썬\n\n\n\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\npenguins |&gt; \n  summarise(평균_체중 = mean(body_mass_g, na.rm = TRUE),\n            최빈종    = mode(species))\n\n# A tibble: 1 × 2\n  평균_체중 최빈종\n      &lt;dbl&gt; &lt;fct&gt; \n1     4202. Adelie\n\n\n\n\n\nimport statistics\n\nmean_body_mass_g = penguins['body_mass_g'].mean(skipna=True)\nmode_species = penguins['species'].mode()[0]\n\nprint(f'펭귄 체중: {mean_body_mass_g}')\n\n펭귄 체중: 4201.754385964912\n\nprint(f'펭귄 최빈종: {mode_species}')\n\n펭귄 최빈종: Adelie\n\n\n\n\n\n\n9.2.2 퍼짐 측도\n숫자형 변수의 경우 다양한 퍼짐을 측정하는 통계량이 존재한다. 변수의 퍼짐을 측정할 때 최대값에서 최소값을 뺀 통계량도 퍼짐을 측정하는 의미있는 지표가 된다. 그외에도 분위수를 기준으로 4분위수를 사용하여 IQR를 계산하거나 상자그림(Box-Plot)에서 분위수에 1.5배수를 곱해 분포의 상하한을 지정한 후 그 범위를 벗어나는 관측점을 이상점으로 정의하는 방법도 있다. 가장 퍼짐을 측정하는 일반적인 방법은 분산과 표준편차를 혹은 MAD가 있다.\n\n\n\n\n남극에 서식하는 펭귄 체중에 대한 분산과 표준편차를 var(), sd() 함수를 사용해서 계산할 수 있다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  summarise(분산_체중     = var(body_mass_g, na.rm = TRUE),\n            표준편차_체중 = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  분산_체중 표준편차_체중\n      &lt;dbl&gt;         &lt;dbl&gt;\n1   643131.          802.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n분산_체중 = penguins['body_mass_g'].var(skipna=True)\n표준편차_체중 = penguins['body_mass_g'].std(skipna=True)\n\nprint(f'펭귄 체중 분산: {분산_체중}')\n\n펭귄 체중 분산: 643131.0773267478\n\nprint(f'펭귄 체중 표준편차: {표준편차_체중}')\n\n펭귄 체중 표준편차: 801.9545356980955\n\n\n\n\n\n범주형 변수에 대한 퍼짐의 측도도 존재하지만 범주형 변수를 구성하는 범주에 크기가 존재하지 않기 때문에 빈도수를 구해서 살펴보는 것이 일반적이다.\n\n\nR\n파이썬\n\n\n\n\npenguins |&gt; \n  count(species, sort = TRUE, name = \"빈도수\")\n\n# A tibble: 3 × 2\n  species   빈도수\n  &lt;fct&gt;      &lt;int&gt;\n1 Adelie       152\n2 Gentoo       124\n3 Chinstrap     68\n\n\n\n\n\nfrequencies = penguins['species'].value_counts().reset_index()\nfrequencies.columns = ['species', '빈도수']\n\nfrequencies\n\n     species  빈도수\n0     Adelie  152\n1     Gentoo  124\n2  Chinstrap   68"
  },
  {
    "objectID": "basic_stat.html#가능성",
    "href": "basic_stat.html#가능성",
    "title": "\n9  통계\n",
    "section": "\n9.3 가능성",
    "text": "9.3 가능성\n가능성, 승산, 예상, 전망, 형세 등 다양한 방식으로 미래를 알고자 하는 인간의 호기심이 집대성된 수학의 한분야가 확률이다.\n\n9.3.1 확률\n대한민국 시도수는 특별시, 광역시, 자치도 등 포함하여 총 17개가 존재한다. 17개 시도 중 시도 하나를 무작위로 뽑게 되면 확률이 얼마나 될까? 먼저, 17개 시도에서 “서울”을 고를 경우 \\(\\frac{1}{17} = 0.05882353\\) 으로 계산된다.\n자연어로 작성된 텍스트를 수학적으로 좀더 엄밀하게 표현하면 다음과 같다.\n시도의 수 \\(n\\)과 선택할 시도의 수 \\(r\\)을 조합의 수식으로 표현하면,\n\\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\n여기서 시도의 수 \\(n\\)은 17이고, 선택할 시도의 수 \\(r\\)은 1이다. 따라서 조합의 수식에 대입하면,\n\\[\nC(17, 1) = \\frac{17!}{1!(17-1)!} = 17\n\\]\n이는 17개의 시도 중에서 하나를 선택하는 모든 가능한 방법이 17가지라는 것을 의미한다.\n그런데 ’서울’이 선택될 확률을 구하려면, ’서울’이 선택될 경우의 수 1를 전체 경우의 수 17로 나누면 된다.\n\\[\nP(\\text{'서울'}) = \\frac{1}{C(17, 1)} = \\frac{1}{17} \\approx 0.05882353\n\\]\n따라서, ’서울’이 선택될 확률은 약 0.05882353 또는 약 5.9%다.\nR과 파이썬을 사용해서 모의시험을 다수 수행하게 되면 동일한 결과를 얻을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nsidoNM &lt;- c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\")\n\n# 17개 시도에서 무작위로 시도 하나 추출\nsample(sidoNM, 1)\n\n[1] \"충남\"\n\n# 상기 과정을 17회 반복\nreplicate(17, sample(sidoNM, 1))\n\n [1] \"전북\" \"강원\" \"광주\" \"광주\" \"경기\" \"광주\" \"경남\" \"충남\" \"대전\" \"경북\"\n[11] \"경남\" \"충남\" \"울산\" \"부산\" \"경남\" \"세종\" \"전북\"\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nreplicate(17, sample(sidoNM, 1) == \"서울\") |&gt; mean()\n\n[1] 0.1176471\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ncalculate_prob &lt;- function(trials = 1000, sido_name = \"서울\") {\n  prob &lt;- (replicate(trials, sample(sidoNM, 1)) == sido_name) |&gt; mean()\n  return(prob)\n}\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\ncalculate_prob(10000, \"제주\")\n\n[1] 0.0596\n\n\n\n\n\nimport random\nimport numpy as np\n\nsidoNM = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"경기\", \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\"]\n\n# 17개 시도에서 무작위로 시도 하나 추출\nprint(random.choice(sidoNM))\n\n제주\n\n# 상기 과정을 17회 반복\nfor _ in range(17):\n    print(random.choice(sidoNM))\n\n부산\n인천\n인천\n강원\n전북\n강원\n부산\n강원\n충북\n충북\n대전\n울산\n제주\n전남\n대전\n전남\n서울\n\n# 17회 시도명을 추출한 결과 \"서울\"이 나올 경우를 평균 냄\nprint(np.mean([random.choice(sidoNM) == \"서울\" for _ in range(17)]))\n\n0.0\n\n# 반복횟수와 시도명을 달리한 모의시험 함수 제작\ndef calculate_prob(trials = 1000, sido_name = \"서울\"):\n    prob = np.mean([random.choice(sidoNM) == sido_name for _ in range(trials)])\n    return prob\n\n# 17개 시도 중 \"제주\"가 나올 확률을 1만번 반복함\nprint(calculate_prob(10000, \"제주\"))\n\n0.0616\n\n\n\n\n\n\n9.3.2 확률의 덧셈법칙\n두 사건 중 적어도 하나만 발생할 확률을 “또는(or)” 연산으로 표현할 수 있는데, 두 사건이 독립을 가정(\\(Pr(\\text{A and B}) = Pr(A) \\times Pr(B)\\))하면 다음과 같이 표현할 수 있다. 이를 확률의 덧셈법칙(addition law of probability)이라고 부른다.\n\\(\\begin{aligned}  Pr(\\text{A or B}) &= Pr(A) + Pr(B) - Pr(\\text{A and B})\\\\  &= Pr(A) + Pr(B) - Pr(A) \\times Pr(B)  \\end{aligned}\\)\n\n9.3.2.1 넥슨 두 타자\n타자의 정규타석은 소속팀의 경기수 \\(\\times\\) 3.1로 정의된다. 보통 한 경기에서 타자가 4 혹은 5번 타석에 들어서고 슬럼프 등으로 2군에 내려가는 것을 감안하여 붙박이 주전급 선수를 가늠하는 기준이다. 이제 넥슨의 수위 타자 두명을 놓고 두 선수가 매번 타석에서 안타를 치거나 두 선수 중 한 선수가 안타를 치는 확률을 확률의 덧셈법칙을 통해 확인해 보자. 규정타석을 446 타석으로 놓고 안타를 1, 범타를 0으로 정해놓고 2017년 7월 21일 기준 타율 데이터를 참조한다.\n\n# 1. 넥센 두 타자 ------------\n서건창 &lt;- rbinom(446, 1, 0.344)\n이정후 &lt;- rbinom(446, 1, 0.333)\n\n# 두선수가 모두 안타를 칠 확률\nmean(서건창 & 이정후)\n\n[1] 0.1188341\n\nmean(서건창 | 이정후)\n\n[1] 0.5358744\n\nmean(서건창==1) + mean(이정후==1) - mean(서건창&이정후)\n\n[1] 0.5358744\n\n\n두 선수가 동시에 안타를 칠 확률은 0.12이 되고, 두 선수 중 적어도 한 선구가 안타를 칠 확률은 0.54이 된다.\n\n9.3.2.2 200 안타를 향해서\n충분한 타석(540)이 주어졌다고 가정하고 200안타 이상을 때릴 확률은 얼마나 될까? \\(\\frac{200}{540}\\) = 0.3703704 단순 계산해도 3할 7푼이 넘는 고타율이다.\n서건창, 이정후 선수를 100,000번 KBO 시즌을 돌리는데 540번 타석에 세워 타율은 현재 타율이라고 가정한다. 이런 가정을 두고 두 선수 중 적어도 한 선수가 200 안타를 칠 확률은 얼마나 될까? 이를 풀기 위해서 한번은 난수를 발생하여 모의실험으로 계산하고, 다른 한번은 누적이항분포 확률을 사용해서 계산한다.\n\n# 2. 200 안타  ------------\n서건창 &lt;- rbinom(100000, 540, 0.344)\n이정후 &lt;- rbinom(100000, 540, 0.333)\n\n# 두선수 중 적어도 한 선수가 200 안타를 칠 확률\nmean(서건창 &gt; 200 | 이정후 &gt; 200)\n\n[1] 0.12042\n\n서건창_확률 &lt;- 1 - pbinom(200, 540, 0.344)\n이정후_확률 &lt;- 1 - pbinom(200, 540, 0.333)\n\n서건창_확률 + 이정후_확률 - 서건창_확률*이정후_확률\n\n[1] 0.1189903\n\n\n\n9.3.3 확률변수 합\n두 확률변수를 곱하거나 더하는 경우를 생각할 수 있다. 이항분포에서 나온 두 변수를 더하여 만들어진 새로운 변수는 어떤 특성을 갖게 되는지 살펴보자\n\\[Z \\sim X + Y\\] 여기서, \\(X \\sim \\text{이항분포}(n, p)\\)를 따르고, \\(Y \\sim \\text{이항분포}(m, p)\\)를 따를 때 두 확률변수를 합한 \\(Z\\)를 살펴보자. 1\n\\[X + Y \\sim \\text{이항분포}(n+m, p)\\]\n\n9.3.3.1 두 변수 합에 대한 기대값과 분산\n독립인 두 변수에 대한 기대값과 분산은 다음과 같다.\n\n\\(E[X+Y] = E[X] + E[Y]\\)\n\\(Var[X+Y] = Var[X] + Var[Y]\\)\n\n9.3.3.2 R 코드을 통한 시각화 및 수치 검정\n\\(X \\sim \\text{이항분포}(n, p) = \\text{이항분포}(10, 0.5)\\)로 두고, \\(Y \\sim \\text{이항분포}(m, p) = \\text{이항분포}(20, 0.5)\\)을 놓게 되면, 이론적으로 \\(X+Y \\sim \\text{이항분포}(n+m, p) = \\text{이항분포}(10+20, 0.5)\\)이 되고, rbinom함수를 통해 난수를 만들고, 두 변수를 합하여 시각화한다.\n\nbinom_df &lt;- tibble(x = rbinom(10000, 10, 0.5),\n                       y = rbinom(10000, 20, 0.5))\n\nbinom_df %&gt;% \n  mutate(z = x + y) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"binom_dist\", values_to = \"cnt\") |&gt; \n  ggplot(aes(x = cnt, y = cnt, color=binom_dist)) +\n    geom_bar(stat=\"identity\") +\n    facet_wrap(~binom_dist, nrow=3) +\n    theme_bw(base_family = \"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"성공횟수\", y=\"빈도수\", title=\"두 확률변수의 합\")\n\n\n\n\n\n\n\n이론값과 난수를 생성하여 두 변수를 합한 결과를 비교한다.\n\nmean(binom_df$x)\n\n[1] 4.9945\n\nmean(binom_df$y)\n\n[1] 10.023\n\nbinom_df %&gt;% mutate(z = x + y) %&gt;% \n  summarise(mean_z = mean(z))\n\n# A tibble: 1 × 1\n  mean_z\n   &lt;dbl&gt;\n1   15.0"
  },
  {
    "objectID": "basic_stat.html#분포",
    "href": "basic_stat.html#분포",
    "title": "\n9  통계\n",
    "section": "\n9.4 분포",
    "text": "9.4 분포"
  },
  {
    "objectID": "basic_stat.html#footnotes",
    "href": "basic_stat.html#footnotes",
    "title": "\n9  통계\n",
    "section": "",
    "text": "Sum of two independent binomial variables↩︎"
  },
  {
    "objectID": "intermediate_stat.html#밀린_병원비_추정",
    "href": "intermediate_stat.html#밀린_병원비_추정",
    "title": "\n10  표본 추출\n",
    "section": "\n10.1 병원비 추정",
    "text": "10.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\n\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n\n# A tibble: 1 × 2\n  amount_est amount_var\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       40.9       35.7\n\n\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\n\n\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다."
  },
  {
    "objectID": "intermediate_stat.html#basic-concept",
    "href": "intermediate_stat.html#basic-concept",
    "title": "\n10  표본 추출\n",
    "section": "\n10.2 표본추출",
    "text": "10.2 표본추출\n\n10.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n\nRows: 1,338\nColumns: 8\n$ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n$ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n$ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n$ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n$ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n$ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n$ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n$ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n\n\n\n\n10.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   total_cup_points species coo          farm_name aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             84.2 Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.92        10\n 2             87.2 Arabica Mexico       la herra…  8.17  7.83    8.17        10\n 3             84   Arabica Colombia     &lt;NA&gt;       7.67  7.67    7.75        10\n 4             80.7 Arabica Costa Rica   gamboa     7.83  7.83    7.5         10\n 5             81.8 Arabica United Stat… kona pac…  7.25  7.75    7.75        10\n 6             81.1 Arabica United Stat… hacienda…  7.67  7.58    7.67        10\n 7             78.5 Arabica Mexico       various    7.25  7.25    7.58        10\n 8             86.2 Arabica Ethiopia     haider a…  8     8.08    8.08        10\n 9             78.6 Arabica Mexico       cofradia   7.92  7.92    7.5         10\n10             84.7 Arabica Peru         &lt;NA&gt;       7.67  7.92    7.83        10\n\n\n\n10.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n\n# A tibble: 10 × 9\n   rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n 2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n 3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n 4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n 5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n 6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n 7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n 8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n 9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n\n# A tibble: 3 × 9\n  rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n  &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1   446             85   Arabica Brazil  sitío sã…  8     7.67    7.67     10   \n2   892             83.1 Arabica Colomb… &lt;NA&gt;       7.83  7.42    7.92     10   \n3  1338             83.7 Arabica United… &lt;NA&gt;       7.75  7.75    8         9.33\n\n\n\n10.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n\n# A tibble: 96 × 8\n# Groups:   coo [37]\n   total_cup_points species coo      farm_name     aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             82.8 Arabica Brazil   capoeirinha    7.67  7.5     7.42        10\n 2             82.2 Arabica Brazil   caxambu        7.67  7.5     7.58        10\n 3             83   Arabica Brazil   fazenda sant…  7.67  7.42    7.5         10\n 4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n 5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n 6             82.3 Arabica China    yun lan coff…  7.5   7.42    7.42        10\n 7             83.6 Arabica China    menglian man…  7.67  7.67    7.75        10\n 8             84.5 Arabica China    puer jiangch…  7.67  7.75    7.58        10\n 9             83.8 Arabica Colombia &lt;NA&gt;           7.83  7.58    7.58        10\n10             83.9 Arabica Colombia &lt;NA&gt;           7.75  7.75    7.92        10\n# ℹ 86 more rows\n\n\n\n10.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n\n# A tibble: 5 × 8\n  total_cup_points species coo    farm_name   aroma  body balance sweetness\n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1             82.3 Arabica Brazil &lt;NA&gt;         7.42  7.5     7.58        10\n2             83.2 Arabica Brazil água limpa   7.75  7.58    7.58        10\n3             82.7 Arabica Brazil santa maria  7.33  7.67    7.5         10\n4             81.8 Arabica Brazil rio verde    7.17  7.75    7.25        10\n5             83   Arabica Brazil capoeirinha  7.5   7.58    7.42        10"
  },
  {
    "objectID": "intermediate_stat.html#basic-concept-comparison",
    "href": "intermediate_stat.html#basic-concept-comparison",
    "title": "\n10  표본 추출\n",
    "section": "\n10.3 표본추출 비교",
    "text": "10.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n10.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n\n[1] 82.1512\n\n\n\n10.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.30519\n\n\n\n10.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n\n[1] 82.0522\n\n\n\n10.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n\n[1] 80.54842"
  },
  {
    "objectID": "intermediate_stat.html#calculate-errors",
    "href": "intermediate_stat.html#calculate-errors",
    "title": "\n10  표본 추출\n",
    "section": "\n10.4 오차 측정",
    "text": "10.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n\n# A tibble: 1 × 4\n  population   srs stratifed cluster\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1       82.2  82.3      82.1    80.5\n\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n\n# A tibble: 4 × 3\n  method     estimation relative_error\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 population       82.2          0    \n2 srs              82.3          0.187\n3 stratifed        82.1          0.121\n4 cluster          80.5          1.95 \n\n\n\n10.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.05226\n\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n\n  [1] 82.23203 82.41060 82.21902 82.15038 82.29481 82.61955 82.33789 81.64466\n  [9] 82.00895 82.25940 82.59985 81.84677 81.83910 82.09511 82.33707 82.22308\n [17] 81.94211 82.07850 82.44759 82.28609 82.47602 82.29060 82.15789 82.28947\n [25] 82.37865 82.30263 82.39188 82.19263 82.41226 82.20165 82.00233 82.06647\n [33] 81.71241 82.31361 82.33263 82.12023 82.30188 82.04744 82.28105 82.25752\n [41] 81.94684 82.06068 81.79880 82.23647 82.84211 82.37203 82.11406 82.34000\n [49] 82.43624 82.43113 81.66158 82.08346 82.09992 82.02774 82.20556 82.19820\n [57] 81.96895 82.08602 81.92992 82.36120 81.81812 82.13662 82.21383 82.06173\n [65] 82.46594 81.91632 81.99331 82.44113 82.54376 82.45444 82.28075 82.45617\n [73] 82.29474 81.62850 82.06173 82.11466 82.46617 82.67143 82.32647 82.14150\n [81] 82.11962 82.51925 82.13797 81.84293 82.01639 82.13489 82.23579 81.99158\n [89] 82.63880 82.22729 81.98158 82.50925 82.44744 82.18368 82.33323 82.34098\n [97] 81.94985 81.91188 81.90188 82.25722\n\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n\n  [1] 81.49827 82.33940 82.18970 82.12195 82.59744 82.16805 82.35135 81.98383\n  [9] 82.26707 82.36812 81.96436 81.90556 82.08165 82.32586 82.35571 82.38820\n [17] 82.05165 82.19459 82.09767 82.12887 82.36030 82.38549 82.05414 82.28241\n [25] 82.14211 81.90015 81.96211 81.89902 81.66774 82.13444 81.87188 82.52895\n [33] 82.53632 82.27797 82.39308 82.19759 81.57737 82.25850 82.27774 82.21090\n [41] 82.25195 82.06579 81.95180 82.03789 81.62083 82.03504 82.09218 82.25459\n [49] 82.00113 82.26617 82.11286 82.12361 82.17564 82.15398 82.36008 82.08233\n [57] 81.94015 82.17398 82.30414 81.99346 82.06218 82.37068 81.94376 82.34105\n [65] 81.77752 82.43925 81.79556 82.13827 82.44737 81.85797 82.46135 81.90444\n [73] 81.79782 82.13180 82.29594 82.19541 81.85301 82.09910 82.43436 82.13872\n [81] 82.13075 82.05331 82.33180 82.42278 82.02865 82.52143 81.66895 81.83947\n [89] 82.41812 81.82729 82.25180 81.97353 82.08218 82.31699 82.13692 81.71083\n [97] 82.18338 82.00955 82.12308 81.98767\n\n\n\n10.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n10.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n\n# A tibble: 5 × 3\n  samp_prop mean_cup_points sd_cup_points\n  &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n1 10 %                 82.1        0.212 \n2 33 %                 82.2        0.111 \n3 50 %                 82.1        0.0783\n4 75 %                 82.1        0.0404\n5 90 %                 82.1        0.0266"
  },
  {
    "objectID": "intermediate_stat.html#calculate-bootstrap",
    "href": "intermediate_stat.html#calculate-bootstrap",
    "title": "\n10  표본 추출\n",
    "section": "\n10.5 신뢰구간",
    "text": "10.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n10.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n\n[1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n\n[1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n\n[1] 2.624232\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n\n[1] 2.488636\n\n\n\n10.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  82.0  82.4  82.8"
  },
  {
    "objectID": "intermediate_stat.html#footnotes",
    "href": "intermediate_stat.html#footnotes",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎"
  },
  {
    "objectID": "sampling.html#밀린_병원비_추정",
    "href": "sampling.html#밀린_병원비_추정",
    "title": "\n10  표본 추출\n",
    "section": "\n10.1 병원비 추정",
    "text": "10.1 병원비 추정\n모집단이 1,000명(\\(N=1,000\\)) 환자 중에서 임의표본추출로 200명(\\(n=200\\))을 뽑았다. 병원에 484계정(\\(N=484\\)) 중에서 9계정(\\(n=9\\))을 임의표본추출 방식으로 뽑아서, 평균적으로 얼마의 병원비가 밀렸는지 추정하고자 한다. 이를 위해서 표본 9명을 뽑아서 밀린 평균 병원비를 조사해 보니 다음과 같다. 1\n\nlibrary(tidyverse)\nhosp_df &lt;- tribble(\n    ~account, ~amount,\n\"y1\", 33.50,\n\"y2\", 32.00,\n\"y3\", 52.00,\n\"y4\", 43.00,\n\"y5\", 40.00,\n\"y6\", 41.00,\n\"y7\", 45.00,\n\"y8\", 42.50,\n\"y9\", 39.00)\n\nhosp_df %&gt;% \n    summarise(amount_est = mean(amount),\n              amount_var = var(amount))\n\n# A tibble: 1 × 2\n  amount_est amount_var\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       40.9       35.7\n\n\n이를 바탕으로 밀린 평균병원비를 추정해 보자. 계정 9개로부터 나온 평균 병원비는 다음 공식으로 통해서 계산이 가능하다.\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{9}y_i}{9} = \\frac{368}{9} = 40.89 \\]\n추정치의 오차를 계산하기 위해서 표본분산을 다음과 같이 먼저 계산한다.\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} {n-1} = 35.67 \\]\n그리고 나서, 추정오차의 한계를 다음과 같이 구한다.\n\n\n\\(N\\): 1,000\n\n\\(n\\): 200\n\n\\(s^2\\): 445.21\n\n\\[2 \\sqrt{\\hat{V}(\\bar{y})} = 2 \\sqrt{(1- \\frac{n}{N}) \\frac{s^2}{n}} = 2 \\sqrt{(1- \\frac{200}{1000}) \\frac{445.21}{200}} = 3.94\\]\n따라서 평균적으로 밀린 병원비 \\(\\mu = 40.89\\)으로 산출되는데 \\(\\mu\\)가 \\(\\bar{y}\\)와의 얼마나 가까운가를 나타내는 추정오차는 3.94로 산출해낼 수 있다.\n\n\n\n\n\n\n비복원 추출 때문에 발생되는 분산의 감소량을 FPC(유한모집단수정, Finite population correction)로 나타내는데, 복원추출인 경우 FPC는 없어지고, 모집단 \\(N\\)이 매우 커서 \\(n\\)이 작은 경우 \\(\\frac{n}{N} \\approx 0\\), 따라서, \\(1 - \\frac{n}{N} \\approx 1\\)이 된다."
  },
  {
    "objectID": "sampling.html#basic-concept",
    "href": "sampling.html#basic-concept",
    "title": "\n10  표본 추출\n",
    "section": "\n10.2 표본추출",
    "text": "10.2 표본추출\n\n10.2.1 커피 데이터\nqacData 팩키지에 커피 리뷰 데이터가 포함되어 있다. coffee 데이터는 Coffee Quality Institute Database 에서 1312 아라비카 커피콩을 스크래핑하여 구축되었으며 수많은 리뷰어가 커피맛을 보고 평가를 내린 평점도 포함되어 있다. Tidy Tuesday에도 커피맛 평가 데이터가 동일하게 이용가능하다.\n커피콩은 커피나무의 씨앗이며 음용 커피의 재료로 사용되는데 경제적으로 가장 중요한 커피나무의 두 종으로는 아라비카와 로부스타가 있으며 전 세계에서 생산되는 커피 중 75~80%가 아라비카이고, 20%가 로부스타라는 통계가 있다. 2\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee &lt;- tuesdata$coffee_ratings\n\ncoffee_df &lt;- coffee %&gt;% \n  select(total_cup_points, species, coo = country_of_origin, farm_name, aroma, body, balance, sweetness) %&gt;% \n  filter(total_cup_points &gt; 50)\n\nglimpse(coffee_df)\n\nRows: 1,338\nColumns: 8\n$ total_cup_points &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75, 88.6…\n$ species          &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\"…\n$ coo              &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\", \"Eth…\n$ farm_name        &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas \\\"san…\n$ aroma            &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, 8.67,…\n$ body             &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, 8.33,…\n$ balance          &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, 8.42,…\n$ sweetness        &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 9.33…\n\n\n\n\n\n\n\n10.2.2 단순 임의추출\n단순 임의추출(Simple Random Sampling)은 모집단(Population) 혹은 표본 틀(Sampling Frame)에서 임의 방식으로 표본을 추출하는 것이다. dplyr 팩키지의 slice_sample() 함수를 사용하면 표본추출관련 대부분의 기능을 이를 통해서 구현할 수 있다. 먼저 앞서 전세계 커피 품종에 대한 평가를 담을 데이터를 모집단으로 가정하고 임의로 10개를 커피콩을 추출해보자.\n\ncoffee_df %&gt;% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   total_cup_points species coo        farm_name   aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             83.3 Arabica Costa Rica several      7.67  7.67    7.67        10\n 2             82.7 Arabica Guatemala  finca medi…  7.67  7.5     7.5         10\n 3             78.5 Arabica Mexico     various      7.25  7.25    7.58        10\n 4             85.4 Arabica Brazil     fazenda se…  8     7.75    8           10\n 5             78.3 Arabica Guatemala  various      7.17  7.17    7.17        10\n 6             81.7 Arabica Mexico     &lt;NA&gt;         7.67  7.5     7.25        10\n 7             83.2 Arabica Guatemala  finca medi…  7.5   7.58    7.5         10\n 8             83.3 Arabica Colombia   &lt;NA&gt;         7.5   7.67    7.58        10\n 9             83.5 Arabica Guatemala  santo toma…  7.83  7.67    7.5         10\n10             83.2 Arabica Guatemala  nueva gran…  7.33  7.33    7.67        10\n\n\n\n10.2.3 계통추출법\n계통추출법(systematic sampling)은 첫 번째 요소는 무작위로 선정한 후, 목록의 매번 k번째 요소를 표본으로 선정하는 표집방법이다. 모집단의 크기를 원하는 표본의 크기로 나누어 k를 계산한다. 이를 R을 통해 구현해보자.\n표본크기(sample_size) 10개를 추출한다. 이를 위해서 먼저 전체 표본 크기를 구한 후에 모집단을 표본크기로 나누는데 정수를 구해 간격(interval) k로 정하고 이를 행번호로 특정한 후에 slice 함수로 표본 추출한다.\n\nsample_size &lt;- 10\npopulation_size &lt;- nrow(coffee_df)\ninterval_k &lt;- population_size %/% sample_size\n\nrow_index &lt;- seq_len(sample_size) * interval_k\n\ncoffee_df %&gt;% \n  rowid_to_column() %&gt;% \n  slice(row_index)\n\n# A tibble: 10 × 9\n   rowid total_cup_points species coo    farm_name aroma  body balance sweetness\n   &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1   133             84.7 Arabica Costa… finca sa…  7.67  7.67    8.58     10   \n 2   266             83.9 Arabica Taiwan very fam…  7.75  7.75    7.67     10   \n 3   399             83.3 Arabica Colom… &lt;NA&gt;       7.83  7.5     7.58     10   \n 4   532             83   Arabica Nicar… santa ro…  7.58  7.92    8.08      9.33\n 5   665             82.5 Arabica Colom… &lt;NA&gt;       7.5   7.58    7.58     10   \n 6   798             82   Arabica Tanza… family f…  7.67  7.42    7.42     10   \n 7   931             81.5 Arabica Brazil sertao     7.5   7.5     7.33     10   \n 8  1064             80.6 Arabica Brazil rio verde  7.42  7.08    7.25     10   \n 9  1197             79.1 Arabica Mexico el desmo…  7.17  7.25    7.08     10   \n10  1330             80.5 Robusta Uganda mannya c…  7.75  7.67    7.58      7.67\n\n\n상기 결과를 바탕으로 계통표본추출법으로 표본을 추출하는 함수를 제작해서 원하는 만큼 표본을 추출한다. 하지만, 이런 경우 원데이터가 특정한 규칙을 내포한 경우 편의가 생길 수 있어 이를 보정하는 로직도 함께 넣어 둔다. slice_sample(prop = 1) 명령어는 데이터프레임을 마구 뒤섞어 혹시 생길 수 있는 편의를 제거하는 역할을 한다.\n\nsystematic_sampling &lt;- function(sample_size = 10) {\n  \n  sample_size &lt;- sample_size\n  population_size &lt;- nrow(coffee_df)\n  interval_k &lt;- population_size %/% sample_size\n  \n  row_index &lt;- seq_len(sample_size) * interval_k\n  \n  systematic_sample &lt;- coffee_df %&gt;% \n    slice_sample(prop = 1) %&gt;% \n    rowid_to_column() %&gt;% \n    slice(row_index)  \n  \n  return(systematic_sample)\n}\n\nsystematic_sampling(3)\n\n# A tibble: 3 × 9\n  rowid total_cup_points species coo     farm_name aroma  body balance sweetness\n  &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1   446             81.2 Arabica Hondur… cerro bu…  7.42  7.33    7.25        10\n2   892             84.5 Arabica Mexico  finca el…  7.92  7.75    7.83        10\n3  1338             82.2 Arabica Hondur… bethel     7.58  7.42    7.42        10\n\n\n\n10.2.4 층화추출법\n층화추출법(Stratified sampling)은 모집단을 먼저 중복되지 않도록 층으로 나눈 다음 각 층에서 표본을 추출하는 방법으로, 필요에 따라 각 층을 다시 하위층으로 나누어 추출하는 다단계 층화 추출을 하기도 한다. dplyr 팩키지를 사용할 경우 slice_sample() 함수를 하위 모집단 그룹, 즉 층(strata)으로 나눠 group_by()로 묶은 후에 임의 복원 혹은 비복원 임의추출방법을 수행한다. 예를 들어, 원산지 국가를 기준으로 각 국가별로 3개 커피콩 품종을 추출하는 코드는 다음과 같다.\n\ncoffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(n = 3, replace = FALSE) %&gt;% \n  arrange(coo)\n\n# A tibble: 96 × 8\n# Groups:   coo [37]\n   total_cup_points species coo      farm_name     aroma  body balance sweetness\n              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1             78.4 Arabica Brazil   santa maria    6.83  6.92    6.83        10\n 2             81   Arabica Brazil   cachoeira da…  7.33  7.33    7.17        10\n 3             73.5 Arabica Brazil   &lt;NA&gt;           7.25  7       6.42        10\n 4             80.3 Arabica Burundi  &lt;NA&gt;           7.08  7.08    7.08        10\n 5             83.3 Arabica Burundi  sogestal kay…  7.75  7.5     7.75        10\n 6             84.5 Arabica China    puer jiangch…  7.67  7.75    7.58        10\n 7             83.2 Arabica China    chen lin       7.5   7.67    7.67        10\n 8             82.8 Arabica China    man ganna es…  7.67  7.67    7.67        10\n 9             83   Arabica Colombia &lt;NA&gt;           7.58  7.58    7.42        10\n10             83.5 Arabica Colombia &lt;NA&gt;           7.75  7.75    7.5         10\n# ℹ 86 more rows\n\n\n\n10.2.5 집락추출법\n집락추출법(Cluster Sampling)은 모집단에서 집단을 일차적으로 표집한 다음, 선정된 각 집단에서 구성원을 표본으로 추출하는 2단계 표본추출방법으로 다단계 표집방법의 특수한 경우다. 집락 내부는 이질적(heterogeneous)이고 집락 간에는 동질적(homogeneous) 특성을 가지도록 하는 것이 특징으로 이런 특성이 만족되어야 보다 큰 대표성을 갖게 된다.\n집락추출법을 통해 커피콩을 추출하는 방식은 먼저 전세계 국가에서 먼저 임의 국가로 표본크기를 3으로 정해 뽑은 후에 다시 이렇게 특정된 국가를 대상으로 2단계 커피콩 임의 추출작업을 수행한다.\n\n## 1 단계 임의추출\ncountry &lt;- coffee_df %&gt;% \n  count(coo) %&gt;% \n  pull(coo)\n\ncountry_sample &lt;- sample(country, size = 3)\n\n## 2 단계 임의추출\ncoffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(n=5) %&gt;% \n  arrange(coo)\n\n# A tibble: 5 × 8\n  total_cup_points species coo           farm_name aroma  body balance sweetness\n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1             82.1 Arabica Indonesia     various    7.5   7.5     7.42        10\n2             80.7 Arabica Indonesia     darmawi    7.58  7.5     7.5         10\n3             83.3 Arabica United State… &lt;NA&gt;       7.92  7.67    7.67        10\n4             81.5 Arabica United State… &lt;NA&gt;       7.33  7.5     7.83        10\n5             80.3 Arabica United State… &lt;NA&gt;       7.17  7.67    7.33        10"
  },
  {
    "objectID": "sampling.html#basic-concept-comparison",
    "href": "sampling.html#basic-concept-comparison",
    "title": "\n10  표본 추출\n",
    "section": "\n10.3 표본추출 비교",
    "text": "10.3 표본추출 비교\n결국 커피콩 데이터에서 추구하는 바는 total_cup_points를 가능하면 적은 비용과 노력으로 정확히 측정할 수 있도록 표본을 선정하는 것이다.\n\n10.3.1 모집단\n먼저 모집단의 total_cup_points 평균을 구해서 이를 절대값으로 삼아 이야기를 풀어가도록 한다.\n\nmean_population &lt;- coffee_df %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_population\n\n[1] 82.1512\n\n\n\n10.3.2 임의추출법\nslice_sample() 함수를 통해 전체 모집단에서 10%를 임의로 추출하여 동일한 방식으로 total_cup_points 평균을 구해보자.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.13789\n\n\n\n10.3.3 계통추출법\n국가를 층(strata)으로 삼아 각 국가별로 10% 커피콩을 뽑아 total_cup_points 평균을 구해보자.\n\nmean_stratified &lt;- coffee_df %&gt;% \n  group_by(coo) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  ungroup() %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_stratified\n\n[1] 81.95732\n\n\n\n10.3.4 집락추출법\n다음은 전체 국가의 20%를 뽑고 각 국가별로 10%를 임의추출하는 2단계 표본추출법, 즉 집락추출법을 사용해서 total_cup_points 평균을 구해보자.\n\n## 1 단계 임의추출\ncountry_sample &lt;- sample(country, size = length(country) %/% 5)\n\n## 2 단계 임의추출\nmean_cluster &lt;- coffee_df %&gt;% \n  filter(coo %in% country_sample) %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_cluster\n\n[1] 81.65486"
  },
  {
    "objectID": "sampling.html#calculate-errors",
    "href": "sampling.html#calculate-errors",
    "title": "\n10  표본 추출\n",
    "section": "\n10.4 오차 측정",
    "text": "10.4 오차 측정\n모집단 total_cup_points 평균과 비교하여 다양한 표본추출방법에 따라 차이가 나는데 이를 통해 상대 오차를 측정할 필요가 있다.\n\nestimation_df &lt;- tibble(\n  \"population\" = mean_population,\n  \"srs\"        = mean_srs,\n  \"stratifed\"  = mean_stratified,\n  \"cluster\"    = mean_cluster\n)\n\nestimation_df \n\n# A tibble: 1 × 4\n  population   srs stratifed cluster\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1       82.2  82.1      82.0    81.7\n\n\n모집단에서 추정값을 뺀 후 100을 곱해 상대오차(Relative Error)를 구해 추정값의 정확성을 상대적으로 비교할 수 있다.\n\\[\\text{상대오차} = \\frac{ | \\text{모집단 측정값} - \\text{표본추출 추정값} | }{\\text{모집단 측정값}} \\times 100\\]\n\nestimation_df %&gt;% \n  pivot_longer(col = everything(), names_to = \"method\", values_to = \"estimation\") %&gt;% \n  mutate(relative_error = abs(mean_population - estimation) / mean_population * 100)\n\n# A tibble: 4 × 3\n  method     estimation relative_error\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 population       82.2         0     \n2 srs              82.1         0.0162\n3 stratifed        82.0         0.236 \n4 cluster          81.7         0.604 \n\n\n\n10.4.1 반복 표본추출\n임의추출방법을 통해 표본을 한번만 추출하는 대신 원하는 만큼 충분히 반복하고자 하면 어떨까? 먼저 앞서 단순 임의추출방법을 다시 확인해보자. 10%의 표본을 추출하여 total_cup_points 평균을 계산한다.\n\nmean_srs &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n\nmean_srs\n\n[1] 82.19203\n\n\nreplicate() 함수를 사용해서 앞서 정의한 단순 임의추출방법을 원하는 만큼 예를 들어 100회 수행하게 된다. 이를 통해서 10% 표본을 뽑아 total_cup_points 평균을 계산하는 작업을 100회 수행시킬 수 있다.\n\nsrs_100 &lt;- replicate(\n  n = 100,\n  expr = coffee_df %&gt;% \n  slice_sample(prop = 0.1) %&gt;% \n  summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n  pull(mean_cup_points)\n)\n\nsrs_100\n\n  [1] 82.15835 82.59120 82.22406 82.15233 82.01256 82.03602 82.10098 82.01857\n  [9] 82.24218 82.68609 81.95293 81.79617 82.54105 81.91368 82.03451 82.23744\n [17] 82.11105 81.96466 82.57759 82.34887 81.91564 82.15880 82.52820 82.03534\n [25] 82.47654 82.28902 82.07015 82.35301 81.87038 82.16624 82.29248 82.19752\n [33] 82.16459 81.94436 82.00917 82.09278 82.16594 82.21541 81.93000 81.81211\n [41] 82.58090 82.27429 81.93038 81.85150 81.87098 82.40707 82.07865 81.90165\n [49] 81.91060 81.85594 82.25947 82.12556 82.43812 82.17023 82.02910 82.22797\n [57] 82.14226 82.11165 81.88955 81.84489 82.34774 81.54902 82.18188 82.09165\n [65] 81.84526 81.85338 81.87128 82.00308 81.72338 81.94293 81.91947 82.13105\n [73] 82.55263 82.20090 82.07737 82.25692 82.09586 81.95872 82.38038 82.61098\n [81] 82.29940 82.45632 82.12925 82.01000 81.79090 82.38865 81.77925 81.93970\n [89] 82.01699 82.40361 81.82150 82.21865 82.46180 81.93429 81.91970 82.34173\n [97] 82.10436 82.38820 82.02195 82.22835\n\n\n코드가 다소 난잡하기 때문에 임의표본추출하는 로직을 따로 떼어 함수로 제작하고 이를 마찬가지 방식으로 replicate() 함수를 사용해서 동일한 작업을 수행한다.\n\nrun_srs &lt;- function(proportion = 0.1) {\n  coffee_df %&gt;% \n    slice_sample(prop = proportion) %&gt;% \n    summarise(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;% \n    pull(mean_cup_points)\n}\n\nsrs_fun_100 &lt;- replicate(\n  n = 100,\n  expr = run_srs(0.1),\n  simplify = TRUE\n)\n\nsrs_fun_100\n\n  [1] 82.16571 81.88188 82.43504 82.68647 81.83128 81.80256 82.38203 82.12955\n  [9] 82.09947 81.82436 82.20308 82.01068 82.47526 82.45098 81.72361 82.08053\n [17] 81.88865 82.25053 82.71579 82.33098 82.21023 81.83985 82.12511 81.93759\n [25] 81.78195 82.12639 82.27865 82.21887 82.18932 82.37150 82.41414 82.02820\n [33] 82.77090 82.16316 82.33008 82.35842 82.04015 82.16511 81.80594 82.05932\n [41] 82.35639 81.86782 82.12812 82.07677 82.47842 82.09489 82.29865 82.32368\n [49] 82.07331 82.25850 82.09639 82.02519 82.04699 81.68015 82.45436 82.20654\n [57] 82.02459 81.63805 82.35008 82.10346 82.24865 82.26090 82.12068 82.17504\n [65] 82.03541 82.31226 82.57398 82.28286 81.87098 82.02113 82.20887 82.07226\n [73] 82.08827 82.16068 82.04143 82.23714 82.32353 82.28496 82.10925 82.68444\n [81] 82.07429 81.99767 82.28662 82.14248 82.30165 82.11211 82.29211 82.25211\n [89] 82.23722 82.05947 82.31594 82.13406 82.05850 82.44880 82.33481 81.95820\n [97] 82.40992 82.50293 82.20045 82.13564\n\n\n\n10.4.2 표본수 증가\n표본크기가 증가할수록 모집단 대표 평균값에 가까이 추정하는 것을 확인할 수 있다.\n\nextrafont::loadfonts()\n\nsrs_samp_size &lt;- function(samp_size) {\n  srs_fun_100 &lt;- replicate(\n    n = 100,\n    expr = run_srs(samp_size),\n    simplify = TRUE\n  ) \n  return(unlist(srs_fun_100))\n}\n\nsamp_size_df &lt;- tibble(\n  samp_10  = srs_samp_size(0.1),\n  samp_33  = srs_samp_size(1/3),\n  samp_50  = srs_samp_size(1/2),\n  samp_75  = srs_samp_size(3/4),\n  samp_90  = srs_samp_size(0.90)\n) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"samp_size\", values_to = \"estimation\")\n\nsamp_size_df %&gt;% \n  ggplot(aes(x=estimation, color = samp_size)) +\n    geom_density() +\n    geom_vline(xintercept = mean_population, color = \"darkgray\") +\n    theme_bw(base_family = \"MaruBrui\") +\n    theme(legend.position = \"top\") +\n    guides(colour = guide_legend(nrow = 1)) +\n    labs(x = \"추정값\",\n         y = \"밀도\",\n         color = \"추출비율\") +\n    scale_x_continuous(labels = ~ scales::comma(.x, accuracy = 1))\n\n\n\n\n\n\n\n\n10.4.3 표준편차\n추정값에 대한 표준편차도 표본크기 변화에 따라 계산해보자.\n\nsamp_size_df %&gt;% \n  group_by(samp_size) %&gt;% \n  summarise(mean_cup_points = mean(estimation),\n            sd_cup_points   = sd(estimation)) %&gt;% \n  mutate(samp_prop = parse_number(samp_size),\n         samp_prop = str_glue(\"{samp_prop} %\")) %&gt;% \n  select(samp_prop, contains(\"cup\"))\n\n# A tibble: 5 × 3\n  samp_prop mean_cup_points sd_cup_points\n  &lt;glue&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n1 10 %                 82.2        0.219 \n2 33 %                 82.2        0.0904\n3 50 %                 82.2        0.0700\n4 75 %                 82.1        0.0409\n5 90 %                 82.1        0.0228"
  },
  {
    "objectID": "sampling.html#calculate-bootstrap",
    "href": "sampling.html#calculate-bootstrap",
    "title": "\n10  표본 추출\n",
    "section": "\n10.5 신뢰구간",
    "text": "10.5 신뢰구간\n부츠트랩(Bootstrap) 방법론은 모집단에서 나온 표본을 다시 모집단으로 가정하고 표본을 복원추출방법을 통해서 추정하는 방식이다. 부츠트랩 방법론을 통해 추정값은 물론 표준오차도 계산이 가능하다. 부츠트랩은 크게 두단계로 나누는데 먼저 재표집하는 단계와 통계량을 계산하는 단계로 나눠진다.\n부츠트랩을 1,000번 실행해서 얻은 결과를 모집단 평균과 시각적으로 비교한다. 부츠트랩을 통해 계산된 평균은 다음과 같은 특징이 있다.\n\n부츠트랩 표본을 통해 도출된 분포의 중심값 평균은 표본 평균과 대체로 동일하다.\n그렇다고 해서 부츠트랩 표본이 모집단 평균은 아니다.\n즉, 부츠트랩 방법론을 통해 모집단과 모집단에서 추출한 표본집단 사이 발생된 편이(bias)를 보정할 수는 없다.\n\n\nset.seed(77777)\n\ncoffee_srs_df &lt;- coffee_df %&gt;% \n  slice_sample(prop = 0.1)\n\n# 1. 부츠트랩 정의\ndefine_bootstrap &lt;- function() {\n  coffee_srs_df %&gt;%\n    # 1 단계 재표본 단계\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    # 2. 단계 통계량 계산\n    summarize(mean_cup_points = mean(total_cup_points, na.rm = TRUE)) %&gt;%\n    pull(mean_cup_points)\n}\n\n# 2. 부츠트랩 실행\nmean_cup_points &lt;- replicate(\n  n = 1000,\n  expr = define_bootstrap()\n)\n\ntibble(mean_cup_point = mean_cup_points) %&gt;% \n  ggplot(aes(x= mean_cup_point)) +\n    geom_histogram(binwidth = 0.02) +\n    geom_vline(xintercept = mean_population, color = \"blue\") +\n    geom_vline(xintercept = mean(mean_cup_points), color = \"red\", linetype = \"dashed\")  +\n    theme_bw()\n\n\n\n\n\n\n\n\n10.5.1 표준오차\n표준편차와 표준오차를 다음 수식을 통해 직관적으로 이해할 수 있다. 즉, 표준편차는 변량에 대한 산포를 측정하는 반면 표준오차는 추정량에 대한 산포를 측정하게 된다.\n\n표준편차(Standard Deviation): \\(\\sqrt{\\operatorname E\\left[(X - \\mu)^2\\right]}\\)\n\n표준오차(Standard Error): \\(\\sqrt{\\operatorname E\\left[(\\overline{X} - \\mu)^2\\right]}\\)\n\n\n이를 통해서 표준오차에 표본크기 제곱근을 곱하게 되면 모집단 표준편차를 구할 수 있게 된다.\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# 1. 모집단 total_cup_points 표준편차\nsd(coffee_df$total_cup_points)\n\n[1] 2.686862\n\n# 2. 표본 total_cup_points 표준편차\nsd(coffee_srs_df$total_cup_points)\n\n[1] 2.599749\n\n# 3. 표본분포(sampling distribution) total_cup_points 표준편차\nsd(srs_fun_100) * sqrt(nrow(coffee_df) * 0.1)\n\n[1] 2.558876\n\n# 4. 부츠트랩 total_cup_points 표준편차\nstandard_error &lt;- sd(mean_cup_points)\n  \nsd_population &lt;- standard_error * sqrt(nrow(coffee_df) * 0.1)\n\nsd_population\n\n[1] 2.488636\n\n\n\n10.5.2 부츠트랩 신뢰구간\n정규분포를 가정해서 신뢰구간을 구할 수도 있으나 단순히 분위수 함수인 quantile()을 사용해서 95% 신뢰구간 상하한을 간단히 구할 수 있다.\n\ntibble(mean_cup_points = mean_cup_points) %&gt;% \n  summarise(lower = quantile(mean_cup_points, 0.025),\n            mean  = mean(mean_cup_points),\n            upper = quantile(mean_cup_points, 0.975))\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  82.0  82.4  82.8"
  },
  {
    "objectID": "sampling.html#footnotes",
    "href": "sampling.html#footnotes",
    "title": "\n10  표본 추출\n",
    "section": "",
    "text": "Richard L. Scheaffer, III William Mendenhall, R. Lyman Ott, Kenneth G. Gerow - “Elementary Survey Sampling”↩︎\n위키백과, “커피콩”↩︎"
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference",
    "href": "hypothesis.html#computer-age-statistical-inference",
    "title": "12  코딩 가설검정",
    "section": "\n12.1 통계적 가설검정",
    "text": "12.1 통계적 가설검정\n통계적 가설 검정(統計的假說檢定, statistical hypothesis test)은 통계적 추측의 하나로서, 모집단 실제의 값이 얼마가 된다는 주장과 관련해, 표본의 정보를 사용해서 가설의 합당성 여부를 판정하는 과정을 의미하는데 이를 위해서 프로세스(Process)와 함께 검정 통계량을 수식으로 나타낼 수 있어야 하고 이를 해석하는 별도의 훈련도 받아야 했고 이 과정에서 상당량의 수학 및 통계학적 지식이 요구된다. 2\n\n유의수준의 결정, 귀무가설과 대립가설 설정\n검정통계량의 설정 (예를 들어, t-검정)\n\n\\(t_{검정통계량} \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\)\n자유도: \\(\\nu \\quad \\approx \\quad {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\\)\n\n\n\n기각역의 설정\n검정통계량 계산\n통계적인 의사결정\n\n통계적 가설검정(Statistical Testing)은 기존 통계학 전공자의 전유물이었으나, 컴퓨터의 일반화와 누구나 코딩을 할 수 있는 현재(2024-01-21)는 더 이상 기존 통념이 통용되지는 않게 되었다. 특히 파이썬 진영에서 이런 움직임이 활발하다. 그렇다고 R 진영에서도 기존의 방식을 고수하는 것은 아니다.",
    "crumbs": [
      "데이터 과학",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "href": "hypothesis.html#computer-age-statistical-inference-ho-ci",
    "title": "12  코딩 가설검정",
    "section": "\n12.3 가설검정과 신뢰구간",
    "text": "12.3 가설검정과 신뢰구간\ninfer 팩키지는 tidyverse 철학(?)에 따라 가설검정과 신뢰구간을 추정하는 목적으로 개발되었다. 크게 통계적 추론은 가설검정과 신뢰구간 추정이 주된 작업이다. 이를 위해서 5가지 동사(verb)를 새로 익혀야 한다.\n\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nvisualize()\n\n\n\n가설검정과 신뢰구간",
    "crumbs": [
      "데이터 과학",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "href": "hypothesis.html#mosquitoes-love-beer-drinkers",
    "title": "12  코딩 가설검정",
    "section": "\n12.4 사례: 맥주와 모기",
    "text": "12.4 사례: 맥주와 모기\n맥주를 마시는 사람이 말라리아 모기에게 매력적으로 보여 더 잘 물리는가? 라는 흥미로운 논문이 발표되었다. (Lefèvre 기타 2010) 이 연구는 맥주를 마신 후 사람의 냄새 (호흡 및 피부 방출 냄새)가 아노펠레스 감비아(Anopheles gambiae, 아프리카 주요 말라리아 매개체)에게 어떤 영향을 미치는지 조사하였다. 맥주를 마신 사람들의 몸 냄새는 모기의 활성화 (이륙 및 상향 풍속 비행에 참여하는 모기의 비율)와 방향성 (사람의 냄새를 향해 비행하는 모기의 비율)을 증가시켰다. 물을 마신 경우에는 사람이 모기에게 끌리는 것에 영향을 미치지 않았다.\n\n12.4.1 가설검정 환경설정\n데이터 전처리와 시각화, 한글설정을 위한 팩키지를 준비한다. 특히 infer 코딩기반 가설검정을 위해 필수적인 팩키지로 활용하는데 기본적인 사용방법은 mtcars, flights 데이터를 활용한 사례를 살펴본다.\n\nflights 데이터 소품문\nmtcars 데이터 소품문\n\n\n# 0. 환경설정 ----------\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(skimr)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(extrafont)\nloadfonts()\n\n# 1. 데이터 가져오기 -----\n\n# beer_dat &lt;- read_csv(\"https://raw.githubusercontent.com/aloy/m107/master/data/mosquitos.csv\")\nbeer_dat &lt;- read_csv(\"data/mosquitos.csv\")\n\nbeer_df &lt;- beer_dat %&gt;% \n    mutate(treatment = factor(treatment, levels = c(\"beer\", \"water\"), labels=c(\"맥주\", \"맹물\"))) \n\n\n12.4.2 탐색적 데이터 분석\n탐색적 데이터 분석을 통해서 말라리아 모기가 맥주를 마신 사람과 맹물을 마신 사람 어디에 더 많이 접근을 하는지 개체수 차이를 살펴본다. 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 우연에 의한 것인지 아니면 맥주가 더 모기에게 섹시하게 반응하는 역할을 하기 때문인지 살펴본다.\n\n# 2. 탐색적 데이터 분석 -----\n## 2.1. 전체 데이터 \nskim(beer_df)\n\n\nData summary\n\n\nName\nbeer_df\n\n\nNumber of rows\n43\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\n맥주: 25, 맹물: 18\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ncount\n0\n1\n21.77\n4.47\n12\n19\n21\n24\n31\n▂▃▇▅▂\n\n\n\n## 2.2. 맥주와 맹물 투여 집단 비교\nbeer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(최소 = min(count),\n                분위수_25 = quantile(count, 0.25),\n                평균 = mean(count),\n                중위수 = median(count),\n                분위수_75 = quantile(count, 0.75),\n                표준편차 = sd(count),\n                중위절대편차 = mad(count)) %&gt;% \n    mutate(맥주맹물차이 = max(평균) - min(평균))\n\n# A tibble: 2 × 9\n  treatment  최소 분위수_25  평균 중위수 분위수_75 표준편차 중위절대편차\n  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1 맥주         17      20    23.6     24        27     4.13         5.93\n2 맹물         12      16.5  19.2     20        22     3.67         2.97\n# ℹ 1 more variable: 맥주맹물차이 &lt;dbl&gt;\n\n## 2.3. 맥주와 맹물 투여 집단 비교 시각화\n\nbeer_density_g &lt;- ggplot(data = beer_df, mapping = aes(x = count, fill=treatment)) +\n    geom_density(aes(y = ..count..), alpha = 0.7) +\n    scale_x_continuous(limits=c(5,40)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    labs(title=\"맥주를 마시면 모기에게 섹시하게 보일까라고 쓰고 잘 물릴까라고 읽는다.\",\n        x=\"채집된 모기 개체수\", y=\"빈도수\", fill=\"실험처리(treatment): \")\n\nbeer_boxplot_g &lt;- ggplot(data = beer_df, mapping = aes(x = treatment, y = count, fill=treatment)) +\n    geom_boxplot(alpha = 0.5) +\n    geom_jitter(width = 0.2) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    coord_flip() +\n    theme(legend.position = \"none\") +\n    labs(title=\"\",\n         y=\"채집된 모기 개체수\", x=\"실험처리\", fill=\"실험처리(treatment): \")\n\ngrid.arrange(beer_density_g, beer_boxplot_g, nrow=2)\n\n\n\n\n\n\n\n\n12.4.3 의사결정\n맥주를 마신 집단과 맹물을 마신 집단간에 평균적으로 4.38 모기개체수 만큼 차이가 나는데 이런 차이가 유의적인지 전통적인 t-검정과 코딩기반 모의실험을 통해서 살펴보자.\n전통 t-검정\nt-검정 결과 유의적인 차이가 나타나느 것으로 나타난다. p-값이 무척이나 작게 나온다.\n\n# 3. 통계 검정 -----\n## 3.1. 전통적인 해석적인 방법 (t-검정)\nt.test(count ~ treatment, beer_df, null = 0, var.equal = TRUE, alternative=\"greater\")\n\n\n    Two Sample t-test\n\ndata:  count by treatment\nt = 3.587, df = 41, p-value = 0.0004416\nalternative hypothesis: true difference in means between group 맥주 and group 맹물 is greater than 0\n95 percent confidence interval:\n 2.323889      Inf\nsample estimates:\nmean in group 맥주 mean in group 맹물 \n          23.60000           19.22222 \n\n\n코딩기반 t-검정\n코딩기반 t-검정은 다음 절차를 통해 준비한다.\n\n코딩기반 t-검정을 수행할 경우 beer_df가 \\(\\delta^*\\)에 해당되어 데이터에서 사전에 계산해 놓는다.\n가설검정 공식을 specify 함수에 명세한다.\n귀무가설을 hypothesize 함수에서 적시한다.\n컴퓨터에서 모의실험 난수를 generate에서 생성시킨다.\n검정 통계량을 calculate 함수에 명시한다.\n\n그리고 나서 p-값, 95% 신뢰구간을 모의실험결과에서 단순히 세어서 정리하면 된다.\n마지막으로 시각적으로 한번 더 확인한다. 즉, 4.38번 더 물리는 것은 극히 드물게 일어나는 사례로 맥주를 마시면 모기에 더 잘 물리게 된다고 볼 수 있다.\n\n## 3.2. `infer` 팩키지 -----\n\n### 3.2.1. 데이터에서 두 집단 간 차이 산출\nbeer_diff &lt;- beer_df %&gt;% \n    group_by(treatment) %&gt;% \n    summarise(mean = mean(count)) %&gt;% \n    summarise(abs(diff(mean))) %&gt;% \n    pull\n\n### 3.2.2. 귀무가설 모형에서 모의실험을 통해서 통계량 산출\nnull_model &lt;- beer_df %&gt;%\n    specify(count ~ treatment) %&gt;%\n    hypothesize(null = \"independence\") %&gt;% \n    generate(reps = 1000, type = \"permute\") %&gt;% \n    calculate(stat = \"diff in means\", order = c(\"맥주\", \"맹물\"))\n\n### 3.2.3. p-갑과 95% 신뢰구간: 백분위수(Percentile) 방법\nnull_model %&gt;%\n    summarize(p_value = mean(stat &gt; beer_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.002\n\nnull_model %&gt;%\n    summarize(l = quantile(stat, 0.025),\n              u = quantile(stat, 0.975))\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -2.60  2.75\n\n### 3.2.4. 시각화\nggplot(null_model, aes(x = stat, fill=\"gray75\")) +\n    geom_density(aes(y=..count..), alpha=0.7) +\n    geom_vline(xintercept = beer_diff, color = \"red\", size=1.5) +\n    scale_x_continuous(limits=c(-5,5)) +\n    scale_fill_viridis(discrete = TRUE) +\n    theme_bw(base_family=\"NanumGothic\") +\n    theme(legend.position = \"none\") +\n    labs(title=\"맥주를 마시고 4.38번 더 모기에 물린다면...  \",\n         x=\"맥주와 맹물 개체수 차이\", y=\"빈도수\")\n\n\n\n\n\n\n\n\n\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric Elguero, Didier Fontenille, François Renaud, Carlo Costantini, 와/과 Frédéric Thomas. 2010. “Beer consumption increases human attractiveness to malaria mosquitoes”. PloS one 5 (3): e9546.",
    "crumbs": [
      "데이터 과학",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#footnotes",
    "href": "hypothesis.html#footnotes",
    "title": "11  코딩 가설검정",
    "section": "",
    "text": "Allen Downey (2016), There is still only one test↩︎\n위키 백과 - 가설 검정↩︎\nHadley Wickham(2017-11-13), The tidy tools manifesto↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFisher, Ronald Aylmer. 1970. “Statistical Methods for Research\nWorkers.” In Breakthroughs in Statistics: Methodology and\nDistribution, 66–70. Springer.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nLefèvre, Thierry, Louis-Clément Gouagna, Kounbobr Roch Dabiré, Eric\nElguero, Didier Fontenille, François Renaud, Carlo Costantini, and\nFrédéric Thomas. 2010. “Beer Consumption Increases Human\nAttractiveness to Malaria Mosquitoes.” PloS One 5 (3):\ne9546.\n\n\nLindquist, Everet Franklin. 1940. “Statistical Analysis in\nEducational Research.”\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "hypothesis.html#computer-age-tidyverse-inference",
    "href": "hypothesis.html#computer-age-tidyverse-inference",
    "title": "12  코딩 가설검정",
    "section": "\n12.2 tidyverse 가설검정",
    "text": "12.2 tidyverse 가설검정\n데이터 과학을 이끌어 나가는 있는 R과 파이썬 진영의 현재 주도적인 흐름을 살펴보자. 우선 다소 차이가 있지만, for 반복루프를 이해하고 이를 코드로 구현할 수만 있다면 컴퓨터를 활용한 가설검정이 가능한 것은 사실이다. 하지만, 2011년 Allen Downey 교수가 주장했던 것처럼 오랜동안 검정된 해석학적 방법(Analytic Method)에 대한 교차검정하는 방식으로 활용하는 것이 추천된다. 3\n\n\nR과 파이썬 검정 가설검정 프레임워크 비교\n\n코딩기반 가설검정은 우선 데이터로부터 시작된다. 데이터를 컴퓨터의 기능을 활용하여 모의실험 표본을 생성하고 나서 귀무가설(\\(H_0\\)) 모형에서 검정통계량을 추출하여 이를 바탕으로 \\(p-값\\)을 계산하여 의사결정을 추진한다.\n통계검정에도 tidyverse를 반영하고 Allen Downey 교수가 주창한 통계검정 프레임워크를 도입하여 극단적으로 말하며 딥러닝 모형이 거의 모든 통계, 기계학습 모형을 통일해 나가듯이 다양한 통계검정에 대해서도 비숫한 위치를 점할 것으로 예측된다.",
    "crumbs": [
      "데이터 과학",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>코딩 가설검정</span>"
    ]
  },
  {
    "objectID": "NHST.html",
    "href": "NHST.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "R.A. Fisher는 NHST의 토대를 만들었으며 분산분석의 개념과 제한된 표본을 이용해 실험을 설계하는 실험계획법에 큰 기여를 했다. 1925년에 그가 발표한 ’Statistical Methods for Research Workers’라는 책에서 유의성 검정(significance test) 개념이 소개된 것을 확인할 수 있다. [@fisher1970statistical]\n귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 바탕으로 한 가설검정(hypothesis testing) 개념을 Neyman과 Pearson이 정립했고, 이를 적용한 최초의 사례는 1940년에 발표한 “Statistical Analysis in Educational Research”라는 책에서 NHST(Null Hypothesis Significance Testing) 개념을 처음으로 사용한 것으로 알려져 있다. [@lindquist1940statistical]\n\n\n\n\nflowchart TD\n    A[귀무가설 & 대립가설 설정]:::process\n    B[\"유의수준 선택 (예, 0.05)\"]:::process\n    C[데이터 수집 및 분석]:::process\n    D[\"검정 통계량 계산 (예, t-점수, z-점수)\"]:::process\n    E[검정 통계량을 임계값과 비교]:::decision\n    F[귀무가설 기각 또는 기각하지 않음]:::decision\n    G[\"결과 보고\"]:::report\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    classDef process fill:#efefef,stroke:#333,stroke-width:1px;\n    classDef decision fill:#ffefef,stroke:#333,stroke-width:1px;\n    classDef report fill:#efefff,stroke:#333,stroke-width:1px;"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "\n13  함수\n",
    "section": "",
    "text": "14 함수 구성요소\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다."
  },
  {
    "objectID": "functions.html#defuse-and-inject-패턴",
    "href": "functions.html#defuse-and-inject-패턴",
    "title": "\n13  함수\n",
    "section": "\n13.1 Defuse-and-Inject 패턴",
    "text": "13.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n\n# A tibble: 11 × 5\n   species island    sex    year body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n 2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n 3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n 4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n 5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n 6 Adelie  Dream     &lt;NA&gt;   2007        2975\n 7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n 8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n 9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\n\n\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다."
  },
  {
    "objectID": "functions.html#역사",
    "href": "functions.html#역사",
    "title": "\n13  함수\n",
    "section": "\n13.2 역사",
    "text": "13.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다."
  },
  {
    "objectID": "functions.html#all-about-function",
    "href": "functions.html#all-about-function",
    "title": "\n13  함수\n",
    "section": "\n13.1 함수 기본 지식",
    "text": "13.1 함수 기본 지식\n함수는 입력값(x)를 넣어 어떤 작업(f)을 수행한 결과를 반환(y) 과정으로 이해할 수 있는데, 인자로 다양한 값을 함수에 넣을 수 있고, 물론 함수가 뭔가 유용한 작업을 수행하기 위한 전제조건을 만족시키는지 확인하는 과정을 assert 개념을 넣어 확인하고 기술된 작업을 수행한 후에 출력값을 변환시키게 된다.\n\n\n데이터 과학 함수 개념"
  },
  {
    "objectID": "functions.html#why-write-function",
    "href": "functions.html#why-write-function",
    "title": "\n13  함수\n",
    "section": "\n13.4 왜 함수가 필요한가?",
    "text": "13.4 왜 함수가 필요한가?\n왜 함수가 필요한지를 데이터를 분석할 때 자주 나오는 변수 정규화 사례를 바탕으로 살펴보자. 데이터프레임에 담긴 변수의 측도가 상이하여 변수를 상대적으로 비교하기 위해 측도를 재조정하여 표준화할 필요가 있다. 변수에서 평균을 빼고 표준편차로 나누는 정규화도 있지만, 최대값에서 최소값을 빼서 분모에 두고 분자에 최소값을 빼서 나누면 모든 변수가 0–1 사이 값으로 척도가 조정된다.\n\\[ f(x)_{\\text{척도조정}} = \\frac{x-min(x)}{max(x)-min(x)} \\]\n\ndf &lt;- data.frame(a=c(1,2,3,4,5),\n                         b=c(10,20,30,40,50),\n                         c=c(7,8,6,1,3),\n                         d=c(5,4,6,5,2))\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) /\n        (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) /\n        (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\ndf        \n\n     a         b         c    d\n1 0.00  0.000000 0.8571429 0.75\n2 0.25 -1.111111 1.0000000 0.50\n3 0.50 -2.222222 0.7142857 1.00\n4 0.75 -3.333333 0.0000000 0.75\n5 1.00 -4.444444 0.2857143 0.00\n\n\n상기 R 코드는 측도를 모두 맞춰서 변수 4개(a, b, c, d)를 비교하거나 향후 분석을 위한 것이다. 하지만, 읽어야 하는 코드중복이 심하고 길어 코드를 작성한 개발자의 의도 가 본의 아니게 숨겨져 있다. 작성한 R 코드에 실수한 것이 있는 경우, 다음 프로그램 실행에서 버그(특히, 구문론이 아닌 의미론적 버그)가 숨겨지게 된다. 상기 코드가 작성되는 과정을 살펴보면 본의 아니게 의도가 숨겨진다는 의미가 어떤 것인지 명확해진다.\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) 코드를 작성한 후, 정상적으로 돌아가는지 확인한다.\n1번 코드가 잘 동작하게 되면 다음 복사하여 붙여넣기 신공을 사용하여 다른 칼럼 작업을 확장해 나간다. df$b, df$c, df$d를 생성하게 된다.\n즉, 복사해서 붙여넣은 것을 변수명을 편집해서 df$b, df$c, df$d 변수를 순차적으로 생성해 낸다.\n\n\n\n\n\n\n\n해들리 위캠 어록\n\n\n\n\n중복은 의도를 숨기게 되고, 복사하여 붙여넣기 두번하면 함수를 작성할 시점이 되었다. (Duplication hides the intent. If you have copied-and-pasted twice, it is time to write a function.)"
  },
  {
    "objectID": "functions.html#time-to-write-function",
    "href": "functions.html#time-to-write-function",
    "title": "\n13  함수\n",
    "section": "\n13.6 함수를 작성하는 시점",
    "text": "13.6 함수를 작성하는 시점\n복사해서 붙여넣는 것을 두번 하게 되면, 함수를 작성할 시점이다. 중복을 제거하는 한 방법이 함수를 작성하는 것이고, 함수를 작성하게 되면 의도가 명확해진다. 함수명을 rescale로 붙이고 이를 실행하게 되면, 의도가 명확하게 드러나게 되고, 복사해서 붙여넣게 되면서 생겨나는 중복과 반복에 의한 실수를 줄일 수 있게 되고, 향후 코드를 갱신할 때도 도움이 된다.\n\nrescale &lt;- function(x){\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a &lt;- rescale(df$a)\ndf$b &lt;- rescale(df$b)\ndf$c &lt;- rescale(df$c)\ndf$d &lt;- rescale(df$d)\n\nrescale() 함수를 사용해서 복사하여 붙여넣는 중복을 크게 줄였으나, 여전히 함수명을 반복해서 복사하여 붙여넣기를 통해 코드를 작성했다. 함수형 프로그래밍을 사용하는 것으로 함수명을 반복적으로 사용하는 것조차도 피할 수 있다.\n\nlibrary(purrr)\ndf &lt;- map_df(df, rescale)\ndf\n\n# A tibble: 5 × 4\n      a     b     c     d\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0     1    0.857  0.75\n2  0.25  0.75 1      0.5 \n3  0.5   0.5  0.714  1   \n4  0.75  0.25 0      0.75\n5  1     0    0.286  0   \n\n\n함수를 사용하지 않고 복사하여 붙여넣기 방식으로 코드를 작성한 경우 의도하지 않은 실수가 있어 함수를 도입하여 작성한 코드와 결과가 다른 것이 존재한다. 코드를 읽어 찾아보거나 실행한 후 결과를 통해 버그를 찾아보는 것도 함수의 의미와 중요성을 파악하는데 도움이 된다.\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것.\n\n한걸음 더 들어가 구체적으로 좋은 함수는 다음과 같은 특성을 지니고 있다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 일관된 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable"
  },
  {
    "objectID": "functions.html#criteria-on-good-function",
    "href": "functions.html#criteria-on-good-function",
    "title": "\n13  함수\n",
    "section": "\n13.4 좋은 함수",
    "text": "13.4 좋은 함수\n좋은 함수를 작성하려면 다음과 같은 조건이 만족되어야 한다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 좋은 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable\n\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것."
  },
  {
    "objectID": "functions.html#function-component-argument",
    "href": "functions.html#function-component-argument",
    "title": "\n13  함수\n",
    "section": "\n14.1 인자(argument)",
    "text": "14.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)"
  },
  {
    "objectID": "functions.html#function-component-argument-assert",
    "href": "functions.html#function-component-argument-assert",
    "title": "\n13  함수\n",
    "section": "\n14.2 인자값 확인 - assert\n",
    "text": "14.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다. 다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n\n[1] 15\n\nsum_numbers(na_vector)\n\nError in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n\nError in if (assert_any_are_na(vec)) {: the condition has length &gt; 1"
  },
  {
    "objectID": "functions.html#function-component-return",
    "href": "functions.html#function-component-return",
    "title": "\n13  함수\n",
    "section": "\n14.3 반환값 확인",
    "text": "14.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\ninter\n\n[1] 37.88458\n\nslope\n\n[1] -2.87579\n\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 쪼개 저장시킨다.\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\niris_df %&gt;% \n  count(train_test)\n\n  train_test   n\n1       test  32\n2      train 118\n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\nglimpse(train)\n\nRows: 118\nColumns: 7\n$ Sepal.Length &lt;dbl&gt; 4.9, 4.7, 4.6, 5.0, 5.4, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.0, 3.2, 3.1, 3.6, 3.9, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.4, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ runif        &lt;dbl&gt; 0.6449041, 0.4740827, 0.5294489, 0.9515431, 0.4366536, 0.…\n$ train_test   &lt;chr&gt; \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"tr…\n\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n\n$intercept\n(Intercept) \n   37.88458 \n\n$beta\n     cyl \n-2.87579"
  },
  {
    "objectID": "functions.html#understand-function",
    "href": "functions.html#understand-function",
    "title": "\n13  함수\n",
    "section": "\n15.1 함수 이해",
    "text": "15.1 함수 이해\n함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 사이언스 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 비교도 겸해보자.\n\n\nR 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n\n[[1]]\n[1] 10\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 21\n\n[[4]]\n[1] 2.333333\n\n\n\n\n파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n\n[10, 4, 21, 2.3333333333333335]"
  },
  {
    "objectID": "functions.html#understand-argument",
    "href": "functions.html#understand-argument",
    "title": "\n13  함수\n",
    "section": "\n15.2 함수 인자",
    "text": "15.2 함수 인자\n함수를 구성하는 중요한 요소는 함수인자다."
  },
  {
    "objectID": "functions.html#call-function",
    "href": "functions.html#call-function",
    "title": "\n13  함수\n",
    "section": "\n15.3 함수 호출",
    "text": "15.3 함수 호출\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 사용하기 위해서 먼저 함수명을 알아야 되고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n\nfunction (x, na.rm = FALSE) \nNULL\n\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야 하고 따라서, 데이터프레임의 변수 하나(lifeExp)를 지정하여 전달하고 na.rm = TRUE도 명세하여 다시 전달해둔다. 이와 같이 인자값이 기본디폴트 설정된 경우 타이핑을 줄일 수 있고, 경우에 따라서 다른 인자를 넣어 전달시켜주면 된다.\n\nlibrary(gapminder)\nsd(gapminder$lifeExp, na.rm = TRUE)\n\n[1] 12.91711"
  },
  {
    "objectID": "functions.html#writing-funciton-dice",
    "href": "functions.html#writing-funciton-dice",
    "title": "\n13  함수\n",
    "section": "\n16.1 주사위",
    "text": "16.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n\n[1] 2\n\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n\n[1] 5\n\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  simulated_value &lt;- sample(dice, size=num_try)\n  return(simulated_value) # 불필요함.\n}\n\ndraw_dice(3)\n\n[1] 6 5 3"
  },
  {
    "objectID": "functions.html#how-to-write-function",
    "href": "functions.html#how-to-write-function",
    "title": "\n13  함수\n",
    "section": "\n13.7 사례: rescale 함수",
    "text": "13.7 사례: rescale 함수\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만드는 과정을 통해 앞서 학습한 사례를 실습해 보자.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)"
  },
  {
    "objectID": "functions.html#function-is-argument",
    "href": "functions.html#function-is-argument",
    "title": "\n13  함수\n",
    "section": "\n13.8 사례: 요약통계 함수",
    "text": "13.8 사례: 요약통계 함수\n데이터를 분석할 때 가장 먼저 수행하는 작업이 요약통계를 통해 데이터를 이해하는 것이다. 이미 훌륭한 요약통계 패키지와 함수가 있지만, 친숙한 개념을 함수로 다시 제작함으로써 함수에 대한 이해를 높일 수 있다.\n요약통계 기능을 먼저 구현한 다음에 중복 제거하여 요약통계 기능 함수를 제작해보자. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성하는 경우, 숫자 1,2,3 만 차이날 뿐 다른 부분은 동일하기 때문에 함수 코드에 중복이 심하게 관찰된다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n여기서 요약통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘겨 요약통계함수를 간략하고 가독성 높게 작성할 수 있다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 가정해보자.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n\n[1] 0.5000000 0.5000000 0.7142857 0.7500000\n\ncol_summary(df, fun = mean)\n\n[1] 0.5000000 0.5000000 0.5714286 0.6000000\n\ncol_summary(df, fun = sd)\n\n[1] 0.3952847 0.3952847 0.4164966 0.3791438"
  },
  {
    "objectID": "functions.html#funciton-component",
    "href": "functions.html#funciton-component",
    "title": "\n13  함수\n",
    "section": "\n13.5 함수 구성요소",
    "text": "13.5 함수 구성요소\n\n13.5.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n\n13.5.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다.\n다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n\n[1] 15\n\nsum_numbers(na_vector)\n\nError in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n\nError in if (assert_any_are_na(vec)) {: the condition has length &gt; 1\n\n\n\n13.5.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\n\ncat(\"절편: \", inter, \"\\n기울기: \", slope)\n\n절편:  37.88458 \n기울기:  -2.87579\n\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 나누어 할당하는 것도 가능하다. 각 붓꽃마다 0~1 사이 난수를 생성하여 할당한다. 그리고 난수값이 0.2 이상이면 훈련, 그렇지 않으면 시험 데이터로 구분한다.\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\ncat(\"총 관측점: \", nrow(iris), \"\\n훈련: \", nrow(train), \"\\n시험: \", nrow(test))\n\n총 관측점:  150 \n훈련:  117 \n시험:  33\n\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n\n$intercept\n(Intercept) \n   37.88458 \n\n$beta\n     cyl \n-2.87579"
  },
  {
    "objectID": "functions.html#how-to-use-function",
    "href": "functions.html#how-to-use-function",
    "title": "\n13  함수\n",
    "section": "\n13.2 함수 사용법",
    "text": "13.2 함수 사용법\n본격적으로 함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 과학 대표 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 자세히 살펴보자.\n\n13.2.1 R 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n\n[[1]]\n[1] 10\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 21\n\n[[4]]\n[1] 2.333333\n\n\n\n13.2.2 파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n\n[10, 4, 21, 2.3333333333333335]\n\n\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 다른 사람이 작성한 함수를 사용하기 위해서 먼저 함수명을 알아야 하고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n\nfunction (x, na.rm = FALSE) \nNULL\n\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야만 표준편차를 계산할 수 있다. 예를 들어, 데이터프레임(penguins)의 변수 하나(bill_length_mm)를 지정하여 전달하고 na.rm = TRUE도 명세하여 인자로 전달한다. 인자값이 기본디폴트 값으로 설정된 경우 타이핑을 줄일 수 있고, 결측값이 포함된 경우에 따라서 다른 인자를 넣어 전달하는 방식으로 함수를 사용한다.\n\nlibrary(palmerpenguins)\n\nsd(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 5.459584"
  },
  {
    "objectID": "functions.html#convert-scripts-to-function",
    "href": "functions.html#convert-scripts-to-function",
    "title": "\n13  함수\n",
    "section": "\n13.3 스크립트 → 함수",
    "text": "13.3 스크립트 → 함수\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다.\n\nR 함수 템플릿을 제작한다.\n\n함수명 &lt;- function() { }\n\n\n스크립트를 함수 몸통에 복사하여 붙인다.\n반복작업되는 인자를 찾아내 이를 인자로 넣어둔다.\n인자값과 연동되는 부분을 찾아 맞춰준다.\n함수명을 적절한 동사를 갖춘 이름으로 작명한다.\n\nreturn이 불필요하기 때문에 R 언어 특성을 반영하여 필요한 경우 제거한다.\n\n\n13.3.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해 본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n\n[1] 5\n\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1, 2, 3, 4, 5, 6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n\n[1] 2\n\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  face &lt;- sample(dice, size=num_try)\n  return(face) # 불필요함.\n}\n\ndraw_dice(3)\n\n[1] 5 6 4"
  },
  {
    "objectID": "functions_ds.html#defuse-and-inject-패턴",
    "href": "functions_ds.html#defuse-and-inject-패턴",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.1 Defuse-and-Inject 패턴",
    "text": "14.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n\n# A tibble: 11 × 5\n   species island    sex    year body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n 2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n 3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n 4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n 5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n 6 Adelie  Dream     &lt;NA&gt;   2007        2975\n 7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n 8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n 9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\n\n\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다."
  },
  {
    "objectID": "functions_ds.html#역사",
    "href": "functions_ds.html#역사",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.2 역사",
    "text": "14.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다.\n\n데이터 분석에서 빈도수가 높은 작업을 Base R과 dplyr 패키지를 사용한 사례를 다음과 같이 비교하면 S언어에서 현재까지 이뤄낸 발전이 가시적으로 다가온다.\n\n\n\n\n\n\n\n작업\nBase R\ndplyr\n\n\n\n행 필터링 (Filter)\nsubset(data, condition)\ndata %&gt;% filter(condition)\n\n\n특정 칼럼 선택 (Select)\ndata[, c(\"col1\", \"col2\")]\ndata %&gt;% select(col1, col2)\n\n\n그룹별 집계작업\naggregate(. ~ grouping_var, data, FUN = mean)\ndata %&gt;% group_by(grouping_var) %&gt;% summarize(new_col = mean(col_name))\n\n\n죠인(Join)\nmerge(data1, data2, by = \"key_column\")\ndata1 %&gt;% inner_join(data2, by = \"key_column\")\n\n\n칼럼 추가\ntransform(data, new_col = some_func(existing_col))\ndata %&gt;% mutate(new_col = some_func(existing_col))\n\n\n행 결합\nrbind(data1, data2)\nbind_rows(data1, data2)\n\n\n칼럼 결합\ncbind(data1, data2)\nbind_cols(data1, data2)\n\n\n정렬\ndata[order(data$col_name), ]\ndata %&gt;% arrange(col_name)\n\n\n\n\n14.2.1 attach() 함수\n데이터프레임에 attach() 함수를 사용하면 데이터프레임을 구성하는 칼럼이 벡터로 작업환경에서 바로 접근하여 작업을 수행할 수 있다. penguins 데이터프레임을 attach()한 결과 bill_depth_mm 벡터가 작업환경에서 바로 접근하여 평균값을 계산할 수 있게 되었다. 작업을 완료한 후에 detach() 를 사용해서 작업환경에서 제거한다.\n\nlibrary(palmerpenguins)\n\nbase::attach(penguins)\n\nls(pos = which(search() == \"penguins\")[1])\n\n[1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n[4] \"flipper_length_mm\" \"island\"            \"sex\"              \n[7] \"species\"           \"year\"             \n\nmean(bill_depth_mm, na.rm = TRUE)\n\n[1] 17.15117\n\ndetach(penguins)\n\n\n14.2.2 with() 함수\nattach() 함수는 편리한 장점이 있지만, 데이터프레임 변수명과 함수명, 또 다른 작업에서 나온 객체명과 충돌이 발생할 경우 전혀 생각하지 못한 문제가 발생할 수 있다. 따라서, 격리를 통해 문제를 단순화하는 것이 필요하다. 이를 위해서 with() 함수를 사용하게 되면 데이터프레임에 속한 칼럼명을 명시하지 않더라도 간결하게 데이터 분석 작업을 이어나갈 수 있다.\n\nlibrary(palmerpenguins)\n\nwith(data = penguins,\n     expr = mean(bill_depth_mm, na.rm = TRUE))\n\n[1] 17.15117\n\n\n\n14.2.3 aggregate() 함수\nBase R 에서 지원되는 aggregate() 함수를 사용해서 동일한 결과를 얻을 수 있다. aggregate() 함수는 with()와 지향점은 유사하지만 구현방식에서 다소 차이가 난다.\n\naggregate(bill_depth_mm ~ 1, \n          data = penguins, \n          FUN = mean, \n          na.rm = TRUE)\n\n  bill_depth_mm\n1      17.15117"
  },
  {
    "objectID": "functions_purrr.html",
    "href": "functions_purrr.html",
    "title": "15  함수형 프로그래밍",
    "section": "",
    "text": "16 함수형 프로그래밍 이론과 실제\n함수는 다음과 같이 될 수도 있어 함수형 프로그래밍 언어가 된다. 5\nFirstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do anything with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.\nround_mean() 함수를 compose() 함수를 사용해서 mean() 함수로 평균을 구한 후에 round()함수로 반올림하는 코드를 다음과 같이 쉽게 작성할 수 있다.\nround_mean &lt;- compose(round, mean)\nround_mean(1:10)\n\n[1] 6\n두번째 사례로 전형적인 데이터 분석 사례로 lm() → anova() → tidy()를 통해 한방에 선형회귀 모형 산출물을 깨끗한 코드로 작성하는 사례를 살펴보자.\nmtcars 데이터셋에서 연비 예측에 변수 두개를 넣고 일반적인 lm() 선형예측모형 제작방식과 동일하게 인자를 넣는다.\nclean_lm &lt;- compose(broom::tidy, anova, lm)\nclean_lm(mpg ~ hp + wt, data=mtcars)\n\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 hp            1  678. 678.       101.   5.99e-11\n2 wt            1  253. 253.        37.6  1.12e- 6\n3 Residuals    29  195.   6.73      NA   NA\ncompose()를 통해 함수를 조합하는 경우 함수의 인자를 함께 전달해야될 경우가 있다. 이와 같은 경우 partial()을 사용해서 인자를 넘기는 함수를 제작하여 compose()에 넣어준다.\nrobust_round_mean &lt;- compose(\n  partial(round, digits=1),\n  partial(mean, na.rm=TRUE))\nrobust_round_mean(c(NA, 1:10))\n\n[1] 5.5\n리스트 칼럼(list-column)과 결합하여 모형에서 나온 데이터 분석결과를 깔끔하게 코드로 제작해보자. 먼저 lm을 돌려 모형 요약하는 함수 summary를 통해 r.squared값을 추출하는 함수를 summary_lm으로 제작한다.\n그리고 나서 nest() 함수로 리스트 칼럼(list-column)을 만들고 두개의 집단 수동/자동을 나타내는 am 변수를 그룹으로 삼아 두 집단에 속한 수동/자동 데이터에 대한 선형 회귀모형을 적합시키고 나서 “r.squared”값을 추출하여 이를 티블 데이터프레임에 저장시킨다.\nsummary_lm &lt;- compose(summary, lm) \n\nmtcars %&gt;%\n  group_by(am) %&gt;%\n  nest() %&gt;%\n  mutate(lm_mod = map(data, ~ summary_lm(mpg ~ hp + wt, data = .x)),\n         r_squared = map(lm_mod, \"r.squared\")) %&gt;%\n  unnest(r_squared)\n\n# A tibble: 2 × 4\n# Groups:   am [2]\n     am data               lm_mod     r_squared\n  &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n1     1 &lt;tibble [13 × 10]&gt; &lt;smmry.lm&gt;     0.837\n2     0 &lt;tibble [19 × 10]&gt; &lt;smmry.lm&gt;     0.768"
  },
  {
    "objectID": "functions_purrr.html#why-functional-programming",
    "href": "functions_purrr.html#why-functional-programming",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.1 왜 함수형 프로그래밍인가?",
    "text": "16.1 왜 함수형 프로그래밍인가?\n데이터 분석을 아주 추상화해서 간략하게 얘기한다면 데이터프레임을 함수에 넣어 새로운 데이터프레임으로 만들어 내는 것이다.\n\n\n함수로 이해하는 데이터 분석\n\n데이터 분석, 데이터 전처리, 변수 선택, 모형 개발이 한번에 해결되는 것이 아니라서, 데이터프레임을 함수에 넣어 상태가 변경된 데이터프레임이 생성되고, 이를 다시 함수에 넣어 또다른 변경된 상태 데이터프레임을 얻게 되는 과정을 쭉 반복해 나간다.\n\n\n데이터 분석 작업흐름\n\n따라서… 데이터 분석에는 함수형 프로그래밍 패러다임을 활용하고, 툴/패키지 개발에는 객체지향 프로그래밍 패러다임 사용이 권장된다.\n\n\n데이터 분석과 툴/패키지 도구 개발"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton",
    "href": "functions_purrr.html#functional-programming-newton",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.2 뉴튼 방법(Newton’s Method)\n",
    "text": "16.2 뉴튼 방법(Newton’s Method)\n\n뉴튼-랩슨 알고리즘으로도 알려진 뉴튼(Newton Method) 방법은 컴퓨터를 사용해서 수치해석 방법으로 실함수의 근을 찾아내는 방법이다.\n특정 함수 \\(f\\) 의 근을 찾을 경우, 함수 미분값 \\(f'\\), 초기값 \\(x_0\\)가 주어지면 근사적 근에 가까운 값은 다음과 같이 정의된다.\n\\[x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)}\\]\n이 과정을 반복하게 되면 오차가 매우 적게 근의 값에 도달하게 된다.\n\\[x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\]\n기하적으로 보면, 파란 선은 함수 \\(f\\) 이고, \\(f\\)를 미분한 \\(f'\\) 빨간 선은 뉴턴방법을 활용하여 근을 구해가는 과정을 시각적으로 보여주고 있다. \\(x_{n-1}\\) 보다 \\(x_n\\)이, \\(x_n\\) 보다 \\(x_{n+1}\\)이 함수 \\(f\\) 근에 더 가깝게 접근해 나가는 것이 확인된다.\n\n\n뉴튼 방법 도식화"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton-method",
    "href": "functions_purrr.html#functional-programming-newton-method",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.3 뉴튼 방법 R 코드 1\n",
    "text": "16.3 뉴튼 방법 R 코드 1\n\n뉴튼 방법을 R코들 구현하면 다음과 같이 612의 제곱근 값을 수치적으로 컴퓨터를 활용하여 구할 수 있다. while같은 루프를 활용하여 반복적으로 해를 구하는 것도 가능하지만 재귀를 활용하여 해를 구하는 방법이 코드를 작성하고 읽는 개발자 관점에서는 훨씬 더 편리하고 권장된다.\n하지만, 속도는 while 루프를 사용하는 것이 R에서는 득이 많다. 이유는 오랜 세월에 걸쳐 최적화 과정을 거쳐 진화했기 때문이다.\n\n\nwhile 루프를 사용한 방법\n\nfind_root &lt;- function(guess, init, eps = 10^(-10)){\n    while(abs(init**2 - guess) &gt; eps){\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"현재 값: \", init, \"\\n\")\n    }\n    return(init)\n}\n\nfind_root(612, 10)\n\n현재 값:  35.6 \n현재 값:  26.39551 \n현재 값:  24.79064 \n현재 값:  24.73869 \n현재 값:  24.73863 \n현재 값:  24.73863 \n\n\n[1] 24.73863\n\n\n\n\n재귀를 사용한 방법\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"재귀방법 현재 값: \", init, \"\\n\")\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\nfind_root_recur(612, 10)\n\n재귀방법 현재 값:  35.6 \n재귀방법 현재 값:  26.39551 \n재귀방법 현재 값:  24.79064 \n재귀방법 현재 값:  24.73869 \n재귀방법 현재 값:  24.73863 \n재귀방법 현재 값:  24.73863 \n\n\n[1] 24.73863"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-hello-world",
    "href": "functions_purrr.html#functional-programming-purrr-hello-world",
    "title": "15  함수형 프로그래밍",
    "section": "\n17.1 purrr 헬로월드",
    "text": "17.1 purrr 헬로월드\npurrr 팩키지를 불러와서 map_dbl() 함수에 구문에 맞게 작성하면 동일한 결과를 깔끔하게 얻을 수 있다. 즉,\n\n\nmap_dbl(): 벡터, 데이터프레임, 리스트에 대해 함수를 원소별로 적용시켜 결과를 double 숫자형으로 출력시킨다.\n\nnumbers: 함수를 각 원소별로 적용시킬 벡터 입력값\n\nfind_root_recur: 앞서 작성한 뉴톤 방법으로 제곱근을 수치적으로 구하는 사용자 정의함수.\n\ninit=1, eps = 10^-10: 뉴톤 방법을 구현한 사용자 정의함수에 필요한 초기값.\n\n\nlibrary(purrr)\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nmap_dbl(numbers, find_root_recur, init=1, eps = 10^-10)\n\n[1] 4 5 6 7 8 9"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-read-iris",
    "href": "functions_purrr.html#functional-programming-purrr-read-iris",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.6 병렬 데이터 분석",
    "text": "16.6 병렬 데이터 분석\n구글 검색을 통해서 쉽게 iris(붓꽃) 데이터를 구할 수 있다. 이를 불러와서 각 종별로 setosa versicolor, virginica로 나눠 로컬 .csv 파일로 저장하고 나서 이를 다시 불러오는 사례를 함수형 프로그래밍으로 구현해본다.\n\n\n붓꽃 데이터 불러오기\n\n먼저 iris.csv 파일을 R로 불러와서 각 종별로 나눠서 iris_종명.csv 파일형식으로 저장시킨다.\n\nlibrary(tidyverse)\niris_df &lt;- read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\")\n\niris_species &lt;- iris_df %&gt;% \n  count(species) %&gt;% pull(species)\n\nfor(i in 1:nrow(iris_df)) {\n  tmp_df &lt;- iris_df %&gt;% \n    filter(species == iris_species[i])\n  species_name &lt;- iris_species[i]\n  tmp_df %&gt;% write_csv(paste0(\"data/iris_\", species_name, \".csv\"))\n}\n\nSys.glob(\"data/iris_*.csv\")\n\n[1] \"data/iris_NA.csv\"         \"data/iris_setosa.csv\"    \n[3] \"data/iris_versicolor.csv\" \"data/iris_virginica.csv\" \n\n\n로컬 파일 iris_종명.csv 형식으로 저장된 데이터를 함수형 프로그래밍을 통해 불러와서 분석작업을 수행해보자. map() 함수를 사용해서 각 종별로 데이터를 깔끔하게 불러왔다.\niris_filename 벡터에 iris_종명.csv과 경로명이 포함된 문자열을 저장시켜 놓고 read_csv() 함수를 각 벡터 원소에 적용시켜 출력값으로 리스트 iris_list 객체를 생성시켰다.\n\niris_filename &lt;- c(\"data/iris_setosa.csv\", \"data/iris_versicolor.csv\", \"data/iris_virginica.csv\")\n\niris_list &lt;- map(iris_filename, read_csv) %&gt;% \n  set_names(iris_species)\n\niris_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name       value              \n  &lt;chr&gt;      &lt;list&gt;             \n1 setosa     &lt;spc_tbl_ [50 × 5]&gt;\n2 versicolor &lt;spc_tbl_ [50 × 5]&gt;\n3 virginica  &lt;spc_tbl_ [50 × 5]&gt;\n\n\niris_list 각 원소는 데이터프레임이라 summary 함수를 사용해서 기술 통계량을 구할 수도 있다. 물론 cor() 함수를 사용해서 iris_list의 각 원소를 지정하는 .x 여기서는 종별 데이터프레임에서 변수 두개를 추출하여 sepal_length, sepal_width 이 둘간의 스피커만 상관계수를 계산하는데 출력값이 double 연속형이라 map_dbl로 저정하여 작업시킨다.\n\nmap(iris_list, summary)\n\n$setosa\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n 1st Qu.:4.800   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.200  \n Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n Mean   :5.006   Mean   :3.418   Mean   :1.464   Mean   :0.244  \n 3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$versicolor\n  sepal_length    sepal_width     petal_length   petal_width   \n Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000  \n 1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200  \n Median :5.900   Median :2.800   Median :4.35   Median :1.300  \n Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326  \n 3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500  \n Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$virginica\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n 1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n 3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nmap_dbl(iris_list, ~cor(.x$sepal_length, .x$sepal_width, method = \"spearman\"))\n\n    setosa versicolor  virginica \n 0.7686085  0.5176060  0.4265165"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-analysis",
    "href": "functions_purrr.html#functional-programming-purrr-analysis",
    "title": "15  함수형 프로그래밍",
    "section": "\n15.6 데이터 분석사례",
    "text": "15.6 데이터 분석사례\niris_list 각 원소는 데이터프레임이라 summary 함수를 사용해서 기술 통계량을 구할 수도 있다. 물론 cor() 함수를 사용해서 iris_list의 각 원소를 지정하는 .x 여기서는 종별 데이터프레임에서 변수 두개를 추출하여 sepal_length, sepal_width 이 둘간의 스피커만 상관계수를 계산하는데 출력값이 double 연속형이라 map_dbl로 저정하여 작업시킨다.\n\nmap(iris_list, summary)\n\n$setosa\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n 1st Qu.:4.800   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.200  \n Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n Mean   :5.006   Mean   :3.418   Mean   :1.464   Mean   :0.244  \n 3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$versicolor\n  sepal_length    sepal_width     petal_length   petal_width   \n Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000  \n 1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200  \n Median :5.900   Median :2.800   Median :4.35   Median :1.300  \n Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326  \n 3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500  \n Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n$virginica\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n 1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n 3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n   species         \n Length:50         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nmap_dbl(iris_list, ~cor(.x$sepal_length, .x$sepal_width, method = \"spearman\"))\n\n    setosa versicolor  virginica \n 0.7686085  0.5176060  0.4265165"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-random-number",
    "href": "functions_purrr.html#functional-programming-purrr-random-number",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.7 표본추출",
    "text": "16.7 표본추출\n서로 다른 난수를 생성시키는 방법을 살펴보자. 정규분포를 가정하고 평균과 표준편차를 달리하는 모수를 지정하고 난수갯수도 숫자를 달리하여 난수를 생성시킨다.\n\n16.7.1 \\(\\mu\\) 평균 변화\n정규분포에서 난수를 10개 추출하는데 표준편차는 1로 고정시키고, 평균만 달리한다. 평균만 달리하기 때문에 map() 함수를 그대로 사용한다. 즉, 입력값으로 평균만 달리하는 리스트를 입력값으로 넣는다.\n\n## 평균을 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\n\nsim_mu_name &lt;- paste0(\"mu: \", normal_mean)\n\nsim_mu_list &lt;- map(normal_mean, ~ data.frame(mean = .x, \n                            random_number = rnorm(mean=.x, sd=1, n=10))) %&gt;% \n  set_names(sim_mu_name)\n\nmap_dbl(sim_mu_list, ~mean(.x$random_number))\n\n    mu: 1     mu: 5    mu: 10 \n 1.572255  4.913465 10.048082 \n\nsim_mu_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name   value        \n  &lt;chr&gt;  &lt;list&gt;       \n1 mu: 1  &lt;df [10 × 2]&gt;\n2 mu: 5  &lt;df [10 × 2]&gt;\n3 mu: 10 &lt;df [10 × 2]&gt;\n\n\n\n16.7.2 \\(\\mu\\) 평균, \\(\\sigma\\) 표준편차\n난수갯수만 고정시키고 평균과 표준편차를 달리하여 난수를 정규분포에서 추출한다. 입력값으로 평균과 표준편차 두개가 되기 때문에 map2() 함수를 사용한다.\n\n## 평균과 표준편차를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\n\nsim_mu_sd_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd)\n\nsim_mu_sd_list &lt;- map2(normal_mean, normal_sd, \n                        ~ data.frame(mean = .x, sd = .y,\n                            random_number = rnorm(mean=.x, sd=.y, n=10))) %&gt;% \n  set_names(sim_mu_sd_name)\n\nmap_dbl(sim_mu_sd_list, ~sd(.x$random_number))\n\nmu: 1,  sd: 10  mu: 5,  sd: 5 mu: 10,  sd: 1 \n    11.3629128      5.4664353      0.9207411 \n\nsim_mu_sd_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name           value        \n  &lt;chr&gt;          &lt;list&gt;       \n1 mu: 1,  sd: 10 &lt;df [10 × 3]&gt;\n2 mu: 5,  sd: 5  &lt;df [10 × 3]&gt;\n3 mu: 10,  sd: 1 &lt;df [10 × 3]&gt;\n\n\n\n16.7.3 \\(\\mu\\), \\(\\sigma\\), 표본크기\n\\(\\mu\\) 평균, \\(\\sigma\\) 표준편차, 표본크기를 모두 다르게 지정하여 난수를 추출한다. 이런 경우 pmap() 함수를 사용하고 입력 리스트가 다수라 이를 normal_list로 한번더 감싸서 이름이 붙은 리스트(named list)형태로 넣어주고, 이를 function() 함수의 내부 인수로 사용한다.\n\n## 평균, 표준편차, 표본크기를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\nnormal_size &lt;- list(10,20,30)\n\nsim_mu_sd_size_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd,\n                              \"  size: \", normal_size)\n\nnormal_list &lt;- list(normal_mean=normal_mean, normal_sd=normal_sd, normal_size=normal_size)\n\nsim_mu_sd_size_list &lt;- pmap(normal_list,\n                            function(normal_mean, normal_sd, normal_size)\n                        data.frame(mean=normal_mean, sd = normal_sd, size = normal_size,\n                            random_number = rnorm(mean=normal_mean, sd=normal_sd, n=normal_size))) %&gt;% \n  set_names(sim_mu_sd_size_name)\n\nmap_dbl(sim_mu_sd_size_list, ~length(.x$random_number))\n\nmu: 1,  sd: 10  size: 10  mu: 5,  sd: 5  size: 20 mu: 10,  sd: 1  size: 30 \n                      10                       20                       30 \n\nsim_mu_sd_size_list |&gt; enframe()\n\n# A tibble: 3 × 2\n  name                     value        \n  &lt;chr&gt;                    &lt;list&gt;       \n1 mu: 1,  sd: 10  size: 10 &lt;df [10 × 4]&gt;\n2 mu: 5,  sd: 5  size: 20  &lt;df [20 × 4]&gt;\n3 mu: 10,  sd: 1  size: 30 &lt;df [30 × 4]&gt;"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-ggplot",
    "href": "functions_purrr.html#functional-programming-purrr-ggplot",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.8 ggplot 시각화",
    "text": "16.8 ggplot 시각화\nlist-column을 활용하여 티블(tibble) 데이터프레임에 담아서 시각화를 진행해도 되고, 다른 방법으로 리스트에 담아서 이를 한장에 찍는 것도 가능하다. 4\n\nlibrary(gapminder)\n\n## 데이터 -----\nthree_country &lt;-  c(\"Korea, Rep.\", \"Japan\", \"China\")\n\ngapminder_tbl &lt;- gapminder %&gt;% \n  filter(str_detect(continent, \"Asia\")) %&gt;% \n  group_by(continent, country) %&gt;% \n  nest() %&gt;% \n  select(-continent) %&gt;% \n  filter(country %in% three_country )\n\n## 티블 데이터 시각화 -----\ngapminder_plot_tbl &lt;- gapminder_tbl %&gt;% \n  mutate(graph = map2(data, country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y)))\n\ngapminder_plot_tbl\n\n# A tibble: 3 × 4\n# Groups:   continent, country [3]\n  continent country     data              graph \n  &lt;fct&gt;     &lt;fct&gt;       &lt;list&gt;            &lt;list&gt;\n1 Asia      China       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n2 Asia      Japan       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n3 Asia      Korea, Rep. &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n\n## 리스트 데이터 시각화 -----\ngapminder_plot &lt;- map2(gapminder_tbl$data , three_country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y))\n\nwalk(gapminder_plot, print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 리스트 데이터 시각화 - 한장에 찍기 -----\ncowplot::plot_grid(plotlist = gapminder_plot)"
  },
  {
    "objectID": "functions_purrr.html#pure-vs-impure-function",
    "href": "functions_purrr.html#pure-vs-impure-function",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.10 순수한 함수 vs 불순한 함수",
    "text": "16.10 순수한 함수 vs 불순한 함수\n순수한 함수(pure function)는 입력값에만 출력값이 의존하게 되는 특성과 부수효과(side-effect)를 갖지 않는 반면 순수하지 않은 함수(impure function)는 환경에 의존하며 부수효과도 갖는다.\n\n\n순수한 함수(pure function)\n\nmin(1:100)\n\n[1] 1\n\nmean(1:100)\n\n[1] 50.5\n\n\n\n순수하지 않은 함수(impure function)\n\nSys.time()\n\n[1] \"2023-08-16 09:31:28 KST\"\n\nrnorm(10)\n\n [1] -0.94622564 -0.80235031  1.16672535 -1.66607893 -1.82123296 -0.08412394\n [7]  0.93100192  0.08507868 -0.99875167  0.98773056\n\n# write_csv(\"data/sample.csv\")\n\n\n\n\n16.10.1 무명함수와 매퍼\n\\(\\lambda\\) (람다) 함수는 무명(anonymous) 함수는 함수명을 갖는 일반적인 함수와 비교하여 함수의 좋은 점은 그대로 누리면서 함수가 많아 함수명으로 메모리가 난잡하게 지져분해지는 것을 막을 수 있다.\n무명함수로 기능르 구현한 후에 매퍼(mapper)를 사용해서 as_mapper() 명칭을 부여하여 함수처럼 사용하는 것도 가능하다. 매퍼(mapper)를 사용하는 이유를 다음과 같이 정리할 수 있다.\n\n간결함(Concise)\n가독성(Easy to read)\n재사용성(Reusable)\n\n정치인 페이스북 페이지에서 팬수를 추출한다. 그리고 이를 이름이 부은 리스트(named list)로 일자별 팬수 추이를 리스트로 준비한다. 그리고 나서 안철수, 문재인, 심상정 세 후보에 대한 최고 팬수증가를 무명함수로 계산한다.\n\nlibrary(tidyverse)\n## 데이터프레임을 리스트로 변환\nahn_df  &lt;- read_csv(\"data/fb_ahn.csv\")  %&gt;% rename(fans = ahn_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nmoon_df &lt;- read_csv(\"data/fb_moon.csv\") %&gt;% rename(fans = moon_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nsim_df  &lt;- read_csv(\"data/fb_sim.csv\")  %&gt;% rename(fans = sim_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\n\nconvert_to_list &lt;- function(df) {\n  df_fans_v &lt;- df$fans %&gt;% \n    set_names(df$fdate)\n  return(df_fans_v)\n}\n\nahn_v  &lt;- convert_to_list(ahn_df)\nmoon_v &lt;- convert_to_list(moon_df)\nsim_v  &lt;- convert_to_list(sim_df)\n\nfans_lst &lt;- list(ahn_fans  = ahn_v,\n                 moon_fans = moon_v,\n                 sim_fans  = sim_v)\n\nlistviewer::jsonedit(fans_lst)\n\n\n\n\n## 무명함수 테스트\nmap_dbl(fans_lst, ~max(.x))\n\n ahn_fans moon_fans  sim_fans \n      796      1464      2029 \n\n\nrlang_lambda_function 무명함수로 increase_1000_fans 작성해서 일별 팬수 증가가 1000명 이상인 경우 keep() 함수를 사용해서 각 후보별로 추출할 수 있다. discard() 함수를 사용해서 반대로 버려버릴 수도 있다.\n\nincrease_1000_fans &lt;- as_mapper( ~.x &gt; 1000)\n\nmap(fans_lst, ~keep(.x, increase_1000_fans))\n\n$ahn_fans\nnamed numeric(0)\n\n$moon_fans\n2017-03-28 2017-04-18 2017-04-20 \n      1464       1310       1093 \n\n$sim_fans\n2017-03-12 2017-03-13 2017-04-14 2017-04-19 2017-04-20 2017-04-21 2017-04-24 \n      1301       1079       1070       1441       1190       1025       1948 \n2017-04-25 \n      2029 \n\n\n술어논리(predicate logic)은 조건을 테스트하여 참(TRUE), 거짓(FALSE)을 반환시킨다. every, some을 사용하여 팬수가 증가한 날이 매일 1,000명이 증가했는지, 전부는 아니고 일부 특정한 날에 1,000명이 증가했는지 파악할 수 있다.\n\n## 세후보 팬수가 매일 모두 1000명 이상 증가했나요?\nmap(fans_lst, ~every(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] FALSE\n\n$sim_fans\n[1] FALSE\n\n## 세후보 팬수가 전체는 아니고 일부 특정한 날에 1000명 이상 증가했나요?\nmap(fans_lst, ~some(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] TRUE\n\n$sim_fans\n[1] TRUE\n\n\n\n16.10.2 고차 함수(High order function)\n고차 함수(High order function)는 함수의 인자로 함수를 받아 함수로 반환시키는 함수를 지칭한다. high_order_fun 함수는 함수를 인자(func)로 받아 함수를 반환시키는 고차함수다. 평균 함수(mean)를 인자로 넣어 출력값으로 mean_na() 함수를 새롭게 생성시킨다. NA가 포함된 벡터를 넣어 평균값을 계산하게 된다.\n\nhigh_order_fun &lt;- function(func){\n  function(...){\n    func(..., na.rm = TRUE)\n  }\n}\n\nmean_na &lt;- high_order_fun(mean)\nmean_na( c(NA, 1:10) )\n\n[1] 5.5\n\n\n벡터가 입력값으로 들어가서 벡터가 출력값으로 나오는 보통 함수(Regular Function)외에 고차함수는 3가지 유형이 있다.\n\n벡터 → 함수: 함수공장(Function Factory)\n함수 → 벡터: Functional - for루프를 purrr 팩키지 map() 함수로 대체\n함수 → 함수: 함수연산자(Function Operator) - Functional과 함께 사용될 경우 adverbs로서 강력한 기능을 발휘\n\n\n\n고차함수 유형\n\n\n16.10.3 부사 - safely, possibly,…\npurrr 팩키지의 대표적인 부사(adverbs)에는 possibly()와 safely()가 있다. 그외에도 silently(), surely() 등 다른 부사도 있으니 필요한 경우 purrr 팩키지 문서를 참조한다.\nsafely(mean)은 동사 함수(mean())를 받아 부사 safely()로 “부사 + 동사”로 기능이 추가된 부사 동사를 반환시킨다. 따라서, NA가 추가된 벡터를 넣을 경우 $result와 $error를 원소로 갖는 리스트를 반환시킨다.\n\nmean_safe &lt;- safely(mean)\nclass(mean_safe)\n\n[1] \"function\"\n\nmean_safe(c(NA, 1:10))\n\n$result\n[1] NA\n\n$error\nNULL\n\n\n이를 활용하여 오류처리작업을 간결하게 수행시킬 수 있다. $result와 $error을 원소로 갖는 리스트를 반환시키기 때문에 오류와 결과값을 추출하여 후속작업을 수행하여 디버깅하는데 유용하게 활용할 수 있다.\n\ntest_lst &lt;- list(\"NA\", 1,2,3,4,5)\nlog_safe &lt;- safely(log)\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"result\")\n\n[[1]]\nNULL\n\n[[2]]\n[1] 0\n\n[[3]]\n[1] 0.6931472\n\n[[4]]\n[1] 1.098612\n\n[[5]]\n[1] 1.386294\n\n[[6]]\n[1] 1.609438\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"error\")\n\n[[1]]\n&lt;simpleError in .Primitive(\"log\")(x, base): non-numeric argument to mathematical function&gt;\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n\n반면에 possibly()는 결과와 otherwise 값을 반환시켜서 오류가 발생되면 중단되는 것이 아니라 오류가 있다는 사실을 알고 예외처리시킨 후에 쭉 정상진행시킨다.\n\nmax_possibly &lt;- possibly(sum, otherwise = \"watch out\")\n\nmax_possibly(c(1:10))\n\n[1] 55\n\nmax_possibly(c(NA, 1:10))\n\n[1] NA\n\nmax_possibly(c(\"NA\", 1:10))\n\n[1] \"watch out\"\n\n\npossibly()는 부울 논리값, NA, 문자열, 숫자를 반환시킬 수 있다.\ntranspose()와 결합하여 safely(), possibly() 결과를 변형시킬 수도 있다.\n\nmap(test_lst, log_safe) %&gt;% length()\n\n[1] 6\n\nmap(test_lst, log_safe) %&gt;% transpose() %&gt;% length()\n\n[1] 2\n\n\ncompact()를 사용해서 NULL을 제거하는데, 앞서 possibly()의 인자로 otherwise=를 지정하는 경우 otherwise=NULL와 같이 정의해서 예외처리로 NULL을 만들어 내고 compact()로 정상처리된 데이터만 얻는 작업흐름을 갖춘다.\n\nnull_lst &lt;- list(1, NULL, 3, 4, NULL, 6, 7, NA)\ncompact(null_lst)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 7\n\n[[6]]\n[1] NA\n\npossibly_log &lt;- possibly(log, otherwise = NULL)\nmap(null_lst, possibly_log) %&gt;% compact()\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1.098612\n\n[[3]]\n[1] 1.386294\n\n[[4]]\n[1] 1.791759\n\n[[5]]\n[1] 1.94591\n\n[[6]]\n[1] NA"
  },
  {
    "objectID": "functions_purrr.html#fp-lambda-functin",
    "href": "functions_purrr.html#fp-lambda-functin",
    "title": "15  함수형 프로그래밍",
    "section": "\n16.2 무명함수(lambda function)와 매퍼(mapper)",
    "text": "16.2 무명함수(lambda function)와 매퍼(mapper)\n\\(\\lambda\\) (람다) 함수는 무명(anonymous) 함수는 함수명을 갖는 일반적인 함수와 비교하여 함수의 좋은 점은 그대로 누리면서 함수가 많아 함수명으로 메모리가 난잡하게 지져분해지는 것을 막을 수 있다.\n무명함수로 기능르 구현한 후에 매퍼(mapper)를 사용해서 as_mapper() 명칭을 부여하여 함수처럼 사용하는 것도 가능하다. 매퍼(mapper)를 사용하는 이유를 다음과 같이 정리할 수 있다.\n\n간결함(Concise)\n가독성(Easy to read)\n재사용성(Reusable)\n\n정치인 페이스북 페이지에서 팬수를 추출한다. 그리고 이를 이름이 부은 리스트(named list)로 일자별 팬수 추이를 리스트로 준비한다. 그리고 나서 안철수, 문재인, 심상정 세 후보에 대한 최고 팬수증가를 무명함수로 계산한다.\n\nlibrary(tidyverse)\n## 데이터프레임을 리스트로 변환\nahn_df  &lt;- read_csv(\"data/fb_ahn.csv\")  %&gt;% rename(fans = ahn_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nmoon_df &lt;- read_csv(\"data/fb_moon.csv\") %&gt;% rename(fans = moon_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nsim_df  &lt;- read_csv(\"data/fb_sim.csv\")  %&gt;% rename(fans = sim_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\n\nconvert_to_list &lt;- function(df) {\n  df_fans_v &lt;- df$fans %&gt;% \n    set_names(df$fdate)\n  return(df_fans_v)\n}\n\nahn_v  &lt;- convert_to_list(ahn_df)\nmoon_v &lt;- convert_to_list(moon_df)\nsim_v  &lt;- convert_to_list(sim_df)\n\nfans_lst &lt;- list(ahn_fans  = ahn_v,\n                 moon_fans = moon_v,\n                 sim_fans  = sim_v)\n\nlistviewer::jsonedit(fans_lst)\n\n\n\n\n## 무명함수 테스트\nmap_dbl(fans_lst, ~max(.x))\n\n ahn_fans moon_fans  sim_fans \n      796      1464      2029 \n\n\nrlang_lambda_function 무명함수로 increase_1000_fans 작성해서 일별 팬수 증가가 1000명 이상인 경우 keep() 함수를 사용해서 각 후보별로 추출할 수 있다. discard() 함수를 사용해서 반대로 버려버릴 수도 있다.\n\nincrease_1000_fans &lt;- as_mapper( ~.x &gt; 1000)\n\nmap(fans_lst, ~keep(.x, increase_1000_fans))\n\n$ahn_fans\nnamed numeric(0)\n\n$moon_fans\n2017-03-28 2017-04-18 2017-04-20 \n      1464       1310       1093 \n\n$sim_fans\n2017-03-12 2017-03-13 2017-04-14 2017-04-19 2017-04-20 2017-04-21 2017-04-24 \n      1301       1079       1070       1441       1190       1025       1948 \n2017-04-25 \n      2029 \n\n\n술어논리(predicate logic)은 조건을 테스트하여 참(TRUE), 거짓(FALSE)을 반환시킨다. every, some을 사용하여 팬수가 증가한 날이 매일 1,000명이 증가했는지, 전부는 아니고 일부 특정한 날에 1,000명이 증가했는지 파악할 수 있다.\n\n## 세후보 팬수가 매일 모두 1000명 이상 증가했나요?\nmap(fans_lst, ~every(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] FALSE\n\n$sim_fans\n[1] FALSE\n\n## 세후보 팬수가 전체는 아니고 일부 특정한 날에 1000명 이상 증가했나요?\nmap(fans_lst, ~some(.x, increase_1000_fans))\n\n$ahn_fans\n[1] FALSE\n\n$moon_fans\n[1] TRUE\n\n$sim_fans\n[1] TRUE"
  },
  {
    "objectID": "functions_purrr.html#high-order-function",
    "href": "functions_purrr.html#high-order-function",
    "title": "15  함수형 프로그래밍",
    "section": "\n16.3 고차 함수(High order function)",
    "text": "16.3 고차 함수(High order function)\n고차 함수(High order function)는 함수의 인자로 함수를 받아 함수로 반환시키는 함수를 지칭한다. high_order_fun 함수는 함수를 인자(func)로 받아 함수를 반환시키는 고차함수다. 평균 함수(mean)를 인자로 넣어 출력값으로 mean_na() 함수를 새롭게 생성시킨다. NA가 포함된 벡터를 넣어 평균값을 계산하게 된다.\n\nhigh_order_fun &lt;- function(func){\n  function(...){\n    func(..., na.rm = TRUE)\n  }\n}\n\nmean_na &lt;- high_order_fun(mean)\nmean_na( c(NA, 1:10) )\n\n[1] 5.5\n\n\n벡터가 입력값으로 들어가서 벡터가 출력값으로 나오는 보통 함수(Regular Function)외에 고차함수는 3가지 유형이 있다.\n\n벡터 → 함수: 함수공장(Function Factory)\n함수 → 벡터: Functional - for루프를 purrr 팩키지 map() 함수로 대체\n함수 → 함수: 함수연산자(Function Operator) - Functional과 함께 사용될 경우 adverbs로서 강력한 기능을 발휘\n\n\n\n고차함수 유형"
  },
  {
    "objectID": "functions_purrr.html#adverbs-safely-possibly",
    "href": "functions_purrr.html#adverbs-safely-possibly",
    "title": "15  함수형 프로그래밍",
    "section": "\n16.4 부사(adverbs) - safely, possibly,…",
    "text": "16.4 부사(adverbs) - safely, possibly,…\npurrr 팩키지의 대표적인 부사(adverbs)에는 possibly()와 safely()가 있다. 그외에도 silently(), surely() 등 다른 부사도 있으니 필요한 경우 purrr 팩키지 문서를 참조한다.\nsafely(mean)은 동사 함수(mean())를 받아 부사 safely()로 “부사 + 동사”로 기능이 추가된 부사 동사를 반환시킨다. 따라서, NA가 추가된 벡터를 넣을 경우 $result와 $error를 원소로 갖는 리스트를 반환시킨다.\n\nmean_safe &lt;- safely(mean)\nclass(mean_safe)\n\n[1] \"function\"\n\nmean_safe(c(NA, 1:10))\n\n$result\n[1] NA\n\n$error\nNULL\n\n\n이를 활용하여 오류처리작업을 간결하게 수행시킬 수 있다. $result와 $error을 원소로 갖는 리스트를 반환시키기 때문에 오류와 결과값을 추출하여 후속작업을 수행하여 디버깅하는데 유용하게 활용할 수 있다.\n\ntest_lst &lt;- list(\"NA\", 1,2,3,4,5)\nlog_safe &lt;- safely(log)\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"result\")\n\n[[1]]\nNULL\n\n[[2]]\n[1] 0\n\n[[3]]\n[1] 0.6931472\n\n[[4]]\n[1] 1.098612\n\n[[5]]\n[1] 1.386294\n\n[[6]]\n[1] 1.609438\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"error\")\n\n[[1]]\n&lt;simpleError in .Primitive(\"log\")(x, base): non-numeric argument to mathematical function&gt;\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n\n반면에 possibly()는 결과와 otherwise 값을 반환시켜서 오류가 발생되면 중단되는 것이 아니라 오류가 있다는 사실을 알고 예외처리시킨 후에 쭉 정상진행시킨다.\n\nmax_possibly &lt;- possibly(sum, otherwise = \"watch out\")\n\nmax_possibly(c(1:10))\n\n[1] 55\n\nmax_possibly(c(NA, 1:10))\n\n[1] NA\n\nmax_possibly(c(\"NA\", 1:10))\n\n[1] \"watch out\"\n\n\npossibly()는 부울 논리값, NA, 문자열, 숫자를 반환시킬 수 있다.\ntranspose()와 결합하여 safely(), possibly() 결과를 변형시킬 수도 있다.\n\nmap(test_lst, log_safe) %&gt;% length()\n\n[1] 6\n\nmap(test_lst, log_safe) %&gt;% transpose() %&gt;% length()\n\n[1] 2\n\n\ncompact()를 사용해서 NULL을 제거하는데, 앞서 possibly()의 인자로 otherwise=를 지정하는 경우 otherwise=NULL와 같이 정의해서 예외처리로 NULL을 만들어 내고 compact()로 정상처리된 데이터만 얻는 작업흐름을 갖춘다.\n\nnull_lst &lt;- list(1, NULL, 3, 4, NULL, 6, 7, NA)\ncompact(null_lst)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 7\n\n[[6]]\n[1] NA\n\npossibly_log &lt;- possibly(log, otherwise = NULL)\nmap(null_lst, possibly_log) %&gt;% compact()\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1.098612\n\n[[3]]\n[1] 1.386294\n\n[[4]]\n[1] 1.791759\n\n[[5]]\n[1] 1.94591\n\n[[6]]\n[1] NA"
  },
  {
    "objectID": "functions_ds.html#데이터프레임-함수",
    "href": "functions_ds.html#데이터프레임-함수",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.3 데이터프레임 함수",
    "text": "14.3 데이터프레임 함수\nR에서의 데이터 마스킹(Data Masking)은 tidyverse 생태계에서 데이터 문법을 담당하는 dplyr 패키지에서 핵심적인 개념이다. 데이터 마스킹을 사용하면 데이터프레임 칼럼을 $, [[ ]]를 사용하지 않고도 칼럼명으로 직접 참조할 수 있어, 데이터를 조작하고 변환할 때 훨씬 직관적이고 가독성 높은 코드를 작성할 수 있다.\npenguins 데이터프레임의 species를 데이터 마스킹 없이 조작하려면 penguins$species 혹은 penguins[['species']]와 같이 구문을 작성해야 하지만 데이터 마스킹을 사용하면 species 만으로 충분한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# 데이터 마스킹을 사용하여 펭귄종(species)이 \"Adelie\"만 추출한다.\npenguins %&gt;% filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\ndplyr 함수의 데이터 마스킹은 비표준 평가 (Non-standard evaluation, NSE)라는 개념에 기반을 두는데, 표현식을 캡처하고 난 후 바로 바로 실행되지 않고 제공된 데이터의 맥락 내에서 평가가 이루어진다. 데이터 마스킹은 강력하며 깔끔한 구문을 제공하지만, 칼럼명과 충돌할 수 있는 환경의 변수 이름이 있을 때 예기치 않은 방식으로 동작한다. 모호한 상황이 발생할 때 항상 다음과 같은 방식으로 .data$column_name 함으로써 데이터 마스킹 재정의(Overriding)를 통해 명확히 한다.\n\nspecies &lt;- \"Chinstrap\"\n\npenguins %&gt;% \n  filter(.data$species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n\n항목\n데이터 마스킹\nTidy Evaluation\n\n\n\n정의\n- 데이터프레임 칼럼명을 직접적인 변수처럼 다룰 수 있는 능력.  - $나 [[ ]] 없이 칼럼 참조를 단순화.\n- R 메타프로그래밍을 위한 프레임워크, 특히 tidyverse 에서 사용.  - 다양한 맥락에서 표현식을 캡쳐하고 평가하는 도구 제공.\n\n\n사용 사례\n- dplyr 함수에서 직접 데이터 조작.  - 코드 가독성 향상.\n- 따옴표 없는 표현식으로 사용자 정의 함수 생성.  - 표현식을 프로그래밍 방식으로 구성 및 평가.  - 표현식 평가 맥락 제어.\n\n\n구현\n- 기본적으로 tidy evaluation 메커니즘을 사용하여 구현됨.\n- rlang 패키지 enquo(), quo(), !! 등을 사용.  - 표현식과 그 환경을 캡쳐하기 위해 쿼저(Quosure) 의존.\n\n\n복잡성\n- 최종 사용자를 위해 간소화.  - 기본적인 복잡성을 추상화.\n- R 메타프로그래밍 이해 필요.  - 고급 사용자에게 더 많은 유연성 제공."
  },
  {
    "objectID": "functions_ds.html#깔끔한-평가",
    "href": "functions_ds.html#깔끔한-평가",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.4 깔끔한 평가",
    "text": "14.4 깔끔한 평가\n깔끔한 평가(Tidy evaluation)은 R tidyverse 프레임워크로, 특히 비표준 평가 (Non-Standard Evaluation, NSE)와 관련하여 tidyverse 함수로 프로그래밍하는 방법을 표준화했다. NSE는 R 함수가 표준과는 다른 맥락에서 표현식을 평가할 때 발생한다.\n\n준인용(Quasiquotation): enquo() 함수를 사용하여 표현식을 캡처하고 !!를 사용하여 표현식의 인용제거(Unquoting)을 가능케 한다.\nPronouns (대명사): .data 대명사는 데이터프레임의 칼럼명을 명시적으로 참조하는데 사용되어 모호성을 제거한다.\n함수: enquo()는 표현식을 캡처하고, quo_name()은 표현식을 문자열로 변환하며, !!는 표현식 인용제거 또는 주입작업을 수행한다.\n\n예를 들어, dplyr 패키지 filter 및 select와 같은 동사를 사용하지만 함수에 칼럼명을 작성하려는 경우, 인수로 전달될 때 이러한 동사가 어떤 칼럼을 참조하는지 명확히 하기 위해 깔끔한 평가(tidy evaluation)가 사용된다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_and_select &lt;- function(data, col_name, threshold) {\n  \n  # 칼럼명 문자열을 기호로 변환\n  col_sym &lt;- sym(col_name)\n  \n  # 준인용(quasiquotation)을 사용해서 칼럼 표현식을 캡쳐\n  col_expr &lt;- enquo(col_sym)\n  \n  # !! 연산자를 이용하여 인용제거(unquote)하고 표현식을 주입\n  data %&gt;% \n    filter(!!col_expr &gt; threshold) %&gt;% \n    select(!!col_expr)\n}\n\nfilter_and_select(penguins, \"bill_length_mm\", 55)\n\n# A tibble: 5 × 1\n  bill_length_mm\n           &lt;dbl&gt;\n1           59.6\n2           55.9\n3           55.1\n4           58  \n5           55.8\n\n\nsym() → enquo() → !!(뱅뱅) 구현방식이 Defuse-and-Inject 패턴으로 내부 동작방식은 동일하지만 사용자 구문은 { 칼럼명 }으로 깔끔해졌다.\n\nfilter_and_select_latest &lt;- function(data, col_name, threshold) {\n  \n  data %&gt;% \n    filter({{ col_name }} &gt; threshold) %&gt;% \n    select({{ col_name }})\n  \n}\n\nfilter_and_select_latest(penguins, bill_length_mm, 55)\n\n# A tibble: 5 × 1\n  bill_length_mm\n           &lt;dbl&gt;\n1           59.6\n2           55.9\n3           55.1\n4           58  \n5           55.8\n\n\n\n데이터 마스킹: 변수를 사용하여 계산하는 arrange(), filter(), summarize() 등 함수에 사용.\nTidy-selection: 변수를 선택하는 select(), relocate(), rename()과 같은 함수에 사용."
  },
  {
    "objectID": "functions_ds.html#요약통계량",
    "href": "functions_ds.html#요약통계량",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.5 요약통계량",
    "text": "14.5 요약통계량\n데이터프레임 요약통계량을 계산하는 코드를 작성해보자. 펭귄종(species) 별로 부리길이를 계산하는 코드를 작성해보자.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(부리길이_평균 = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species   부리길이_평균\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie             38.8\n2 Chinstrap          48.8\n3 Gentoo             47.5\n\n\n이번에는 그룹변수와 데이터프레임 칼럼명을 달리하여 그룹별로 평균을 계사하는 함수를 작성하여 코드로 작성해보자.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by(group_varname) |&gt; \n    summarise(부리길이_평균 = mean(varname, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_varname` is not found.\n\n\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall."
  },
  {
    "objectID": "functions_ds.html#데이터-마스킹",
    "href": "functions_ds.html#데이터-마스킹",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.3 데이터 마스킹",
    "text": "14.3 데이터 마스킹\nR에서의 데이터 마스킹(Data Masking)은 tidyverse 생태계에서 데이터 문법을 담당하는 dplyr 패키지에서 핵심적인 개념이다. 데이터 마스킹을 사용하면 데이터프레임 칼럼을 $, [[ ]]를 사용하지 않고도 칼럼명으로 직접 참조할 수 있어, 데이터를 조작하고 변환할 때 훨씬 직관적이고 가독성 높은 코드를 작성할 수 있다.\npenguins 데이터프레임의 species를 데이터 마스킹 없이 조작하려면 penguins$species 혹은 penguins[['species']]와 같이 구문을 작성해야 하지만 데이터 마스킹을 사용하면 species 만으로 충분한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# 데이터 마스킹을 사용하여 펭귄종(species)이 \"Adelie\"만 추출한다.\npenguins %&gt;% filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\ndplyr 함수의 데이터 마스킹은 비표준 평가 (Non-standard evaluation, NSE)라는 개념에 기반을 두는데, 표현식을 캡처하고 난 후 바로 바로 실행되지 않고 제공된 데이터의 맥락 내에서 평가가 이루어진다. 데이터 마스킹은 강력하며 깔끔한 구문을 제공하지만, 칼럼명과 충돌할 수 있는 환경의 변수 이름이 있을 때 예기치 않은 방식으로 동작한다. 모호한 상황이 발생할 때 항상 다음과 같은 방식으로 .data$column_name 함으로써 데이터 마스킹 재정의(Overriding)를 통해 명확히 한다.\n\nspecies &lt;- \"Chinstrap\"\n\npenguins %&gt;% \n  filter(.data$species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n\n항목\n데이터 마스킹\nTidy Evaluation\n\n\n\n정의\n- 데이터프레임 칼럼명을 직접적인 변수처럼 다룰 수 있는 능력.  - $나 [[ ]] 없이 칼럼 참조를 단순화.\n- R 메타프로그래밍을 위한 프레임워크, 특히 tidyverse 에서 사용.  - 다양한 맥락에서 표현식을 캡쳐하고 평가하는 도구 제공.\n\n\n사용 사례\n- dplyr 함수에서 직접 데이터 조작.  - 코드 가독성 향상.\n- 따옴표 없는 표현식으로 사용자 정의 함수 생성.  - 표현식을 프로그래밍 방식으로 구성 및 평가.  - 표현식 평가 맥락 제어.\n\n\n구현\n- 기본적으로 tidy evaluation 메커니즘을 사용하여 구현됨.\n- rlang 패키지 enquo(), quo(), !! 등을 사용.  - 표현식과 그 환경을 캡쳐하기 위해 쿼저(Quosure) 의존.\n\n\n복잡성\n- 최종 사용자를 위해 간소화.  - 기본적인 복잡성을 추상화.\n- R 메타프로그래밍 이해 필요.  - 고급 사용자에게 더 많은 유연성 제공."
  },
  {
    "objectID": "functions_ds.html#그룹별-평균",
    "href": "functions_ds.html#그룹별-평균",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.5 그룹별 평균",
    "text": "14.5 그룹별 평균\n\n\n\n\n데이터프레임 요약통계량을 계산하는 코드를 작성해보자. 펭귄종(species) 별로 부리길이를 계산하는 코드를 작성해보자.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(부리길이_평균 = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species   부리길이_평균\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie             38.8\n2 Chinstrap          48.8\n3 Gentoo             47.5\n\n\n이번에는 그룹변수와 데이터프레임 칼럼명을 달리하여 그룹별로 평균을 계사하는 함수를 작성하여 코드로 작성해보자.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by(group_varname) |&gt; \n    summarise(부리길이_평균 = mean(varname, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_varname` is not found.\n\n\n상기 코드가 동작하지 않는 이유는 함수에 tidyverse 코드를 함수에 단순히 전달해서 넣기 때문에 발생했다. 다음과 같이 칼러명을 포용(embracing)하는 방식으로 { 칼럼명 }과 같이 함수에 사용되는 데이터프레임 변수명을 명시적으로 작성할 경우 문제가 해결된다.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by( {{ group_varname }} ) |&gt; \n    summarise(부리길이_평균 = mean( {{ varname }}, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n\n# A tibble: 3 × 2\n  species   부리길이_평균\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie             38.8\n2 Chinstrap          48.8\n3 Gentoo             47.5"
  },
  {
    "objectID": "functions_ds.html#그래프",
    "href": "functions_ds.html#그래프",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.6 그래프",
    "text": "14.6 그래프\n\n\n\n\n데이터프레임에서 범주형 변수를 하나 선택하여 빈도수를 시각화하는 스크립트를 다음과 같이 작성한다.\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  count(island) |&gt; \n  ggplot(aes(x=island, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n범주형 변수를 막대그래프로 시각화하는 함수를 제작해보자.\n\ndraw_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    count( {{ varname }} ) |&gt; \n    ggplot(aes(x = {{ varname }}, y = n )) + \n      geom_col()\n}\n\npenguins |&gt; draw_bar_chart(year)\n\n\n\n\n\n\n\n함수내에서 새로운 변수 이름을 생성하는 경우 := 연산자를 사용해야 한다. 깔끔한 평가(tidy evaluation)에서 = 와 동일한 역할을 수행하는 것이 := 이기 때문이다. 예를 들어, 펭귄이 서식하고 있는 섬을 기준으로 빈도수를 내림차순 막대그래프를 작성할 경우, 함수 내부에서 범주형 변수를 다시 재정의해야 하는데 이 경우 := 연산자의 도입이 필요하다.\n\nlibrary(forcats)\n\norder_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    mutate({{ varname }} :=  fct_rev(fct_infreq( {{ varname }} ))) |&gt;\n    ggplot(aes(y = {{ varname }} )) + \n      geom_bar()\n}\n\npenguins |&gt; order_bar_chart(island)\n\n\n\n\n\n\n\n그래프를 작성할 때 거의 항상 등장하는 문제가 그래프 x-축, y-축 라벨을 붙이고 그래프 제목, 범례 등 텍스트를 넣어야 한다. 이런 경우 stringr, glue 패키지의 다양한 함수를 깔끔한 평가(tidy evaluation)에서 지원하는 기능이 rlang 패키지 englue() 함수다.\n\ndraw_bar_chart &lt;- function(dataframe, varname, penguin_species) {\n  \n  title_label &lt;- rlang::englue(\"남극에 서식하고 있는 펭귄 종 ({{ varname }}) {penguin_species} 빈도수\")\n  \n  dataframe |&gt; \n    filter({{ varname }} == penguin_species) |&gt; \n    count( island ) |&gt; \n    ggplot(aes(x = island, y = n )) + \n      geom_col() +\n      labs(title = title_label)\n}\n\npenguins |&gt; draw_bar_chart(species, \"Adelie\")"
  },
  {
    "objectID": "functions_ds.html#역사-1",
    "href": "functions_ds.html#역사-1",
    "title": "\n14  데이터 과학 함수\n",
    "section": "\n14.7 역사",
    "text": "14.7 역사\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall."
  },
  {
    "objectID": "functions_purrr.html#map-reduce-apply",
    "href": "functions_purrr.html#map-reduce-apply",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.4 Map(), Reduce()와 *apply() 함수",
    "text": "16.4 Map(), Reduce()와 *apply() 함수\n함수를 인자로 받는 함수를 고차함수(High-order function)라고 부른다. 대표적으로 Map(), Reduce()가 있다. 숫자 하나가 아닌 벡터에 대한 제곱근을 구하기 위해서 Map 함수를 사용한다. 2 3\n\n# 제곱근 함수 -------------------------------------------\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\n# 벡터에 대한 제곱근 계산 \n\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nMap(find_root_recur, numbers, init=1, eps = 10^-10)\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 8\n\n[[6]]\n[1] 9\n\n\n숫자 하나를 받는 함수가 아니라, 벡터를 인자로 받아 제곱근을 계산하는 함수를 작성할 경우 함수 내부에서 함수를 인자로 받을 수 있도록 Map 함수를 활용한다.\n\n# `Map` 벡터 제곱근 계산\n\nfind_vec_root_recur &lt;- function(numbers, init, eps = 10^(-10)){\n    return(Map(find_root_recur, numbers, init, eps))\n}\n\nnumbers_z &lt;- c(9, 16, 25, 49, 121)\nfind_vec_root_recur(numbers_z, init=1, eps=10^(-10))\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 11\n\n\n이러한 패턴이 많이 활용되어 *apply 함수가 있어, 이전에 많이 사용했을 것이다. 벡터를 인자로 먼저 넣고, 함수명을 두번째 인자로 넣고, 함수에 들어갈 매개변수를 순서대로 쭉 나열하여 lapply, sapply 함수에 넣는다.\n\n# `lapply` 활용 제급근 계산\n\nlapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 11\n\nsapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n\n[1]  3  4  5  7 11\n\n\nReduce 함수도 삶을 편안하게 할 수 있는, 루프를 회피하는 또다른 방법이다. 이름에서 알 수 있듯이 numbers_z 벡터 원소 각각에 대해 해당 연산작업 +, %%을 수행시킨다. %%는 나머지 연산자로 기본디폴트 설정으로 \\(\\frac{10}{7}\\)로 몫 대신에 나머지 3을 우선 계산하고, 그 다음으로 \\(\\frac{3}{5}\\)로 최종 나머지 3을 순차적으로 계산하여 결과를 도출한다.\n\n# Reduce ----------------------------------------------\nnumbers_z\n\n[1]   9  16  25  49 121\n\nReduce(`+`, numbers_z)\n\n[1] 220\n\nnumbers_z &lt;- c(10,7,5)\nReduce(`%%`, numbers_z)\n\n[1] 3"
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr",
    "href": "functions_purrr.html#functional-programming-purrr",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.5 purrr 팩키지",
    "text": "16.5 purrr 팩키지\n*apply 계열 함수는 각각의 자료형에 맞춰 기억하기가 쉽지 않아, 매번 도움말을 찾아 확인하고 코딩을 해야하는 번거러움이 많다. 데이터 분석을 함수형 프로그래밍 패러다임으로 실행하도록 purrr 팩키지가 개발되었다. 이를 통해 데이터 분석 작업이 수월하게 되어 저녁이 있는 삶이 길어질 것으로 기대된다.\n\n16.5.1 purrr 헬로월드\npurrr 팩키지를 불러와서 map_dbl() 함수에 구문에 맞게 작성하면 동일한 결과를 깔끔하게 얻을 수 있다. 즉,\n\n\nmap_dbl(): 벡터, 데이터프레임, 리스트에 대해 함수를 원소별로 적용시켜 결과를 double 숫자형으로 출력시킨다.\n\nnumbers: 함수를 각 원소별로 적용시킬 벡터 입력값\n\nfind_root_recur: 앞서 작성한 뉴톤 방법으로 제곱근을 수치적으로 구하는 사용자 정의함수.\n\ninit=1, eps = 10^-10: 뉴톤 방법을 구현한 사용자 정의함수에 필요한 초기값.\n\n\nlibrary(purrr)\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nmap_dbl(numbers, find_root_recur, init=1, eps = 10^-10)\n\n[1] 4 5 6 7 8 9"
  },
  {
    "objectID": "functions_purrr.html#fp-theory-practice",
    "href": "functions_purrr.html#fp-theory-practice",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.9 함수형 프로그래밍 이론과 실제",
    "text": "16.9 함수형 프로그래밍 이론과 실제\n함수는 다음과 같이 될 수도 있어 함수형 프로그래밍 언어가 된다. 5\n\n함수의 인자\n함수로 반환\n리스트에 저장\n변수에 저장\n무명함수\n조작할 수 있다.\n\nFirstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do anything with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.\n\n\n\n\n\n\nJohn Chambers 창시자가 말하는 R 계산의 기본원칙\n\n\n\n\n존재하는 모든 것은 객체다. (Everything that exists is an object.)\n일어나는 모든 것은 함수호출이다. (Everything that happens is a function call.)\n\n\nlibrary(tidyverse)\nclass(`%&gt;%`)\n\n[1] \"function\"\n\nclass(`$`)\n\n[1] \"function\"\n\nclass(`&lt;-`)\n\n[1] \"function\"\n\nclass(`+`)\n\n[1] \"function\""
  },
  {
    "objectID": "functions_purrr.html#fp-clean-code",
    "href": "functions_purrr.html#fp-clean-code",
    "title": "16  함수형 프로그래밍",
    "section": "\n16.11 깨끗한 코드",
    "text": "16.11 깨끗한 코드\nround_mean() 함수를 compose() 함수를 사용해서 mean() 함수로 평균을 구한 후에 round()함수로 반올림하는 코드를 다음과 같이 쉽게 작성할 수 있다. 6\n\nround_mean &lt;- compose(round, mean)\nround_mean(1:10)\n\n[1] 6\n\n\n두번째 사례로 전형적인 데이터 분석 사례로 lm() → anova() → tidy()를 통해 한방에 선형회귀 모형 산출물을 깨끗한 코드로 작성하는 사례를 살펴보자.\nmtcars 데이터셋에서 연비 예측에 변수 두개를 넣고 일반적인 lm() 선형예측모형 제작방식과 동일하게 인자를 넣는다.\n\nclean_lm &lt;- compose(broom::tidy, anova, lm)\nclean_lm(mpg ~ hp + wt, data=mtcars)\n\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 hp            1  678. 678.       101.   5.99e-11\n2 wt            1  253. 253.        37.6  1.12e- 6\n3 Residuals    29  195.   6.73      NA   NA       \n\n\ncompose()를 통해 함수를 조합하는 경우 함수의 인자를 함께 전달해야될 경우가 있다. 이와 같은 경우 partial()을 사용해서 인자를 넘기는 함수를 제작하여 compose()에 넣어준다.\n\nrobust_round_mean &lt;- compose(\n  partial(round, digits=1),\n  partial(mean, na.rm=TRUE))\nrobust_round_mean(c(NA, 1:10))\n\n[1] 5.5\n\n\n리스트 칼럼(list-column)과 결합하여 모형에서 나온 데이터 분석결과를 깔끔하게 코드로 제작해보자. 먼저 lm을 돌려 모형 요약하는 함수 summary를 통해 r.squared값을 추출하는 함수를 summary_lm으로 제작한다.\n그리고 나서 nest() 함수로 리스트 칼럼(list-column)을 만들고 두개의 집단 수동/자동을 나타내는 am 변수를 그룹으로 삼아 두 집단에 속한 수동/자동 데이터에 대한 선형 회귀모형을 적합시키고 나서 “r.squared”값을 추출하여 이를 티블 데이터프레임에 저장시킨다.\n\nsummary_lm &lt;- compose(summary, lm) \n\nmtcars %&gt;%\n  group_by(am) %&gt;%\n  nest() %&gt;%\n  mutate(lm_mod = map(data, ~ summary_lm(mpg ~ hp + wt, data = .x)),\n         r_squared = map(lm_mod, \"r.squared\")) %&gt;%\n  unnest(r_squared)\n\n# A tibble: 2 × 4\n# Groups:   am [2]\n     am data               lm_mod     r_squared\n  &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n1     1 &lt;tibble [13 × 10]&gt; &lt;smmry.lm&gt;     0.837\n2     0 &lt;tibble [19 × 10]&gt; &lt;smmry.lm&gt;     0.768"
  },
  {
    "objectID": "models.html#회귀분석",
    "href": "models.html#회귀분석",
    "title": "\n14  모형\n",
    "section": "\n14.1 회귀분석",
    "text": "14.1 회귀분석\n회귀분석은 갤톤(Galton) 부모와 자식의 신장간의 관계를 회귀식으로 표현한 데이터셋이 유명하다. (Caffo 2015) (Friendly 2023) 부모의 신장을 기초로 자녀의 신장을 예측하는 회귀식을 구하기 전에 산점도를 통해 관계를 살펴보면 다음과 같다. 성별에 대한 신장의 차이도 산점도를 통해 시각적으로 확인된다.\n\nlibrary(tidyverse)\nlibrary(HistData)\ndata(GaltonFamilies)\n\nlibrary(ggplot2)\n\n## 1. 산점도 \n# 성별 색상으로 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n\n\n\n\n\n\n# 다른 산점도로 성별 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight, colours=gender)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\") +\n    facet_wrap(~gender)\n\n\n\n\n\n\n# 성별 상관없는 회귀직선\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point() +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n\n\n\n\n\n\n\n선형대수로 회귀계수를 추정하는 문제를 풀면 다음과 같이 정의된다. 한번 미분해서 \\(\\nabla f(\\beta ) = -2Xy + X^t X \\beta =0\\) 0으로 놓고 푼 값은 최소값이 되는데 이유는 \\(\\beta\\)에 대해서 두번 미분하게 되면 \\(2 X^t X\\) 로 양수가 되기 때문이다.\n\\[f(\\beta ) = ||y - \\beta X ||^2 = (y - \\beta X)^t (y - \\beta X) = y^t y - 2 y^t X^t \\beta + \\beta^t X^t X \\beta\\]\n\\[\\nabla f(\\beta ) = -2Xy + X^t X \\beta\\]\n\\[\\beta = (X^t X)^{-1} X^t y \\]\n위에서 정의된 방식으로 수식을 정의하고 이를 R로 코딩하면 회귀계수를 다음과 같이 구할 수 있다.\n\n## 2. 회귀분석\n# 선형대수 수식으로 계산\n\nx &lt;- GaltonFamilies$midparentHeight\ny &lt;- GaltonFamilies$childHeight\n\nx &lt;- cbind(1, x)\n\nsolve(t(x) %*% x) %*% t(x) %*% y\n\n        [,1]\n  22.6362405\nx  0.6373609\n\n\n이를 lm 함수를 사용해서 다시 풀면 위에서 선형대수 수식으로 계산한 것과 동일함을 확인하게 된다.\n\n# lm 함수를 통해 계산\n\nlm(childHeight ~ midparentHeight, data=GaltonFamilies) %&gt;% coef()\n\n    (Intercept) midparentHeight \n     22.6362405       0.6373609"
  },
  {
    "objectID": "models.html#r-population-growth",
    "href": "models.html#r-population-growth",
    "title": "\n13  모형\n",
    "section": "\n13.2 시군 인구증가",
    "text": "13.2 시군 인구증가\n통계청에서 제공하는 연도별 인구데이터를 바탕으로 2005년부터 2016년까지 인구변동 데이터를 바탕으로 시군별 인구변동에 대해 이해한다.\n국가통계포털, KOSIS에서 “통계표” 검색창에 “도시지역 인구현황(시군구)” 입력하게 되면 2005년부터 2016까지 시군구별 인구데이터를 받아올 수 있다. 직접 다운로드 링크\n\n13.2.1 인구변동이 많은 시군\n데이터 가져오기 및 데이터 전처리\n인구변동이 많은 시군 통계 분석을 위해 필요한 팩키지를 불러 읽어온다. 통계청 KOSIS에서 다운로드 받은 파일을 data 폴더 아래 저장하고 나서, 전처리 작업을 수행한다. 서울특별시를 포함한 광역시에 포함된 군은 데이터 분석에 제외할 것이기 때문에 stringr 정규표현식 기능을 활용하여 깔끔하게 향후 데이터분석을 위한 데이터프레임으로 정리한다.\n\n# 0. 환경설정 -----------\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggpubr)\nextrafont::loadfonts()\n\n# 1. 데이터 가져오기 ---------\n\nkor_dat &lt;- read_excel(\"data/korea-pop-zipf-law.xlsx\", sheet=\"데이터\", skip=1) \n\nkor_dat &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` !=\"전국\") %&gt;% \n  filter(`인구현황별` ==\"전체인구(A)\")\n\n# 2. 데이터 전처리 ---------\n## 2.1. 시군 뽑아내기 -------\n\nsigungu_v &lt;- kor_dat %&gt;% count(`소재지(시군구)별`) %&gt;% \n  pull(`소재지(시군구)별`)\n\nsigungu_v &lt;- sigungu_v[str_detect(sigungu_v, \"시$|군$\")]\n\n## 2.2. 시군 데이터 전처리 -------\n\nkor_df &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` %in% sigungu_v) %&gt;% \n  filter(!is.na(`2016 년`)) %&gt;% group_by(`소재지(시군구)별`) %&gt;% \n  summarise_if(is.numeric, mean) %&gt;% \n  rename(시군 = `소재지(시군구)별`)\n\n## 2.3. 시각화 데이터 변환  -------\nkor_lng_df &lt;- kor_df %&gt;% \n  gather(연도, 인구수, -시군) %&gt;% \n  mutate(연도 = as.numeric(str_extract(연도, \"[0-9]+\")))\n\n\n13.2.2 문제점\n대한민국 시군이 166 이기 때문에 인구변동이 많은 시군을 추출하기 위해서 서울시는 천만명 근처이고 가장 작은 울릉군은 2016년 기준 10,001 명이라 편차가 매우 크다. 따라서, 이를 시각화를 하게 되면 문제점이 한눈에 파악된다.\n\n# 3. 시각화 ----------------\n## 3.1. 시군별 연도별 인구수 변화\nkor_lng_df %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=시군, group=시군)) +\n    geom_point() +\n    geom_line() +\n    theme_pubclean(base_family=\"NanumGothic\") +\n    scale_y_log10(labels=scales::comma) +\n    scale_x_date(date_labels = \"%Y\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\")\n\n\n\n\n\n\n\n\n13.2.3 FP 통한 문제해결\n이러한 문제점에 대해 가장 많이 활용되는 기법이 자료구조로 티블(tibble)을 도입하고, 데이터 분석을 위한 방법으로 함수형 프로그래밍을 조합하는 것이다.\n티블 자료구조\n데이터프레임을 기존 폭넓은(wide) 형태를 긴(long) 형태로 변환하고 이를 nest()를 적용시키면 함수형 프로그램을 적용시킬 수 있는 자료구조가 된다. 더불어, 선형회귀모형을 각 시군별로 적용시킬 예정이라 회귀모형 함수도 생성시켜 둔다.\n그리고 나서 전체 시군별로 연도별 인구변화를 회귀분석으로 수행하여 수행결과를 broom 팩키지의 tidy, glance, augment 함수를 활용하여 데이터와 모형분석결과를 결합시킨다.\n\n# 4. 인구변화 심한 시군 추출 ----------------\n## 4.1. 데이터 \nkor_sigun_tb &lt;- kor_lng_df %&gt;% group_by(시군) %&gt;% \n  nest()\n\n## 4.2. 모형 - 선형회귀모형\nsigun_model &lt;- function(df) {\n  lm(인구수 ~ 연도, data=df)\n}\n\n## 4.3. 데이터 + 모형 결합\nkor_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  mutate(model = map(data, sigun_model)) %&gt;% \n  mutate(\n    tidy    = map(model, broom::tidy),\n    glance  = map(model, broom::glance),\n    결정계수 = glance %&gt;% map_dbl(\"r.squared\"),\n    augment = map(model, broom::augment)\n  )\n\nDT::datatable(kor_sigun_tb)\n\n\n\n\n\n급성장 시군 - 결정계수(\\(R^2\\))\n회귀분석 결정계수(\\(R^2\\))기준 인구가 연도별로 높은 상관관계를 갖는 시군과 그렇지 않는 상위 하위 5개 시군을 뽑아 시각화 해보자.\n\n## 4.4. 고성장 및 정체 시군 추출\n\nkor_high_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"고성장\")\n\nkor_low_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(-5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"저성장\")\n\nkor_growth_sigun_tb &lt;- bind_rows(kor_high_growth_sigun_tb, kor_low_growth_sigun_tb)\n\n## 4.5. 고성장 및 정체 시군 시각화\n\nkor_growth_sigun_tb %&gt;% \n  mutate(구분 = factor(구분),\n         시군 = fct_reorder(시군, 결정계수)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=시군)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_log10(labels=scales::comma) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~시군, nrow=2, scale=\"free\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n급성장 시군 - 회귀계수\n회귀분석 결정계수(\\(R^2\\))기준이 아닌 회귀계수(\\(\\beta_1\\))를 기준으로 상위 5개, 하위 5개 시군을 뽑아 연도별 인구변화를 시각화해보자.\n\n# 5. 인구 증가 및 인구감소 ----------------\nkor_sigun_reg_df &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0)\n\ntop10_plus_sigun &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(5, 증감) %&gt;% \n  pull(시군)\n\ntop10_minus_sigun &lt;-kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(-5, 증감) %&gt;% \n  pull(시군)\n\nkor_lng_df %&gt;% \n  left_join(kor_sigun_reg_df, by=\"시군\") %&gt;% \n  filter(시군 %in% c(top10_plus_sigun, top10_minus_sigun)) %&gt;%\n  mutate(구분 = ifelse(시군 %in% top10_plus_sigun, \"성장\", \"역성장\"),\n         시군 = fct_reorder(시군, -증감)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=구분)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_continuous(labels=scales::comma) +\n  scale_x_date(date_labels = \"%Y\") +\n  theme(legend.position = \"none\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  facet_wrap(~시군, scale=\"free\", nrow=2)"
  },
  {
    "objectID": "whole_game.html#데이터-가져오기",
    "href": "whole_game.html#데이터-가져오기",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.1 데이터 가져오기",
    "text": "3.1 데이터 가져오기\nMathematics of the Coxcombs"
  },
  {
    "objectID": "whole_game.html#배경",
    "href": "whole_game.html#배경",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.2 배경",
    "text": "3.2 배경\n크림 전쟁은 1853년부터 1856년까지 일어난 큰 전쟁이었다. 한쪽에는 러시아, 반면 다른 한쪽에는 영국, 프랑스, 오스만 제국 (현대 투르키에), 그리고 나중에 사르디니아 (현대 이탈리아의 일부)가 동맹을 구성하여 전쟁을 치뤘다. 전쟁이 바로 시작된 이유는 러시아가 오스만 제국 내 정교회 신자들을 보호하려 하려는 명분을 내세웠지만, 사실 더많은 영토를 차지하기 위함이였다. 양측간 전쟁은 흑해를 두고 남하하는 러시아에 맞서 동맹군이 크림반도에서 발생하여 “크림전쟁”(Crimean War)으로 불린다. 영화로 소개된 경기병대의 돌격 (“Charge of the Light Brigade”), 영국 간호사 플로렌스 나이팅게일의 활약, 전신과 철도의 본격적인 도입으로 큰 의미를 갖는 전쟁이기도 하다. 많은 전투와 많은 사람들이 죽은 후, 1856년 파리 조약으로 전쟁은 마무리되어, 러시아 확장은 잠시 멈추게 돼었고, 오스만 제국도 한숨 돌린 계기가 되었다.\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원(Scutari Hospital, Turkey)은 영국 군 병원으로 개조되었다. 크림전쟁에서 부상을 당한 수많은 병사가 치료를 위해 이곳으로 보내졌지만, 병자와 부상병들을 감당할 수 있도록 설계되지 않았고 제대로된 역할도 수행하지 못했다. 1854년 나이팅게일이 간호사 일행과 함께 도착했을 때, 비위생적인 환경과 고통받는 병사들을 보고 경악했다. 나이팅게일의 스쿠타리 병원에서 경험은 병원과 의료 서비스를 개선하여 이와 같은 고통과 비극이 재발하지 않도록 향후 프로젝트의 중요한 동기와 방향이 되었다.\n\n\n스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)\n\n환자의 사망율을 42%에서 2%로 낮추고 집중치료실(ICU)을 설치하여 상태가 중한 환자를 격리하여 집중관리하는 등 근대적인 간호체계를 수립하는 데 기여하였다."
  },
  {
    "objectID": "whole_game.html#데이터",
    "href": "whole_game.html#데이터",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.3 데이터",
    "text": "3.3 데이터\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원에서 몇년간에 걸쳐 수작업으로 종이에 분석가능한 형태의 자료를 만들어내는 것은 결코 쉬운 작업이 아니다.\n\n\n원본 데이터"
  },
  {
    "objectID": "whole_game.html#그래프-진화",
    "href": "whole_game.html#그래프-진화",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.4 그래프 진화",
    "text": "3.4 그래프 진화\n출처: How Florence Nightingale Changed Data Visualization Forever - The celebrated nurse improved public health through her groundbreaking use of graphic storytelling\n복잡한 논거를 제시하는 대신 구체적인 주장에 데이터 시각화와 데이터 스토리텔링(Storytelling)을 통해 청중에 한걸음 더 다가섰다. 나이팅게일의 스토리텔링은 열악한 위생 상태와 과밀로 인해 불필요한 죽음이 얼마나 많이 발생하는지 이해하기 쉬운 비교를 통해 이야기를 구성해서 설득해 나갔다. 예를 들어, 군대 사망률을 민간인 사망률(유사한 환경의 맨체스터)과 비교하는 프레임을 제시하고, 군대 막사에서 생활하는 평시 병사들이 비슷한 연령대 민간인 남성보다 더 높은 비율로 사망하는 것을 제시했다. 이를 통해, 데이터가 보여주는 현실을 부정할 수 없게 만들었고, 군대 행정에 극적인 개혁을 이끌어냈다.\n\n\n\n\n(a) 막대그래프\n\n\n\n(b) 맨체스터 사망\n\n\n\n(c) 빅토리아 여왕 보고(I)\n\n\n\n(d) 빅토리아 여왕 보고(II)\n\n\n\n(e) 빅토리아 여왕 보고(III)\n\n\n그림 3.1: 나이팅게일 그래프 진화과정"
  },
  {
    "objectID": "whole_game.html#성과와-영향",
    "href": "whole_game.html#성과와-영향",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.5 성과와 영향",
    "text": "3.5 성과와 영향\n나이팅게일 캠페인이 민간 공중보건에 미친 가장 큰 영향은 실현되기까지 오랜 기간에 걸쳐 다각도로 검토되었고, 마침내 1875년 영국 공중보건법(British Public Health Act)에 법제화되었다. 이 법에는 잘 정비된 하수도, 깨끗한 수돗물, 건축법 규제 등의 요건이 담겨있다. 질병에 대한 면역력을 강화하는 백신과 농작물 수확량을 획기적으로 늘리는 인공비료 개발과 함께 이 제도적인 노력으로 평균 수명을 두 배로 늘리는 원동력이 되었다."
  },
  {
    "objectID": "whole_game.html",
    "href": "whole_game.html",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "",
    "text": "4 데이터\nrladies/spain_nightingale GitHub 저장소에서 엑셀 형태로 된 데이터를 가져와서 전처리할 수 있다.\nlibrary(tidyverse)\nlibrary(readxl)\n\ndeath_raw &lt;- read_excel(\"data/datos_florence.xlsx\", sheet = \"Sheet1\", skip = 1)\n\ndeath_tbl &lt;- death_raw |&gt; \n  janitor::clean_names() |&gt; \n  set_names(c(\"Month\", \"Army\", \"Disease\", \"Wounds\", \"Other\", \"Disease.rate\", \"Wounds.rate\", \"Other.rate\")) |&gt; \n  mutate(Date = lubridate::my(Month)) |&gt; \n  separate(Month, into = c(\"Month\", \"Year\"), sep = \" |_\") |&gt; \n  select(Date, Month, Year, everything()) \n\ndeath_tbl\n\n# A tibble: 24 × 10\n   Date       Month Year   Army Disease Wounds Other Disease.rate Wounds.rate\n   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1854-04-01 Apr   1854   8571       1      0     5          1.4         0  \n 2 1854-05-01 May   1854  23333      12      0     9          6.2         0  \n 3 1854-06-01 Jun   1854  28333      11      0     6          4.7         0  \n 4 1854-07-01 Jul   1854  28722     359      0    23        150           0  \n 5 1854-08-01 Aug   1854  30246     828      1    30        328.          0.4\n 6 1854-09-01 Sep   1854  30290     788     81    70        312.         32.1\n 7 1854-10-01 Oct   1854  30643     503    132   128        197          51.7\n 8 1854-11-01 Nov   1854  29736     844    287   106        341.        116. \n 9 1854-12-01 Dec   1854  32779    1725    114   131        632.         41.7\n10 1855-01-01 Jan   1855  32393    2761     83   324       1023.         30.7\n# ℹ 14 more rows\n# ℹ 1 more variable: Other.rate &lt;dbl&gt;\nHistDate 패키지에 동일한 데이터셋이 잘 정제되어 있어 이를 바로 활용해도 좋다.\nlibrary(HistData)\n\nHistData::Nightingale |&gt; \n  as_tibble()\n\n# A tibble: 24 × 10\n   Date       Month  Year  Army Disease Wounds Other Disease.rate Wounds.rate\n   &lt;date&gt;     &lt;ord&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1854-04-01 Apr    1854  8571       1      0     5          1.4         0  \n 2 1854-05-01 May    1854 23333      12      0     9          6.2         0  \n 3 1854-06-01 Jun    1854 28333      11      0     6          4.7         0  \n 4 1854-07-01 Jul    1854 28722     359      0    23        150           0  \n 5 1854-08-01 Aug    1854 30246     828      1    30        328.          0.4\n 6 1854-09-01 Sep    1854 30290     788     81    70        312.         32.1\n 7 1854-10-01 Oct    1854 30643     503    132   128        197          51.7\n 8 1854-11-01 Nov    1854 29736     844    287   106        341.        116. \n 9 1854-12-01 Dec    1854 32779    1725    114   131        632.         41.7\n10 1855-01-01 Jan    1855 32393    2761     83   324       1023.         30.7\n# ℹ 14 more rows\n# ℹ 1 more variable: Other.rate &lt;dbl&gt;"
  },
  {
    "objectID": "whole_game.html#데이터와-사투",
    "href": "whole_game.html#데이터와-사투",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n4.1 데이터와 사투",
    "text": "4.1 데이터와 사투\n\ndeath_viz &lt;- death_tbl %&gt;% \n  select(Date, Disease.rate, Wounds.rate, Other.rate) %&gt;% \n  pivot_longer(-Date, names_to = \"사망원인\", values_to = \"사망자수\") |&gt; \n  mutate(사망원인 = str_replace_all(사망원인, \"\\\\.rate\", \"\"), \n         체제 = ifelse(Date &lt;= as.Date(\"1855-03-01\"), \"이전\", \"이후\")) %&gt;% \n  mutate(체제 = factor(체제, levels = c(\"이전\", \"이후\"))) %&gt;%  \n  mutate(해당월 = month(Date, label = TRUE, abbr = TRUE))\n\ndeath_viz\n\n# A tibble: 72 × 5\n   Date       사망원인 사망자수 체제  해당월\n   &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;fct&gt; &lt;ord&gt; \n 1 1854-04-01 Disease       1.4 이전  4     \n 2 1854-04-01 Wounds        0   이전  4     \n 3 1854-04-01 Other         7   이전  4     \n 4 1854-05-01 Disease       6.2 이전  5     \n 5 1854-05-01 Wounds        0   이전  5     \n 6 1854-05-01 Other         4.6 이전  5     \n 7 1854-06-01 Disease       4.7 이전  6     \n 8 1854-06-01 Wounds        0   이전  6     \n 9 1854-06-01 Other         2.5 이전  6     \n10 1854-07-01 Disease     150   이전  7     \n# ℹ 62 more rows"
  },
  {
    "objectID": "whole_game.html#시각화",
    "href": "whole_game.html#시각화",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.3 시각화",
    "text": "3.3 시각화\n‘ggplot2’ 패키지를 이용하여 크림전쟁 나이팅게일 활약상을 담은 데이터를 시각화한다. 나이팅게일 활약 전과 후로 데이터(death_viz)를 나눠 “크림전쟁 병사 사망원인”에 대한 극좌표계 시각화를 통해 이해하기 쉬운 설득력있는 시각화 결과물을 제시하고 있다. 추가적으로, ‘showtext’ 패키지로 구글 “Noto Serif KR” 글꼴을 선택적용하고, ‘hrbrthemes’ 라이브러리를 이용하여 뒷 배경 검정색을 사용하여 붉은색 질병으로 인한 사망자수 확연한 감소를 시각적으로 강조한다.\n\nlibrary(hrbrthemes) \nlibrary(showtext)\nshowtext.auto()\nfont_add_google(name = \"Noto Serif KR\", family = \"noto_serif\")\nnoto_font &lt;- \"noto_serif\"\n\ndeath_gg &lt;- death_viz %&gt;% \n  ggplot(aes(x = 해당월, y = 사망자수, fill = 사망원인)) +\n  geom_col(color = \"grey20\") + \n  theme_modern_rc(base_family = noto_font, subtitle_family = noto_font) + \n  scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\"), name = \"\") +\n  scale_y_sqrt() +\n  facet_wrap(~ 체제) + \n  coord_equal(ratio = 1) +  \n  coord_polar() +\n  labs(title = \"크림전쟁 병사 사망원인\", \n       subtitle = \"데이터 시각화와 커뮤니케이션\", \n       caption = \"데이터 출처: 크림전쟁 사망자\") + \n  theme(legend.position = \"top\", \n        text = element_text(family = noto_font, size = 18),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        plot.margin = unit(rep(0.7, 4), \"cm\"),\n        plot.title = element_text(color = \"white\", family = noto_font, size = 25),\n        plot.caption = element_text(color = \"grey70\", family = noto_font, size = 12),\n        plot.subtitle = element_text(color = \"grey70\", size = 13),\n        legend.text = element_text(color = \"white\", size = 15),\n        strip.text = element_text(color = \"white\", size = 25, face = \"bold\", family = noto_font, hjust = 0.5))\n\ndeath_gg\n\n\n\n\n\n\n\n\n3.3.1 선그래프\n나이팅게일은 간호 분야의 선구자로 잘 알려져 있지만, 통계학자로서 “콕스콤(CoxComb)” 또는 “장미 다이어그램”(Rose Diagram)으로 알려진 원그래프를 제시하였지만 현재는 시간의 흐름에 따라 병사 사망자수 변화를 조치 전후로 명확히 하는 방법으로 선그래프가 기본 기법으로 자리잡고 있다.\n\nextrafont::loadfonts()\n\ndeath_new_gg &lt;- death_viz |&gt; \n  ggplot(aes(x = Date, y = 사망자수, color = 사망원인)) +\n    geom_line() +\n    geom_point() +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)          \n\ndeath_new_gg\n\n\n\n\n\n\n\n\n3.3.2 막대그래프\n동일한 정보를 막대그래프를 통해 시각화를 할 수도 있다. 원그래프와 비교하여 보면 명확하게 사망자수를 직관적으로 비교할 수 있다는 점에서 큰 장점이 있다.\n\ndeath_viz |&gt; \n  ggplot() +\n    geom_col(aes(x = Date, y = 사망자수, fill = 사망원인), colour=\"white\") +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\")) + \n    # theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)"
  },
  {
    "objectID": "whole_game.html#나이팅게일-신화탄생",
    "href": "whole_game.html#나이팅게일-신화탄생",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.1 나이팅게일 신화탄생",
    "text": "3.1 나이팅게일 신화탄생\n\n3.1.1 배경\n크림 전쟁은 1853년부터 1856년까지 일어난 큰 전쟁이었다. 한쪽에는 러시아, 반면 다른 한쪽에는 영국, 프랑스, 오스만 제국 (현대 투르키에), 그리고 나중에 사르디니아 (현대 이탈리아의 일부)가 동맹을 구성하여 전쟁을 치뤘다. 전쟁이 바로 시작된 이유는 러시아가 오스만 제국 내 정교회 신자들을 보호하려 하려는 명분을 내세웠지만, 사실 더많은 영토를 차지하기 위함이였다. 양측간 전쟁은 흑해를 두고 남하하는 러시아에 맞서 동맹군이 크림반도에서 발생하여 “크림전쟁”(Crimean War)으로 불린다. 영화로 소개된 경기병대의 돌격 (“Charge of the Light Brigade”), 영국 간호사 플로렌스 나이팅게일의 활약, 전신과 철도의 본격적인 도입으로 큰 의미를 갖는 전쟁이기도 하다. 많은 전투와 많은 사람들이 죽은 후, 1856년 파리 조약으로 전쟁은 마무리되어, 러시아 확장은 잠시 멈추게 돼었고, 오스만 제국도 한숨 돌린 계기가 되었다.\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원(Scutari Hospital, Turkey)은 영국 군 병원으로 개조되었다. 크림전쟁에서 부상을 당한 수많은 병사가 치료를 위해 이곳으로 보내졌지만, 병자와 부상병들을 감당할 수 있도록 설계되지 않았고 제대로된 역할도 수행하지 못했다. 1854년 나이팅게일이 간호사 일행과 함께 도착했을 때, 비위생적인 환경과 고통받는 병사들을 보고 경악했다. 나이팅게일의 스쿠타리 병원에서 경험은 병원과 의료 서비스를 개선하여 이와 같은 고통과 비극이 재발하지 않도록 향후 프로젝트의 중요한 동기와 방향이 되었다.\n\n\n스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)\n\n환자의 사망율을 42%에서 2%로 낮추고 집중치료실(ICU)을 설치하여 상태가 중한 환자를 격리하여 집중관리하는 등 근대적인 간호체계를 수립하는 데 기여하였다.\n\n3.1.2 원본 데이터\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원에서 몇년간에 걸쳐 수작업으로 종이에 분석가능한 형태의 자료를 만들어내는 것은 결코 쉬운 작업이 아니다.\n\n\n원본 데이터\n\n\n3.1.3 그래프 진화\n출처: How Florence Nightingale Changed Data Visualization Forever - The celebrated nurse improved public health through her groundbreaking use of graphic storytelling\n복잡한 논거를 제시하는 대신 구체적인 주장에 데이터 시각화와 데이터 스토리텔링(Storytelling)을 통해 청중에 한걸음 더 다가섰다. 나이팅게일의 스토리텔링은 열악한 위생 상태와 과밀로 인해 불필요한 죽음이 얼마나 많이 발생하는지 이해하기 쉬운 비교를 통해 이야기를 구성해서 설득해 나갔다. 예를 들어, 군대 사망률을 민간인 사망률(유사한 환경의 맨체스터)과 비교하는 프레임을 제시하고, 군대 막사에서 생활하는 평시 병사들이 비슷한 연령대 민간인 남성보다 더 높은 비율로 사망하는 것을 제시했다. 이를 통해, 데이터가 보여주는 현실을 부정할 수 없게 만들었고, 군대 행정에 극적인 개혁을 이끌어냈다.\n\n\n\n\n(a) 막대그래프\n\n\n\n(b) 맨체스터 사망\n\n\n\n(c) 빅토리아 여왕 보고(I)\n\n\n\n(d) 빅토리아 여왕 보고(II)\n\n\n\n(e) 빅토리아 여왕 보고(III)\n\n\n그림 3.1: 나이팅게일 그래프 진화과정\n\n\n\n3.1.4 설득\n나이팅게일은 크림 전쟁 중 병원에서의 위생 문제와 관련된 데이터를 수집하고 분석하여 그 결과를 시각화했고, 병원에서의 사망 원인 중 대부분이 감염성 질병으로 인한 것을 발견했다. 이러한 감염성 질병은 부적절한 위생 조건과 밀접한 관련이 있음을 확인했다.\n나이팅게일은 병원의 위생 상태를 개선을 통해 수많은 생명을 구할 수 있다는 사실을 확인했고 연구결과와 권장 사항을 다양한 영국 정부부처에 제출했고, 특히 1858년에 영국의 장관들에게 보고서를 제출했다. 이를 통해서 군 병원의 위생 조건을 개선하는 데 큰 영향을 미쳤다.\n\n\n나이팅게일과 빅토리아 여왕\n\n\n3.1.5 성과와 영향\n나이팅게일 캠페인이 민간 공중보건에 미친 가장 큰 영향은 실현되기까지 오랜 기간에 걸쳐 다각도로 검토되었고, 마침내 1875년 영국 공중보건법(British Public Health Act)에 법제화되었다. 이 법에는 잘 정비된 하수도, 깨끗한 수돗물, 건축법 규제 등의 요건이 담겨있다. 질병에 대한 면역력을 강화하는 백신과 농작물 수확량을 획기적으로 늘리는 인공비료 개발과 함께 이 제도적인 노력으로 평균 수명을 두 배로 늘리는 원동력이 되었다."
  },
  {
    "objectID": "whole_game.html#작업과정",
    "href": "whole_game.html#작업과정",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.2 작업과정",
    "text": "3.2 작업과정\n\n3.2.1 디지털 데이터\nrladies/spain_nightingale GitHub 저장소에서 엑셀 형태로 된 데이터를 가져와서 전처리할 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndeath_raw &lt;- read_excel(\"data/datos_florence.xlsx\", sheet = \"Sheet1\", skip = 1)\n\ndeath_tbl &lt;- death_raw |&gt; \n  janitor::clean_names() |&gt; \n  set_names(c(\"Month\", \"Army\", \"Disease\", \"Wounds\", \"Other\", \"Disease.rate\", \"Wounds.rate\", \"Other.rate\")) |&gt; \n  mutate(Date = lubridate::my(Month)) |&gt; \n  separate(Month, into = c(\"Month\", \"Year\"), sep = \" |_\") |&gt; \n  select(Date, Month, Year, everything()) \n\ndeath_tbl\n\n# A tibble: 24 × 10\n   Date       Month Year   Army Disease Wounds Other Disease.rate Wounds.rate\n   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1854-04-01 Apr   1854   8571       1      0     5          1.4         0  \n 2 1854-05-01 May   1854  23333      12      0     9          6.2         0  \n 3 1854-06-01 Jun   1854  28333      11      0     6          4.7         0  \n 4 1854-07-01 Jul   1854  28722     359      0    23        150           0  \n 5 1854-08-01 Aug   1854  30246     828      1    30        328.          0.4\n 6 1854-09-01 Sep   1854  30290     788     81    70        312.         32.1\n 7 1854-10-01 Oct   1854  30643     503    132   128        197          51.7\n 8 1854-11-01 Nov   1854  29736     844    287   106        341.        116. \n 9 1854-12-01 Dec   1854  32779    1725    114   131        632.         41.7\n10 1855-01-01 Jan   1855  32393    2761     83   324       1023.         30.7\n# ℹ 14 more rows\n# ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\n\nHistDate 패키지에 동일한 데이터셋이 잘 정제되어 있어 이를 바로 활용해도 좋다.\n\nlibrary(HistData)\n\nHistData::Nightingale |&gt; \n  as_tibble()\n\n# A tibble: 24 × 10\n   Date       Month  Year  Army Disease Wounds Other Disease.rate Wounds.rate\n   &lt;date&gt;     &lt;ord&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1854-04-01 Apr    1854  8571       1      0     5          1.4         0  \n 2 1854-05-01 May    1854 23333      12      0     9          6.2         0  \n 3 1854-06-01 Jun    1854 28333      11      0     6          4.7         0  \n 4 1854-07-01 Jul    1854 28722     359      0    23        150           0  \n 5 1854-08-01 Aug    1854 30246     828      1    30        328.          0.4\n 6 1854-09-01 Sep    1854 30290     788     81    70        312.         32.1\n 7 1854-10-01 Oct    1854 30643     503    132   128        197          51.7\n 8 1854-11-01 Nov    1854 29736     844    287   106        341.        116. \n 9 1854-12-01 Dec    1854 32779    1725    114   131        632.         41.7\n10 1855-01-01 Jan    1855 32393    2761     83   324       1023.         30.7\n# ℹ 14 more rows\n# ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\n\n\n3.2.2 데이터와 사투\n앞서 준비한 death_tbl 데이터프레임에서 사망 관련 데이터를 처리하고 시각화하기 위한 전처리를 수행하여 시각화를 위한 준비작업을 수행한다. 먼저 Date, Disease.rate, Wounds.rate, Other.rate 칼럼을 선택하고, pivot_longer 함수를 사용해 시각화에 적합한 데이터로 재구조화한다. str_replace_all 함수를 사용하여 칼럼 이름에서 “.rate”를 제거하고, ifelse 함수를 이용해 날짜를 기준으로 나이팅게일 팀이 준비한 방식을 적용하기 전과후 “이전”과 “이후”로 체제로 구분한다. factor 함수를 사용하여 범주 순서를 정의하고, 마지막으로 month 함수를 이용해 날짜에서 해당 월을 추출하고 death_viz에 저장한다.\n\ndeath_viz &lt;- death_tbl %&gt;% \n  select(Date, Disease.rate, Wounds.rate, Other.rate) %&gt;% \n  pivot_longer(-Date, names_to = \"사망원인\", values_to = \"사망자수\") |&gt; \n  mutate(사망원인 = str_replace_all(사망원인, \"\\\\.rate\", \"\"), \n         체제 = ifelse(Date &lt;= as.Date(\"1855-03-01\"), \"조치이전\", \"조치이후\")) %&gt;% \n  mutate(체제 = factor(체제, levels = c(\"조치이전\", \"조치이후\"))) %&gt;%  \n  mutate(해당월 = month(Date, label = TRUE, abbr = TRUE)) |&gt; \n  mutate(사망원인 = case_when(사망원인 == \"Disease\" ~ \"질병\",\n                              사망원인 == \"Wounds\" ~ \"부상\",\n                              사망원인 == \"Other\" ~ \"기타\")) |&gt; \n  mutate(사망원인 = factor(사망원인, levels = c(\"질병\", \"부상\", \"기타\")))\n\ndeath_viz\n\n# A tibble: 72 × 5\n   Date       사망원인 사망자수 체제     해당월\n   &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;ord&gt; \n 1 1854-04-01 질병          1.4 조치이전 4     \n 2 1854-04-01 부상          0   조치이전 4     \n 3 1854-04-01 기타          7   조치이전 4     \n 4 1854-05-01 질병          6.2 조치이전 5     \n 5 1854-05-01 부상          0   조치이전 5     \n 6 1854-05-01 기타          4.6 조치이전 5     \n 7 1854-06-01 질병          4.7 조치이전 6     \n 8 1854-06-01 부상          0   조치이전 6     \n 9 1854-06-01 기타          2.5 조치이전 6     \n10 1854-07-01 질병        150   조치이전 7     \n# ℹ 62 more rows"
  },
  {
    "objectID": "whole_game.html#section",
    "href": "whole_game.html#section",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.4 ",
    "text": "3.4"
  },
  {
    "objectID": "whole_game.html#기본지식",
    "href": "whole_game.html#기본지식",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.4 기본지식",
    "text": "3.4 기본지식\n\nextrafont::loadfonts()\n\ndeath_new_gg &lt;- death_viz |&gt; \n  ggplot(aes(x = Date, y = 사망자수, color = 사망원인)) +\n    geom_line() +\n    geom_point() +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)          \n\ndeath_new_gg\n\n\n\n\n\n\n\n\ndeath_viz |&gt; \n  ggplot(aes(x = 해당월, y = 사망자수, color = 사망원인, group = 사망원인)) +\n    geom_line() +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    facet_wrap(~체제) +\n    coord_equal(ratio = 1) +    \n    coord_polar()"
  },
  {
    "objectID": "whole_game.html#표-문법",
    "href": "whole_game.html#표-문법",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.4 표 문법",
    "text": "3.4 표 문법\n데이터 문법, 그래프 문법에 이어 최근 “표 문법”이 새롭게 자리를 잡아가고 있다. 표 문법에 맞춰 나이팅게일 크림전쟁 사망자수를 조치 이전과 조치 이후로 나눠 요약하면 확연한 차이를 파악할 수 있다.\ngt와 gtExtras 패키지를 활용하여 death_viz 데이터프레임을 사망 원인별 사망자 수를 “조치 이전”과 “조치 이후”로 구분하여 표를 두개 생성한다. 각 표은 날짜, 질병, 부상, 기타 범주로 사망자 수와 그 합계를 표시하며, 총 사망자수가 250명을 초과하는 행에 대한 강조 색상을 입히고 나서 두 표를 나란히 배치하여 조치 전후 효과를 시각적으로 비교한다.\n\nlibrary(gt)\nlibrary(gtExtras)\n\nbefore_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이전\")\n\nafter_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이후\")\n\nbefore_gt &lt;- before_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n    tab_spanner(label = \"조치 이전\", columns = c(질병, 부상, 기타)) |&gt; \n    data_color(\n      columns = c(질병, 부상, 기타, 합계),\n      rows = 합계 &gt; 250,      \n      method = \"numeric\",\n      palette = \"ggsci::red_material\")\n\nafter_gt &lt;- after_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n  tab_spanner(label = \"조치 이후\", columns = c(질병, 부상, 기타)) |&gt; \n  data_color(\n    columns = c(질병, 부상, 기타, 합계),\n    rows = 합계 &gt; 250,      \n    method = \"numeric\",\n    palette = \"ggsci::red_material\")\n\ngtExtras::gt_two_column_layout(list(before_gt, after_gt))\n\n\n\n\n\n\n\n\n날짜\n      \n        조치 이전\n      \n      합계\n    \n\n질병\n      부상\n      기타\n    \n\n\n\n1854-04-01\n1\n0\n7\n8\n\n\n1854-05-01\n6\n0\n5\n11\n\n\n1854-06-01\n5\n0\n2\n7\n\n\n1854-07-01\n150\n0\n10\n160\n\n\n1854-08-01\n328\n0\n12\n341\n\n\n1854-09-01\n312\n32\n28\n372\n\n\n1854-10-01\n197\n52\n50\n299\n\n\n1854-11-01\n341\n116\n43\n499\n\n\n1854-12-01\n632\n42\n48\n721\n\n\n1855-01-01\n1,023\n31\n120\n1,174\n\n\n1855-02-01\n823\n16\n140\n979\n\n\n1855-03-01\n480\n13\n69\n562\n\n\n\n\n\n\n\n\n\n\n\n날짜\n      \n        조치 이후\n      \n      합계\n    \n\n질병\n      부상\n      기타\n    \n\n\n\n1855-04-01\n178\n18\n21\n217\n\n\n1855-05-01\n172\n17\n12\n201\n\n\n1855-06-01\n248\n64\n10\n322\n\n\n1855-07-01\n108\n38\n9\n154\n\n\n1855-08-01\n130\n44\n7\n181\n\n\n1855-09-01\n48\n69\n5\n122\n\n\n1855-10-01\n33\n14\n5\n51\n\n\n1855-11-01\n56\n10\n10\n77\n\n\n1855-12-01\n25\n5\n8\n38\n\n\n1856-01-01\n11\n0\n13\n25\n\n\n1856-02-01\n7\n0\n5\n12\n\n\n1856-03-01\n4\n0\n9\n13"
  },
  {
    "objectID": "whole_game.html#커뮤니케이션",
    "href": "whole_game.html#커뮤니케이션",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.5 커뮤니케이션",
    "text": "3.5 커뮤니케이션\n데이터를 기반으로 뭔가 유용한 것을 창출한 후에 이를 알리기 위해 커뮤니케이션 단계를 거치게 된다. 가장 흔히 사용하는 방식은 엑셀, 워드, 파워포인트와 같은 MS 오피스 제품을 활용하는 방식이다. 과거 SAS, SPSS, 미니탭 등 외산 통계 팩키지로 데이터를 분석하고 유용한 모형 등을 찾아낸 후에 이를 커뮤니케이션하기 위해 MS 오피스 제품을 통해 커뮤니케이션을 하기도 했다. 하지만, 각각은 별개의 시스템으로 분리되어 있어 일일이 사람손이 가는 번거러움이 많았다. 이를 해결하기 하는 방법은 하나의 도구 혹은 언어로 모든 작업을 처리하는 것이다. [^meghan]\n[^meghan] : Meghan Hall (June 15, 2021), “Extending R Markdown”, RStudio: R in Sports Analytics,\n우선 엑셀은 tidyverse 로 대체가 되고, 워드는 R 마크다운을 거쳐 쿼토(Quarto), 파워포인트도 R 마크다운(xaringan 등)에서 진화한 reveal.js 기반 쿼토 슬라이드가 빠르게 자리를 잡아가고 있다.\n\n\n오피스 기반 커뮤니케이션 현재 상태점검\n\n데이터 과학을 커뮤니케이션하는 방식은 다양한 방식이 존재하지만 직장상사 뿐만 아니라 집단지성을 넘어 AI를 적극 도입하여 데이터 분석 역량을 고도화하는데 동료 개발자 및 협업하시는 분들과 커뮤니케이션 뿐만 아니라 불특정 다수를 대상으로 한 인터넷에 공개와 공유를 통해 새로운 관계를 맺어가는 것도 그 중요성을 더해가고 있다.\n\n동료 개발자나 협업하시는 분: .qmd 파일\n직장상사\n\nPDF 파일: quarto, pandoc\n\n파워포인트 슬라이스덱: reveal.js 기반 quarto\n\n대쉬보드: flexdashboard\n\n\n\n일반 공개\n\n웹사이트: distill을 지나 quarto\n\n블로그: blogdown을 지나 quarto\n\n책: bookdown을 지나 quarto"
  },
  {
    "objectID": "cs_models.html#r-population-by-sigun",
    "href": "cs_models.html#r-population-by-sigun",
    "title": "15  사례: 시군 인구증가",
    "section": "\n15.1 인구변동이 많은 시군",
    "text": "15.1 인구변동이 많은 시군\n\n15.1.1 데이터 가져오기 및 데이터 전처리\n인구변동이 많은 시군 통계 분석을 위해 필요한 팩키지를 불러 읽어온다. 통계청 KOSIS에서 다운로드 받은 파일을 data 폴더 아래 저장하고 나서, 전처리 작업을 수행한다. 서울특별시를 포함한 광역시에 포함된 군은 데이터 분석에 제외할 것이기 때문에 stringr 정규표현식 기능을 활용하여 깔끔하게 향후 데이터분석을 위한 데이터프레임으로 정리한다.\n\n# 0. 환경설정 -----------\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggpubr)\nextrafont::loadfonts()\n\n# 1. 데이터 가져오기 ---------\n\nkor_dat &lt;- read_excel(\"data/korea-pop-zipf-law.xlsx\", sheet=\"데이터\", skip=1) \n\nkor_dat &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` !=\"전국\") %&gt;% \n  filter(`인구현황별` ==\"전체인구(A)\")\n\n# 2. 데이터 전처리 ---------\n## 2.1. 시군 뽑아내기 -------\n\nsigungu_v &lt;- kor_dat %&gt;% count(`소재지(시군구)별`) %&gt;% \n  pull(`소재지(시군구)별`)\n\nsigungu_v &lt;- sigungu_v[str_detect(sigungu_v, \"시$|군$\")]\n\n## 2.2. 시군 데이터 전처리 -------\n\nkor_df &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` %in% sigungu_v) %&gt;% \n  filter(!is.na(`2016 년`)) %&gt;% group_by(`소재지(시군구)별`) %&gt;% \n  summarise_if(is.numeric, mean) %&gt;% \n  rename(시군 = `소재지(시군구)별`)\n\n## 2.3. 시각화 데이터 변환  -------\nkor_lng_df &lt;- kor_df %&gt;% \n  gather(연도, 인구수, -시군) %&gt;% \n  mutate(연도 = as.numeric(str_extract(연도, \"[0-9]+\")))"
  },
  {
    "objectID": "cs_models.html#r-population-by-sigun-problem",
    "href": "cs_models.html#r-population-by-sigun-problem",
    "title": "15  사례: 시군 인구증가",
    "section": "\n15.2 문제점",
    "text": "15.2 문제점\n대한민국 시군이 166 이기 때문에 인구변동이 많은 시군을 추출하기 위해서 서울시는 천만명 근처이고 가장 작은 울릉군은 2016년 기준 10,001 명이라 편차가 매우 크다. 따라서, 이를 시각화를 하게 되면 문제점이 한눈에 파악된다.\n\n# 3. 시각화 ----------------\n## 3.1. 시군별 연도별 인구수 변화\nkor_lng_df %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=시군, group=시군)) +\n    geom_point() +\n    geom_line() +\n    theme_pubclean(base_family=\"NanumGothic\") +\n    scale_y_log10(labels=scales::comma) +\n    scale_x_date(date_labels = \"%Y\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\")"
  },
  {
    "objectID": "cs_models.html#r-population-by-sigun-solution",
    "href": "cs_models.html#r-population-by-sigun-solution",
    "title": "15  사례: 시군 인구증가",
    "section": "\n15.3 FP 통한 문제해결",
    "text": "15.3 FP 통한 문제해결\n이러한 문제점에 대해 가장 많이 활용되는 기법이 자료구조로 티블(tibble)을 도입하고, 데이터 분석을 위한 방법으로 함수형 프로그래밍을 조합하는 것이다.\n\n15.3.1 티블 자료구조\n데이터프레임을 기존 폭넓은(wide) 형태를 긴(long) 형태로 변환하고 이를 nest()를 적용시키면 함수형 프로그램을 적용시킬 수 있는 자료구조가 된다. 더불어, 선형회귀모형을 각 시군별로 적용시킬 예정이라 회귀모형 함수도 생성시켜 둔다.\n그리고 나서 전체 시군별로 연도별 인구변화를 회귀분석으로 수행하여 수행결과를 broom 팩키지의 tidy, glance, augment 함수를 활용하여 데이터와 모형분석결과를 결합시킨다.\n\n# 4. 인구변화 심한 시군 추출 ----------------\n## 4.1. 데이터 \nkor_sigun_tb &lt;- kor_lng_df %&gt;% group_by(시군) %&gt;% \n  nest()\n\n## 4.2. 모형 - 선형회귀모형\nsigun_model &lt;- function(df) {\n  lm(인구수 ~ 연도, data=df)\n}\n\n## 4.3. 데이터 + 모형 결합\nkor_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  mutate(model = map(data, sigun_model)) %&gt;% \n  mutate(\n    tidy    = map(model, broom::tidy),\n    glance  = map(model, broom::glance),\n    결정계수 = glance %&gt;% map_dbl(\"r.squared\"),\n    augment = map(model, broom::augment)\n  )\n\nDT::datatable(kor_sigun_tb)\n\n\n\n\n\n\n15.3.2 급성장 시군 - 결정계수(\\(R^2\\))\n회귀분석 결정계수(\\(R^2\\))기준 인구가 연도별로 높은 상관관계를 갖는 시군과 그렇지 않는 상위 하위 5개 시군을 뽑아 시각화 해보자.\n\n## 4.4. 고성장 및 정체 시군 추출\n\nkor_high_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"고성장\")\n\nkor_low_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(-5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"저성장\")\n\nkor_growth_sigun_tb &lt;- bind_rows(kor_high_growth_sigun_tb, kor_low_growth_sigun_tb)\n\n## 4.5. 고성장 및 정체 시군 시각화\n\nkor_growth_sigun_tb %&gt;% \n  mutate(구분 = factor(구분),\n         시군 = fct_reorder(시군, 결정계수)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=시군)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_log10(labels=scales::comma) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~시군, nrow=2, scale=\"free\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n15.3.3 급성장 시군 - 회귀계수\n회귀분석 결정계수(\\(R^2\\))기준이 아닌 회귀계수(\\(\\beta_1\\))를 기준으로 상위 5개, 하위 5개 시군을 뽑아 연도별 인구변화를 시각화해보자.\n\n# 5. 인구 증가 및 인구감소 ----------------\nkor_sigun_reg_df &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0)\n\ntop10_plus_sigun &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(5, 증감) %&gt;% \n  pull(시군)\n\ntop10_minus_sigun &lt;-kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(-5, 증감) %&gt;% \n  pull(시군)\n\nkor_lng_df %&gt;% \n  left_join(kor_sigun_reg_df, by=\"시군\") %&gt;% \n  filter(시군 %in% c(top10_plus_sigun, top10_minus_sigun)) %&gt;%\n  mutate(구분 = ifelse(시군 %in% top10_plus_sigun, \"성장\", \"역성장\"),\n         시군 = fct_reorder(시군, -증감)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=구분)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_continuous(labels=scales::comma) +\n  scale_x_date(date_labels = \"%Y\") +\n  theme(legend.position = \"none\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  facet_wrap(~시군, scale=\"free\", nrow=2)"
  },
  {
    "objectID": "models.html#모형-개발과정",
    "href": "models.html#모형-개발과정",
    "title": "\n14  모형\n",
    "section": "\n14.3 모형 개발과정",
    "text": "14.3 모형 개발과정\n통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다. 다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고, 지난한 과정이 될 수도 있다.\n\n데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 팩키지와 소통이 될 수 있는 데이터형태)가 되도록 준비한다.\n변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도 적용한다.\n변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를 계산한다. 특히 변수간 상관관계가 \\(r &gt; 0.9\\) 혹은 근처인 경우 변수를 빼거나 다른 방법을 강구한다.\n동일한 척도로 회귀계수를 추정하고 평가하려는 경우, scale() 함수로 척도로 표준화한다.\n모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다. 만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로 개발한다.\n\n\nplot() 함수를 사용해서 이상점이 있는지, 비선형관계를 잘 잡아냈는지 시각적으로 확인한다.\n다양한 모형을 적합시키고 R^2 와 RMSE, 정확도 등 모형평가 결과가 가장 좋은 것을 선정한다.\n절약성의 원리(principle of parsimony)를 필히 준수하여 가장 간결한 모형이 되도록 노력한다.1\n\n\n\n최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를 넣어 마무리한다.\n\n\n\n\n\n\n\n키보드로 통계모형 표현법\n\n\n\n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n\n주효과에 대해 변수를 입력으로 넣을 +를 사용한다.\n교호작용을 변수간에 표현할 때 :을 사용한다. 예를 들어 x*y는 x+y+x:z와 같다.\n모든 변수를 표기할 때 .을 사용한다.\n종속변수와 예측변수를 구분할 때 ~을 사용한다. y ~ .은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n특정변수를 제거할 때는 -를 사용한다. y ~ . -x는 모든 예측변수를 사용하고, 특정 변수 x를 제거한다는 의미다.\n상수항을 제거할 때는 -1을 사용한다.\n\n\n\n\n\n\n\n\nR 공식구문\n수학적 표현\n설명\n\n\n\ny~x\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시키는 1차 선형회귀식\n\n\ny~x -1\n\\(y_i = \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시 절편 없는 1차 선형회귀식\n\n\ny~x+z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i\\)\n\nx와 z를 y에 적합시키는 1차 선형회귀식\n\n\ny~x:z\n\\(y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z 교호작용 항을 y에 적합시키는 1차 선형회귀식\n\n\ny~x*z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z, 교호작용항을 y에 적합시키는 1차 선형회귀식\n\n\n\n\n\n\n14.3.1 과대적합 사례\n\\(y=x^2 + \\epsilon\\) 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n\ntidyr, modelr, ggplot2 팩키지를 불러와서 환경을 설정한다.\n\n\\(y=x^2 + \\epsilon\\), 오차는 \\(N(0, 0.25)\\)을 따르는 모형을 생성하고, df 데이터프레임에 결과를 저장한다.\n\npoly_fit_model 함수를 통해 7차 다항식으로 적합시킨다.\n적합결과를 ggplot을 통해 시각화한다.\n\n1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수 있다.\n\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(modelr)\n\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model &lt;- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx &lt;- seq(-1, 1, length=20)\ny &lt;- true_model(x)\ndf &lt;- tibble(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model &lt;- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\n# fitted_mod &lt;- poly_fit_model(df, 10)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\nfitted_tbl &lt;- tibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(x = list(seq(-1, 1, length=20))) |&gt;\n  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, \n                                         prediction = .y, \n                                         .name_repair = \"unique\"))) |&gt; \n  select(차수, fitted_df) |&gt; \n  mutate(차수 = as.factor(차수)) |&gt;   \n  unnest(fitted_df) \n\n\ndf %&gt;% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))\n\n\n\n\n\n\n\n다항식 차수를 달리하여 데이터에 1~10차 다항식을 적합시킨 후에 오차를 계산했다. 1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그 이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록 낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가 필요한지는 의문이다.\n\ntibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(true_y = map(데이터, 2)) |&gt;\n  # 참값과 모형예측값\n  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |&gt; \n  # 오차\n  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |&gt; \n  mutate(차수 = factor(차수)) |&gt; \n  mutate(color = c(\"gray30\", \"blue\", rep(\"gray30\", 8))) |&gt; \n  # 시각화\n  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs( x = \"차수\",\n          y = \"RMSE 오차\",\n          title = \"다항식 차수를 달리하여 계산한 RMSE 오차\") +\n    scale_fill_manual(values = c(\"blue\", \"gray30\", rep(\"blue\", 8))) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science. Leanpub.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of Statistics and Data Visualization."
  },
  {
    "objectID": "models.html#자판으로-통계모형-표현법",
    "href": "models.html#자판으로-통계모형-표현법",
    "title": "\n14  모형\n",
    "section": "\n14.3 자판으로 통계모형 표현법",
    "text": "14.3 자판으로 통계모형 표현법\n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n\n주효과에 대해 변수를 입력으로 넣을 +를 사용한다.\n교호작용을 변수간에 표현할 때 :을 사용한다. 예를 들어 x*y는 x+y+x:z와 같다.\n모든 변수를 표기할 때 .을 사용한다.\n종속변수와 예측변수를 구분할 때 ~을 사용한다. y ~ .은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n특정변수를 제거할 때는 -를 사용한다. y ~ . -x는 모든 예측변수를 사용하고, 특정 변수 x를 제거한다는 의미다.\n상수항을 제거할 때는 -1을 사용한다.\n\n\n\n\n\n\n\n\nR 공식구문\n수학 모형\n설명\n\n\n\ny~x\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시키는 1차 선형회귀식\n\n\ny~x -1\n\\(y_i = \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시 절편 없는 1차 선형회귀식\n\n\ny~x+z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i\\)\n\nx와 z를 y에 적합시키는 1차 선형회귀식\n\n\ny~x:z\n\\(y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z 교호작용 항을 y에 적합시키는 1차 선형회귀식\n\n\ny~x*z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z, 교호작용항을 y에 적합시키는 1차 선형회귀식\n\n\n\n\n14.3.1 2. 과대적합 사례\n\\(y=x^2 + \\epsilon\\) 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n\ntidyr, modelr, ggplot2 팩키지를 불러와서 환경을 설정한다.\n\n\\(y=x^2 + \\epsilon\\), 오차는 \\(N(0, 0.25)\\)을 따르는 모형을 생성하고, df 데이터프레임에 결과를 저장한다.\n\npoly_fit_model 함수를 통해 7차 다항식으로 적합시킨다.\n적합결과를 ggplot을 통해 시각화한다.\n\n\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyr)\nlibrary(modelr)\nlibrary(ggplot2)\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model &lt;- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx = seq(-1,1, length=20)\ny = true_model(x)\ndf &lt;- data.frame(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model &lt;- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\nfitted_mod &lt;- poly_fit_model(df, 7)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\ngrid &lt;- df %&gt;% tidyr::expand(x = seq_range(x, 50))\npreds &lt;- grid %&gt;% modelr::add_predictions(fitted_mod, var = \"y\")\n\ndf %&gt;% \n  ggplot(aes(x, y)) +\n  geom_line(data=preds) +\n  geom_point()\n\n\n\n\n\n\nmodelr::rmse(fitted_mod, df)\n\n[1] 0.1795755\n\n\n\n14.3.2 3. 전통적인 가내수공업 방식 모형개발 사례\n데이터과학 제품을 만드는 방식은 여러가지 방식이 존재한다. 공학적인 방식으로 보면 장인이 제자에게 비법을 가미해서 전통적으로 내려오던 가내수공업 방식부터 컨베이어 벨트를 타고 포드생산방식을 거쳐 Mass Customization을 지나 기계학습과 딥러닝이 결합된 모형개발 방식까지 정말 다양한 방식이 혼재되어 있다.\n전통적인 가내수공업 방식은 인간이 가장 많은 것을 이해하고 주문형 모형을 만들어내는 가장 최적의 방식이다. 이를 간략하게 살펴본다.\n\n##========================================================\n## 01. 데이터 준비\n##========================================================\n## 모의시험 데이터 생성\n\nx &lt;- seq(1, 100,1)\ny &lt;- x**2 + jitter(x, 1000)\n\ndf &lt;- data.frame(x,y)\nhead(df)\n\n  x          y\n1 1  155.21177\n2 2 -150.08778\n3 3 -151.04044\n4 4  218.51674\n5 5  -50.81708\n6 6   36.15769\n\n##========================================================\n## 02. 탐색적 데이터 분석\n##========================================================\n# 통계량\npsych::describe(df)\n\n  vars   n    mean      sd  median trimmed     mad     min      max    range\nx    1 100   50.50   29.01   50.50   50.50   37.06    1.00   100.00    99.00\ny    2 100 3420.75 3057.34 2659.51 3128.18 3284.37 -151.04 10292.84 10443.88\n  skew kurtosis     se\nx 0.00    -1.24   2.90\ny 0.61    -0.91 305.73\n\n# 산점도\nplot(x, y)\n\n\n\n\n\n\n##========================================================\n## 03. 모형 적합\n##========================================================\n\n#---------------------------------------------------------\n# 3.1. 선형회귀 적합\nlm.m &lt;- lm(y ~ x, data=df)\nsummary(lm.m)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-982.5 -645.1 -231.9  574.3 1815.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1737.860    152.201  -11.42   &lt;2e-16 ***\nx             102.151      2.617   39.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755.3 on 98 degrees of freedom\nMultiple R-squared:  0.9396,    Adjusted R-squared:  0.939 \nF-statistic:  1524 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x,y, data=df, cex=0.7)\nabline(lm.m, col='blue')\n\n# 잔차 \nplot(resid(lm.m))\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n#---------------------------------------------------------\n# 3.2. 비선형회귀 적합\n# 비선형회귀적합\ndf$x2 &lt;- df$x**2\n\nnlm.m &lt;- lm(y ~ x + x2, data=df)\nsummary(nlm.m)\n\n\nCall:\nlm(formula = y ~ x + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-193.35 -101.39  -14.28   95.52  230.21 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -35.63765   35.85917  -0.994    0.323    \nx             2.01999    1.63886   1.233    0.221    \nx2            0.99139    0.01572  63.062   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 117.1 on 97 degrees of freedom\nMultiple R-squared:  0.9986,    Adjusted R-squared:  0.9985 \nF-statistic: 3.367e+04 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x, y, data=df, cex=0.7)\nlines(x, fitted(nlm.m), col='blue')\n# 잔차 \nplot(resid(nlm.m), cex=0.7)\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n\n데이터를 준비하고 \\(y = \\beta_0 + \\beta_1 x + \\beta_1 x^2\\) 수식으로 돌아가는 시스템에서 데이터를 추출하고 이를 먼저 선형 모형으로 적합시키고 나서, 오차 및 모형 분석을 통한 후에 최종적으로 2차 모형을 적합시켜 잔차 및 모형 결과를 최종적으로 검증하는 것을 시연했다.\n\n\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science. Leanpub.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of Statistics and Data Visualization."
  },
  {
    "objectID": "models.html#가내수공업-모형개발",
    "href": "models.html#가내수공업-모형개발",
    "title": "\n14  모형\n",
    "section": "\n14.2 가내수공업 모형개발",
    "text": "14.2 가내수공업 모형개발\n데이터 과학 분야의 제품 개발 방식은 다양하다. 엔지니어링의 관점에서 볼 때, 전통적인 장인의 기술이 제자에게 계승되는 방식에서 시작하여, 포드의 대량 생산 방식을 거치고, 대량 맞춤생산(Mass Customization) 방식으로 발전하여, 현재에는 기계 학습과 딥러닝이 통합된 혁신적인 개발 방식까지 다양한 방법론이 혼재되어 있다.\n전통적인 가내수공업 방식은 개별 주문에 따라 제품을 최적화하여 만드는 방식이다. 이 방식에서는 인간의 경험과 지식이 중요한 역할을 하며, 고객의 특별한 요구사항을 충족하기 위해 맞춤형 모델을 개발한다. 이러한 방식을 간략히 살펴보면, 각 고객의 고유한 필요에 따라 제품이나 서비스를 특별히 설계하고 생산하는 것이 핵심이다.\n\n##========================================================\n## 01. 데이터 준비\n##========================================================\n## 모의시험 데이터 생성\n\nx &lt;- seq(1, 100,1)\ny &lt;- x**2 + jitter(x, 1000)\n\ndf &lt;- tibble(x,y)\nhead(df)\n\n# A tibble: 6 × 2\n      x      y\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 148.  \n2     2 -38.8 \n3     3 202.  \n4     4 -39.9 \n5     5 223.  \n6     6  -5.23\n\n##========================================================\n## 02. 탐색적 데이터 분석\n##========================================================\n# 통계량\npsych::describe(df)\n\n  vars   n    mean      sd median trimmed     mad    min     max   range skew\nx    1 100   50.50   29.01   50.5   50.50   37.06   1.00  100.00   99.00 0.00\ny    2 100 3433.85 3039.65 2627.9 3131.94 3359.18 -39.85 9917.51 9957.37 0.62\n  kurtosis     se\nx    -1.24   2.90\ny    -0.90 303.96\n\n# 산점도\nplot(x, y)\n\n\n\n\n\n\n##========================================================\n## 03. 모형 적합\n##========================================================\n\n#---------------------------------------------------------\n# 3.1. 선형회귀 적합\nlm.m &lt;- lm(y ~ x, data=df)\nsummary(lm.m)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-987.2 -670.5 -220.2  529.7 1734.2 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1687.581    154.645  -10.91   &lt;2e-16 ***\nx             101.414      2.659   38.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 767.4 on 98 degrees of freedom\nMultiple R-squared:  0.9369,    Adjusted R-squared:  0.9363 \nF-statistic:  1455 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x,y, data=df, cex=0.7)\nabline(lm.m, col='blue')\n\n# 잔차 \nplot(resid(lm.m))\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n#---------------------------------------------------------\n# 3.2. 비선형회귀 적합\n# 비선형회귀적합\ndf$x2 &lt;- df$x**2\n\nnlm.m &lt;- lm(y ~ x + x2, data=df)\nsummary(nlm.m)\n\n\nCall:\nlm(formula = y ~ x + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-224.64  -96.05  -35.53  103.94  206.67 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.78554   36.59748   1.142    0.256    \nx           -0.31297    1.67260  -0.187    0.852    \nx2           1.00720    0.01604  62.775   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 119.6 on 97 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9985 \nF-statistic: 3.195e+04 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x, y, data=df, cex=0.7)\nlines(x, fitted(nlm.m), col='blue')\n# 잔차 \nplot(resid(nlm.m), cex=0.7)\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n\n데이터 준비단계에서 모의시험 데이터를 생성한다. 독립변수 \\(x\\)는 1부터 100까지의 연속적인 수열이며, 종속변수 \\(y\\)는 \\(x^2\\)에 노이즈를 추가한 값으로 정의한다. 생성데이터는 df라는 데이터프레임에 저장하며, 데이터 처음 몇 행을 출력하여 확인한다.\n탐색적 데이터 분석 (EDA) 단계에서 데이터의 기술통계량을 psych 패키지 describe 함수를 활용하여 출력하고, 데이터 분포와 관계를 확인하기 위해 \\(x\\)와 \\(y\\)의 산점도로 시각화한다.\n모형 적합 단계에선 먼저, 선형회귀 모델을 사용하여 \\(y\\)를 \\(x\\)로 예측한다. 적합된 선형 모형 결과를 출력하고, 적합된 모델과 잔차를 시각적으로 확인한다. \\(y = \\beta_0 + \\beta_1 x + \\beta_1 x^2\\) 형태를 가진 2차 회귀 모형을 적합시킨다. \\(x^2\\) 항을 데이터프레임에 추가해서 2차 회귀 모델을 적합한 후, 결과를 출력하고, 적합된 모형과 잔차를 시각적으로 확인한다.\n\n\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science. Leanpub.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of Statistics and Data Visualization."
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun",
    "href": "models_case_study.html#r-population-by-sigun",
    "title": "17  사례: 시군 인구증가",
    "section": "\n17.1 인구변동이 많은 시군",
    "text": "17.1 인구변동이 많은 시군\n\n17.1.1 데이터 가져오기 및 데이터 전처리\n인구변동이 많은 시군 통계 분석을 위해 필요한 팩키지를 불러 읽어온다. 통계청 KOSIS에서 다운로드 받은 파일을 data 폴더 아래 저장하고 나서, 전처리 작업을 수행한다. 서울특별시를 포함한 광역시에 포함된 군은 데이터 분석에 제외할 것이기 때문에 stringr 정규표현식 기능을 활용하여 깔끔하게 향후 데이터분석을 위한 데이터프레임으로 정리한다.\n\n# 0. 환경설정 -----------\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggpubr)\nextrafont::loadfonts()\n\n# 1. 데이터 가져오기 ---------\n\nkor_dat &lt;- read_excel(\"data/korea-pop-zipf-law.xlsx\", sheet=\"데이터\", skip=1) \n\nkor_dat &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` !=\"전국\") %&gt;% \n  filter(`인구현황별` ==\"전체인구(A)\")\n\n# 2. 데이터 전처리 ---------\n## 2.1. 시군 뽑아내기 -------\n\nsigungu_v &lt;- kor_dat %&gt;% count(`소재지(시군구)별`) %&gt;% \n  pull(`소재지(시군구)별`)\n\nsigungu_v &lt;- sigungu_v[str_detect(sigungu_v, \"시$|군$\")]\n\n## 2.2. 시군 데이터 전처리 -------\n\nkor_df &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` %in% sigungu_v) %&gt;% \n  filter(!is.na(`2016 년`)) %&gt;% group_by(`소재지(시군구)별`) %&gt;% \n  summarise_if(is.numeric, mean) %&gt;% \n  rename(시군 = `소재지(시군구)별`)\n\n## 2.3. 시각화 데이터 변환  -------\nkor_lng_df &lt;- kor_df %&gt;% \n  gather(연도, 인구수, -시군) %&gt;% \n  mutate(연도 = as.numeric(str_extract(연도, \"[0-9]+\")))"
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-problem",
    "href": "models_case_study.html#r-population-by-sigun-problem",
    "title": "17  사례: 시군 인구증가",
    "section": "\n17.2 문제점",
    "text": "17.2 문제점\n대한민국 시군이 166 이기 때문에 인구변동이 많은 시군을 추출하기 위해서 서울시는 천만명 근처이고 가장 작은 울릉군은 2016년 기준 10,001 명이라 편차가 매우 크다. 따라서, 이를 시각화를 하게 되면 문제점이 한눈에 파악된다.\n\n# 3. 시각화 ----------------\n## 3.1. 시군별 연도별 인구수 변화\nkor_lng_df %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=시군, group=시군)) +\n    geom_point() +\n    geom_line() +\n    theme_pubclean(base_family=\"NanumGothic\") +\n    scale_y_log10(labels=scales::comma) +\n    scale_x_date(date_labels = \"%Y\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\")"
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-solution",
    "href": "models_case_study.html#r-population-by-sigun-solution",
    "title": "17  사례: 시군 인구증가",
    "section": "\n17.3 FP 통한 문제해결",
    "text": "17.3 FP 통한 문제해결\n이러한 문제점에 대해 가장 많이 활용되는 기법이 자료구조로 티블(tibble)을 도입하고, 데이터 분석을 위한 방법으로 함수형 프로그래밍을 조합하는 것이다.\n\n17.3.1 티블 자료구조\n데이터프레임을 기존 폭넓은(wide) 형태를 긴(long) 형태로 변환하고 이를 nest()를 적용시키면 함수형 프로그램을 적용시킬 수 있는 자료구조가 된다. 더불어, 선형회귀모형을 각 시군별로 적용시킬 예정이라 회귀모형 함수도 생성시켜 둔다.\n그리고 나서 전체 시군별로 연도별 인구변화를 회귀분석으로 수행하여 수행결과를 broom 팩키지의 tidy, glance, augment 함수를 활용하여 데이터와 모형분석결과를 결합시킨다.\n\n# 4. 인구변화 심한 시군 추출 ----------------\n## 4.1. 데이터 \nkor_sigun_tb &lt;- kor_lng_df %&gt;% group_by(시군) %&gt;% \n  nest()\n\n## 4.2. 모형 - 선형회귀모형\nsigun_model &lt;- function(df) {\n  lm(인구수 ~ 연도, data=df)\n}\n\n## 4.3. 데이터 + 모형 결합\nkor_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  mutate(model = map(data, sigun_model)) %&gt;% \n  mutate(\n    tidy    = map(model, broom::tidy),\n    glance  = map(model, broom::glance),\n    결정계수 = glance %&gt;% map_dbl(\"r.squared\"),\n    augment = map(model, broom::augment)\n  )\n\nDT::datatable(kor_sigun_tb)\n\n\n\n\n\n\n17.3.2 급성장 시군 - 결정계수(\\(R^2\\))\n회귀분석 결정계수(\\(R^2\\))기준 인구가 연도별로 높은 상관관계를 갖는 시군과 그렇지 않는 상위 하위 5개 시군을 뽑아 시각화 해보자.\n\n## 4.4. 고성장 및 정체 시군 추출\n\nkor_high_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"고성장\")\n\nkor_low_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(-5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"저성장\")\n\nkor_growth_sigun_tb &lt;- bind_rows(kor_high_growth_sigun_tb, kor_low_growth_sigun_tb)\n\n## 4.5. 고성장 및 정체 시군 시각화\n\nkor_growth_sigun_tb %&gt;% \n  mutate(구분 = factor(구분),\n         시군 = fct_reorder(시군, 결정계수)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=시군)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_log10(labels=scales::comma) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~시군, nrow=2, scale=\"free\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n17.3.3 급성장 시군 - 회귀계수\n회귀분석 결정계수(\\(R^2\\))기준이 아닌 회귀계수(\\(\\beta_1\\))를 기준으로 상위 5개, 하위 5개 시군을 뽑아 연도별 인구변화를 시각화해보자.\n\n# 5. 인구 증가 및 인구감소 ----------------\nkor_sigun_reg_df &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0)\n\ntop10_plus_sigun &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(5, 증감) %&gt;% \n  pull(시군)\n\ntop10_minus_sigun &lt;-kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(-5, 증감) %&gt;% \n  pull(시군)\n\nkor_lng_df %&gt;% \n  left_join(kor_sigun_reg_df, by=\"시군\") %&gt;% \n  filter(시군 %in% c(top10_plus_sigun, top10_minus_sigun)) %&gt;%\n  mutate(구분 = ifelse(시군 %in% top10_plus_sigun, \"성장\", \"역성장\"),\n         시군 = fct_reorder(시군, -증감)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=구분)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_continuous(labels=scales::comma) +\n  scale_x_date(date_labels = \"%Y\") +\n  theme(legend.position = \"none\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  facet_wrap(~시군, scale=\"free\", nrow=2)"
  },
  {
    "objectID": "models_building.html#모형-개발과정",
    "href": "models_building.html#모형-개발과정",
    "title": "\n15  모형 개발\n",
    "section": "\n15.3 모형 개발과정",
    "text": "15.3 모형 개발과정\n통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다. 다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고, 지난한 과정이 될 수도 있다.\n\n데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 팩키지와 소통이 될 수 있는 데이터형태)가 되도록 준비한다.\n변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도 적용한다.\n변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를 계산한다. 특히 변수간 상관관계가 \\(r &gt; 0.9\\) 혹은 근처인 경우 변수를 빼거나 다른 방법을 강구한다.\n동일한 척도로 회귀계수를 추정하고 평가하려는 경우, scale() 함수로 척도로 표준화한다.\n모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다. 만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로 개발한다.\n\n\nplot() 함수를 사용해서 이상점이 있는지, 비선형관계를 잘 잡아냈는지 시각적으로 확인한다.\n다양한 모형을 적합시키고 R^2 와 RMSE, 정확도 등 모형평가 결과가 가장 좋은 것을 선정한다.\n절약성의 원리(principle of parsimony)를 필히 준수하여 가장 간결한 모형이 되도록 노력한다.2\n\n\n\n최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를 넣어 마무리한다.\n\n\n\n\n\n\n\n키보드로 통계모형 표현법\n\n\n\n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n\n주효과에 대해 변수를 입력으로 넣을 +를 사용한다.\n교호작용을 변수간에 표현할 때 :을 사용한다. 예를 들어 x*y는 x+y+x:z와 같다.\n모든 변수를 표기할 때 .을 사용한다.\n종속변수와 예측변수를 구분할 때 ~을 사용한다. y ~ .은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n특정변수를 제거할 때는 -를 사용한다. y ~ . -x는 모든 예측변수를 사용하고, 특정 변수 x를 제거한다는 의미다.\n상수항을 제거할 때는 -1을 사용한다.\n\n\n\n\n\n\n\n\nR 공식구문\n수학적 표현\n설명\n\n\n\ny~x\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시키는 1차 선형회귀식\n\n\ny~x -1\n\\(y_i = \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시 절편 없는 1차 선형회귀식\n\n\ny~x+z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i\\)\n\nx와 z를 y에 적합시키는 1차 선형회귀식\n\n\ny~x:z\n\\(y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z 교호작용 항을 y에 적합시키는 1차 선형회귀식\n\n\ny~x*z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z, 교호작용항을 y에 적합시키는 1차 선형회귀식"
  },
  {
    "objectID": "models_building.html#과대적합-사례",
    "href": "models_building.html#과대적합-사례",
    "title": "\n15  모형 개발\n",
    "section": "\n15.4 과대적합 사례",
    "text": "15.4 과대적합 사례\n\\(y=x^2 + \\epsilon\\) 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n\ntidyr, modelr, ggplot2 팩키지를 불러와서 환경을 설정한다.\n\n\\(y=x^2 + \\epsilon\\), 오차는 \\(N(0, 0.25)\\)을 따르는 모형을 생성하고, df 데이터프레임에 결과를 저장한다.\n\npoly_fit_model 함수를 통해 7차 다항식으로 적합시킨다.\n적합결과를 ggplot을 통해 시각화한다.\n\n1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수 있다.\n\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(modelr)\n\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model &lt;- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx &lt;- seq(-1, 1, length=20)\ny &lt;- true_model(x)\ndf &lt;- tibble(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model &lt;- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\n# fitted_mod &lt;- poly_fit_model(df, 10)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\nfitted_tbl &lt;- tibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(x = list(seq(-1, 1, length=20))) |&gt;\n  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, \n                                         prediction = .y, \n                                         .name_repair = \"unique\"))) |&gt; \n  select(차수, fitted_df) |&gt; \n  mutate(차수 = as.factor(차수)) |&gt;   \n  unnest(fitted_df) \n\n\ndf %&gt;% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))\n\n\n\n\n\n\n\n다항식 차수를 달리하여 데이터에 1~10차 다항식을 적합시킨 후에 오차를 계산했다. 1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그 이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록 낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가 필요한지는 의문이다.\n\ntibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(true_y = map(데이터, 2)) |&gt;\n  # 참값과 모형예측값\n  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |&gt; \n  # 오차\n  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |&gt; \n  mutate(차수 = factor(차수)) |&gt; \n  mutate(color = c(\"gray30\", \"blue\", rep(\"gray30\", 8))) |&gt; \n  # 시각화\n  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs( x = \"차수\",\n          y = \"RMSE 오차\",\n          title = \"다항식 차수를 달리하여 계산한 RMSE 오차\") +\n    scale_fill_manual(values = c(\"blue\", \"gray30\", rep(\"blue\", 8))) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nAbu-Mostafa, Yaser S, Malik Magdon-Ismail, 와/과 Hsuan-Tien Lin. 2012. Learning from data. Vol 4. AMLBook New York."
  },
  {
    "objectID": "models_building.html#ml-basic-elements",
    "href": "models_building.html#ml-basic-elements",
    "title": "\n15  모형 개발\n",
    "section": "\n15.1 기계학습/회귀모형 구성요소",
    "text": "15.1 기계학습/회귀모형 구성요소\n일반 모형을 “신호 + 잡음(signal + noise)”로 가정하고 다음과 같은 수식으로 표현할 수 있다.\n\\[y = f(x) + \\epsilon\\]\n\n출력 : \\(y\\), 관심갖고 있는 결과변수\n입력 : \\(x\\), 설명/예측 변수\n\n\n\\(y\\)의 변동성을 설명하는 목적의 모형을 구축하는 경우 \\(x\\)는 설명변수\n\n\\(y\\)의 변동성을 예측하는 목적의 모형을 구축하는 경우 \\(x\\)는 예측변수\n\n\n가설: : \\(g: x \\rightarrow y\\), \\(x\\)는 \\(y\\)에 영향을 주는 인과관계가 존재한다.\n목적함수 : \\(f: x \\rightarrow y\\), \\(y\\)와 \\(x\\)를 연관시켜주는 함수\n데이터: \\((x_1 , y_1 ), (x_2 , y_2 ), \\dots, (x_n , y_n )\\)\n\n오차: \\(\\epsilon\\), \\(f: x \\rightarrow y\\)으로 설명되지 않는 부분\n\n결국, 잡음이 낀 데이터에서 잡음을 제거하고 신호만 뽑아내는 것이 회귀모형, 기계학습 모형이라고 볼 수 있다. 회귀모형과 기계학습 모형은 회귀모형이 특정 함수형태를 가정하고 데이터에서 신호와 잡음을 구부하는데 초점이 과거 맞춰졌다면, 기계학습모형은 \\(x\\)는 \\(y\\)의 인과관계를 가정으로 놓고 신호와 잡음을 가장 잘 발라낼 수 있는 함수를 찾아내는데 초점을 두고 있다.\n\n\n기계학습 도해"
  },
  {
    "objectID": "models_building.html#대-기계학습-원리",
    "href": "models_building.html#대-기계학습-원리",
    "title": "\n15  모형 개발\n",
    "section": "\n15.2 3대 기계학습 원리",
    "text": "15.2 3대 기계학습 원리\n기계학습 알고리즘 개발자가 데이터를 학습시켜 기계학습 알고리즘을 뽑아내는 과정에 3대 기계학습 원리가 적용된다. 1\n\n\n오컴의 면도날(Occam’s Razor): 사고 절약의 원리(Principle of Parsimony)라고도 불리며, 같은 현상을 설명하는 두가지 모형이 있다면, 단순한 모형을 선택한다.\n표집 편향(Sampling Bias): 모집단을 대표성의 원리에 따라 표본을 추출하지 못할 때, 기계학습 알고리즘도 편향된 표본을 학습하여 결과를 왜곡시킨다.\n데이터 염탐 편향(Data Snooping Bias): 데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 된다.\n\n\n\n3대 기계학습 원리\n\n\n오컴의 면도날\n\n동일한 조건이면 더 단순한 것을 선택하는 것으로, 가장 큰 이유는 갖고 있는 데이터를 벗어나 새로운 데이터를 갖게 될 경우 학습시킨 기계학습 알고리즘이 더 좋은 성능을 보인다는 것이다. 결국 수많은 가능한 모형중에서 하나를 선택하는 기준이 된다.\n\nAn explanation of the data should be made as simple as possible, but no simpler – Albert Einstein\n\n\n표집 편향\n\n1948년 미국 대통령선거에서 트루먼이 듀이 후보를 물리치고 대통령이 된 것은 알려진 사실이다. 하지만, 대부분의 여론조사에서 듀이의 승리를 예상했지만, 사실은 그 반대로 나타났다. 그 당시 여론조사를 전화기를 사용하였는데, 문제는 전화기가 부유층이 많이 소유하고 있어 미국 대통령선거 모집단을 대표하는 대표성에 문제가 있어 왜곡된 결과가 도출된 것이다.\n상업적으로 개인금융의 신용카드발급, 신용평가에도 동일한 문제가 발생한다. 사실 수익성은 저신용자가 높아 이를 살펴보면, 신용평가에 사용될 데이터는 저신용자는 카드를 발급받을 수 없어 데이터베이스에는 표집편향된 고객정보만 존재하는 것을 어렵지 않게 볼 수 있다.\n\nIf the data is sampled in a biased way, learning will produce a similarly biased outome.\n\n\n데이터 염탐 편향\n\n데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 하지만, 현실적으로 현업에서 작업하는 사람들이 흔히 범하는 실수다. 동일한 데이터에 대해 갖가지 기계학습 알고리즘을 적용해서 가장 좋은 성능이 나오는 알고리즘을 선정한다. 문제는 데이터가 바뀌면 어떨까? 아마 기대했던 성능이 나오지 못할 가능성이 크다.\n\nIf you torture the data long enough, it will confess"
  },
  {
    "objectID": "models_case_study.html#purrr-many-models",
    "href": "models_case_study.html#purrr-many-models",
    "title": "16  사례: 시군 인구증가",
    "section": "\n16.4 gapminder 회귀모형",
    "text": "16.4 gapminder 회귀모형\npurrr 팩키지를 활용하여 원본 모형 데이터와 모형을 하나의 데이터프레임(tibble)에 담을 수가 있다. 즉, 6가지 서로 다른 회귀모형을 일괄적으로 적합시키고 가장 AIC 값이 적은 회귀모형을 선택하는 코드를 다음과 같이 작성한다. 1\n\n\nreg_models: 다양한 회귀모형을 정의한다.\n\nmutate(map()): 정의한 회귀모형 각각을 적합시키고 모형성능 지표를 추출한다.\nAIC 기준으로 가장 낮은 모형을 선정한다.\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n## 데이터셋 준비 -----\ngapminder &lt;- gapminder %&gt;%\n  set_names(colnames(.) %&gt;% tolower())\n\n## 다양한 회귀모형 -----\nreg_models &lt;- list(\n  `01_pop` = 'lifeexp ~ pop',\n  `02_gdppercap` = 'lifeexp ~ gdppercap',\n  `03_simple` = 'lifeexp ~ pop + gdppercap',\n  `04_medium` = 'lifeexp ~ pop + gdppercap + continent + year',\n  `05_more`   = 'lifeexp ~ pop + gdppercap + country + year',\n  `06_full`   = 'lifeexp ~ pop + gdppercap + year*country')\n\nmodel_tbl &lt;- tibble(reg_formula = reg_models) %&gt;%\n  mutate(model_name = names(reg_formula)) %&gt;% \n  select(model_name, reg_formula) %&gt;% \n  mutate(reg_formula = map(reg_formula, as.formula))\n\nmodel_tbl\n\n# A tibble: 6 × 2\n  model_name   reg_formula \n  &lt;chr&gt;        &lt;named list&gt;\n1 01_pop       &lt;formula&gt;   \n2 02_gdppercap &lt;formula&gt;   \n3 03_simple    &lt;formula&gt;   \n4 04_medium    &lt;formula&gt;   \n5 05_more      &lt;formula&gt;   \n6 06_full      &lt;formula&gt;   \n\n## 회귀모형 적합 및 모형 성능 지표 -----\nmodel_tbl &lt;- model_tbl %&gt;%\n  mutate(fit = map(reg_formula, ~lm(., data = gapminder))) %&gt;% \n  mutate(model_glance = map(fit, broom::glance),\n         rsquare      = map_dbl(model_glance, ~.$r.squared),\n         AIC          = map_dbl(model_glance, ~.$AIC)) %&gt;% \n  arrange(AIC)\n\nmodel_tbl\n\n# A tibble: 6 × 6\n  model_name   reg_formula  fit          model_glance      rsquare    AIC\n  &lt;chr&gt;        &lt;named list&gt; &lt;named list&gt; &lt;named list&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 06_full      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.976    7752.\n2 05_more      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.932    9268.\n3 04_medium    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.717   11420.\n4 03_simple    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.347   12836.\n5 02_gdppercap &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.341   12850.\n6 01_pop       &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.00422 13553.\n\n\n\n16.4.1 교차검증 CV\n\n데이터를 10조각내서 교차검정을 통해 RMSE가 가장 작은 회귀모형이 어떤 것인지 살펴보자. cross_df() 함수로 교차검증 splits 데이터와 모형을 준비한다. 다음으로 analysis() 함수로 교차검증 데이터에 대해서 회귀모형 각각을 적합시키고, assessment() 함수로 적합시킨 모형에 대해 모형성능을 살펴본다. 마지막으로 RMSE 회귀모형 성능지표를 통해 모형선택을 한다.\n\n## 교차검정 -----\nvalid_tbl &lt;- gapminder %&gt;%\n  rsample::vfold_cv(10)\n\ncv_tbl &lt;- list(test_training = list(valid_tbl), \n               model_name = model_tbl$model_name)  \n  \ncv_tbl &lt;- tidyr::expand_grid(test_training = list(valid_tbl), \n                             model_name = model_tbl$model_name)\n\ncv_tbl &lt;- cv_tbl %&gt;%\n  mutate(model_number = row_number()) %&gt;%  # Manually creating the model_number column\n  left_join(model_tbl %&gt;% select(model_name, reg_formula), by = \"model_name\") %&gt;% \n  unnest(cols = c(test_training))\n\ncv_tbl\n\n# A tibble: 60 × 5\n   splits             id     model_name model_number reg_formula \n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;   \n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;   \n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;   \n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;   \n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;   \n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;   \n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;   \n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;   \n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;   \n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;   \n# ℹ 50 more rows\n\n## 교차검정 analysis, assessment -----\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)))) %&gt;%\n  mutate(RMSE = map2_dbl(fit, splits, ~modelr::rmse(.x, rsample::assessment(.y))))\n\ncv_fit_tbl\n\n# A tibble: 60 × 7\n   splits             id     model_name model_number reg_formula  fit       RMSE\n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt; &lt;named &gt; &lt;dbl&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.09\n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;    &lt;lm&gt;      3.27\n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.52\n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.28\n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;    &lt;lm&gt;      3.22\n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.21\n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.17\n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.40\n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.48\n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.77\n# ℹ 50 more rows\n\n## 시각화 -----\ncv_fit_tbl %&gt;%\n  ggplot(aes(RMSE, fill = model_name)) +\n  geom_density(alpha = 0.75) +\n  labs(x = \"RMSE\", title = \"gapminder 회귀모형별 교차검정 분포\")\n\n\n\n\n\n\n\n\n16.4.2 병렬처리 - furrr\n\nparallel::detectCores()을 통해 전체 코어 숫자를 확인하고 이를 병렬처리를 통해 교차검증에 따른 시간을 대폭 절감시킨다. 이를 위해서 future 팩키지를 사용하고 절약되는 시간을 측정하기 위해서 tictoc 팩키지를 동원한다.\n\nlibrary(furrr)\nlibrary(tictoc)\n\nplan(multisession, workers = parallel::detectCores() - 1)\n\n\n\npurrr 순차처리\n\n## purrr 순차처리 -----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y))))\n\ntoc()\n\n1.28 sec elapsed\n\n\n\nfurrr 병렬처리\n\n## furrr 병렬처리 ----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = future_map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)), .progress=TRUE)) \n\ntoc()\n\n16.77 sec elapsed"
  },
  {
    "objectID": "models_many.html#model-reg-viz-data",
    "href": "models_many.html#model-reg-viz-data",
    "title": "16  많은 회귀모형",
    "section": "\n16.1 기대수명 데이터",
    "text": "16.1 기대수명 데이터\ngapminder 데이터를 가지고 회귀모형을 구축하고 모형을 활용하여 종속변수(기대수명, lifeExp)가 늘어나지 못한 국가를 뽑아내고 이를 시각적으로 확인해보자.\n\n# 0. 환경설정 -----\n# 데이터\nlibrary(gapminder)\n\n# Tidyverse\nlibrary(tidyverse)\n\n# 모형\nlibrary(broom)\n\n# 1. 데이터 -----\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\n\n16.1.1 기대수명 회귀분석\n대륙과 국가를 그룹으로 잡아 회귀분석을 각각에 대해서 돌리고 나서, 모형 결과값을 데이터와 모형이 함께 위치하도록 티블(tibble)에 저장시켜 놓은다. 그리고 나서, 주요한 회귀모형 성능지표인 결정계수(\\(R^2\\))를 기준으로 정렬시킨다.\n\n# 2. 모형 -----\ncountry_model &lt;- function(df)\n  lm(lifeExp ~ year, data=df)\n\nby_country &lt;- gapminder %&gt;% \n  group_by(country, continent) %&gt;% \n  nest() %&gt;% \n  mutate(model = map(data, country_model),\n         model_glance = map(model, glance),\n         rsquare = map_dbl(model_glance, ~.$r.squared)) |&gt; \n  ungroup()\n\nby_country %&gt;% \n  arrange(rsquare)\n\n# A tibble: 142 × 6\n   country          continent data              model  model_glance      rsquare\n   &lt;fct&gt;            &lt;fct&gt;     &lt;list&gt;            &lt;list&gt; &lt;list&gt;              &lt;dbl&gt;\n 1 Rwanda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0172\n 2 Botswana         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0340\n 3 Zimbabwe         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0562\n 4 Zambia           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0598\n 5 Swaziland        Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0682\n 6 Lesotho          Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0849\n 7 Cote d'Ivoire    Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.283 \n 8 South Africa     Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.312 \n 9 Uganda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.342 \n10 Congo, Dem. Rep. Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.348 \n# ℹ 132 more rows\n\n\n\n16.1.2 회귀모형 시각화\n데이터셋 by_country를 이용하여 각 나라별로 회귀 모델의 $ R^2 $ (결정 계수) 값을 기반으로 “적합국”과 “비적합국”을 분류하고, 그 결과를 시각화한다. $ R^2 $ 값이 큰 나라 5개와 작은 나라 5개를 추출하여 기대수명 변화를 그래프로 대조하여 국가별 차이를 명확히 한다.\n\n\n최대 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 큰 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_max_countries에 저장한다.\n\n\n\n최소 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 작은 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_min_countries에 저장한다.\n\n\n\n데이터 필터링 및 시각화:\n\n총 10개 국가에 해당하는 데이터만 by_country에서 추출한다.\n추출된 데이터에서 나라명, 대륙명, $ R^2 $ 값, 그리고 원데이터를 추출한다.\n결과를 $ R^2 $ 값의 내림차순으로 정렬한다.\n데이터를 정리하여 각 나라의 연도별 기대수명을 나타내는 선그래프를 생성하고, 그래프에서는 $ R^2 $ 값이 높은 국가들을 “발전된국가”로, 낮은 값을 가진 국가들을 “개발국”으로 분류하여 색상을 달리하여 시각화할 재료로 준비한다.\n\n\n\n최종적으로는 각 국가별로 연도에 따른 기대수명 변화를 보여주는 그래프를 생성한다. $ R^2 $ 값이 높은 국가들은 빨간색으로, 낮은 국가들은 파란색으로 표시한고, 마지막으로 회귀모형이 얼마나 잘 적합되었는지에 따라 각 국가의 기대수명 변화 패턴을 비교한다. 파란색으로 회귀모형이 잘 적합된 경우에도 서로 다른 패턴이 확인된다. 즉, 선진국과 개발도상국 모두 제2차 세계대전 이후 기대수명이 증대했으나 선진국은 높은 기대수명에서 개도국은 낮은 기대수명에서 시작해서 모두 기대수명이 높아진 것이 눈에 띈다. 하지만, 빨간색으로 회귀계수가 낮은 나라는 기대수명이 높아지다가 특정 사건으로 인해 기대수명이 제자리로 돌아온 이후 다시 기대수명이 높아지는 추세를 보여 비선형적 관계를 보여주고 있어 회귀계수가 전반적으로 낮게 나타났다.\n\nrsquare_max_countries &lt;- by_country |&gt; \n  slice_max(order_by = rsquare, n = 5)  |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nrsquare_min_countries &lt;- by_country |&gt; \n  slice_min(order_by = rsquare, n = 5) |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nby_country |&gt; \n  filter(country %in% rsquare_max_countries | country %in% rsquare_min_countries) |&gt; \n  select(country, continent, rsquare, data) |&gt; \n  arrange(desc(rsquare)) |&gt; \n  unnest(data) |&gt; \n  mutate(country = factor(country, levels = c(rsquare_max_countries, rsquare_min_countries))) |&gt; \n  mutate(class = if_else(country %in% rsquare_max_countries, \"발전된국가\", \"개발국\")) |&gt; \n  ggplot(aes(x = year, y = lifeExp, color = class)) +\n    geom_line() +\n    facet_wrap(~country, nrow = 2) +\n    scale_color_manual(values = c(\"red\", \"blue\")) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    labs(title =\"회귀모형 기대수명 적합국과 비적합국\",\n         x = \"\",\n         y = \"기대수명\",\n         caption = \"자료출처: gapminder\")"
  },
  {
    "objectID": "models_many.html#모형식별",
    "href": "models_many.html#모형식별",
    "title": "16  많은 회귀모형",
    "section": "\n16.2 모형식별",
    "text": "16.2 모형식별\npurrr 팩키지를 활용하여 원본 모형 데이터와 모형을 하나의 데이터프레임(tibble)에 담을 수가 있다. 즉, 6가지 서로 다른 회귀모형을 일괄적으로 적합시키고 가장 AIC 값이 적은 회귀모형을 선택하는 코드를 다음과 같이 작성한다. 1\n\n\nreg_models: 다양한 회귀모형을 정의한다.\n\nmutate(map()): 정의한 회귀모형 각각을 적합시키고 모형성능 지표를 추출한다.\nAIC 기준으로 가장 낮은 모형을 선정한다.\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n## 데이터셋 준비 -----\ngapminder &lt;- gapminder %&gt;%\n  set_names(colnames(.) %&gt;% tolower())\n\n## 다양한 회귀모형 -----\nreg_models &lt;- list(\n  `01_pop` = 'lifeexp ~ pop',\n  `02_gdppercap` = 'lifeexp ~ gdppercap',\n  `03_simple` = 'lifeexp ~ pop + gdppercap',\n  `04_medium` = 'lifeexp ~ pop + gdppercap + continent + year',\n  `05_more`   = 'lifeexp ~ pop + gdppercap + country + year',\n  `06_full`   = 'lifeexp ~ pop + gdppercap + year*country')\n\nmodel_tbl &lt;- tibble(reg_formula = reg_models) %&gt;%\n  mutate(model_name = names(reg_formula)) %&gt;% \n  select(model_name, reg_formula) %&gt;% \n  mutate(reg_formula = map(reg_formula, as.formula))\n\nmodel_tbl\n\n# A tibble: 6 × 2\n  model_name   reg_formula \n  &lt;chr&gt;        &lt;named list&gt;\n1 01_pop       &lt;formula&gt;   \n2 02_gdppercap &lt;formula&gt;   \n3 03_simple    &lt;formula&gt;   \n4 04_medium    &lt;formula&gt;   \n5 05_more      &lt;formula&gt;   \n6 06_full      &lt;formula&gt;   \n\n## 회귀모형 적합 및 모형 성능 지표 -----\nmodel_tbl &lt;- model_tbl %&gt;%\n  mutate(fit = map(reg_formula, ~lm(., data = gapminder))) %&gt;% \n  mutate(model_glance = map(fit, broom::glance),\n         rsquare      = map_dbl(model_glance, ~.$r.squared),\n         AIC          = map_dbl(model_glance, ~.$AIC)) %&gt;% \n  arrange(AIC)\n\nmodel_tbl\n\n# A tibble: 6 × 6\n  model_name   reg_formula  fit          model_glance      rsquare    AIC\n  &lt;chr&gt;        &lt;named list&gt; &lt;named list&gt; &lt;named list&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 06_full      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.976    7752.\n2 05_more      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.932    9268.\n3 04_medium    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.717   11420.\n4 03_simple    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.347   12836.\n5 02_gdppercap &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.341   12850.\n6 01_pop       &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.00422 13553.\n\n\n\n16.2.1 교차검증 CV\n\n데이터를 10조각내서 교차검정을 통해 RMSE가 가장 작은 회귀모형이 어떤 것인지 살펴보자. cross_df() 함수로 교차검증 splits 데이터와 모형을 준비한다. 다음으로 analysis() 함수로 교차검증 데이터에 대해서 회귀모형 각각을 적합시키고, assessment() 함수로 적합시킨 모형에 대해 모형성능을 살펴본다. 마지막으로 RMSE 회귀모형 성능지표를 통해 모형선택을 한다.\n\n## 교차검정 -----\nvalid_tbl &lt;- gapminder %&gt;%\n  rsample::vfold_cv(10)\n\ncv_tbl &lt;- list(test_training = list(valid_tbl), \n               model_name = model_tbl$model_name)  \n  \ncv_tbl &lt;- tidyr::expand_grid(test_training = list(valid_tbl), \n                             model_name = model_tbl$model_name)\n\ncv_tbl &lt;- cv_tbl %&gt;%\n  mutate(model_number = row_number()) %&gt;%  # Manually creating the model_number column\n  left_join(model_tbl %&gt;% select(model_name, reg_formula), by = \"model_name\") %&gt;% \n  unnest(cols = c(test_training))\n\ncv_tbl\n\n# A tibble: 60 × 5\n   splits             id     model_name model_number reg_formula \n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;   \n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;   \n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;   \n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;   \n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;   \n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;   \n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;   \n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;   \n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;   \n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;   \n# ℹ 50 more rows\n\n## 교차검정 analysis, assessment -----\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)))) %&gt;%\n  mutate(RMSE = map2_dbl(fit, splits, ~modelr::rmse(.x, rsample::assessment(.y))))\n\ncv_fit_tbl\n\n# A tibble: 60 × 7\n   splits             id     model_name model_number reg_formula  fit       RMSE\n   &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt; &lt;named &gt; &lt;dbl&gt;\n 1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.18\n 2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.13\n 3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;    &lt;lm&gt;      3.07\n 4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.88\n 5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.33\n 6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.57\n 7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.35\n 8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.57\n 9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.26\n10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.45\n# ℹ 50 more rows\n\n## 시각화 -----\ncv_fit_tbl %&gt;%\n  ggplot(aes(RMSE, fill = model_name)) +\n  geom_density(alpha = 0.75) +\n  labs(x = \"RMSE\", title = \"gapminder 회귀모형별 교차검정 분포\")\n\n\n\n\n\n\n\n\n16.2.2 병렬처리 - furrr\n\nparallel::detectCores()을 통해 전체 코어 숫자를 확인하고 이를 병렬처리를 통해 교차검증에 따른 시간을 대폭 절감시킨다. 이를 위해서 future 팩키지를 사용하고 절약되는 시간을 측정하기 위해서 tictoc 팩키지를 동원한다.\n\nlibrary(furrr)\nlibrary(tictoc)\n\nplan(multisession, workers = parallel::detectCores() - 1)\n\n\n\npurrr 순차처리\n\n## purrr 순차처리 -----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y))))\n\ntoc()\n\n1.14 sec elapsed\n\n\n\nfurrr 병렬처리\n\n## furrr 병렬처리 ----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = future_map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)), .progress=TRUE)) \n\ntoc()\n\n18.17 sec elapsed"
  },
  {
    "objectID": "map.html",
    "href": "map.html",
    "title": "\n22  지도 공간정보\n",
    "section": "",
    "text": "22.1 도구의 진화\nGIS, 좌표계, 파일 형식, 계층(Layer), 교차분석 등 주요개념을 바탕으로 도구 진화과정을 살펴보자. 초기 PC가 보급되면서 1980년대 GIS 시스템은 사일로 형태 고가 시스템이었으며, 1990년대에는 1980년대 개발된 GIS 시스템간 연결이 시작되면서 스파게티 현상이 심해지면서 새로운 애플리케이션과 파일 형식이 도입되는데 필요 이상의 낭비가 심해지는 구조가 되었다.\n2000년대 GDAL(Geospatial Data Abstraction Layer)이 등장하여 개발자가 각 파일 형식별로 서로 다른 드라이버를 작성하는 대신 GDAL 클라이언트 드라이버만 개발하면 되기 때문에 일대 혁신이 일어났다. 2010년대는 인공위성을 통한 대량의 데이터가 넘쳐나며 이를 저장하기 위해서 클라우드 서비스가 우후죽순처럼 생겨났고 각기 다른 파편화된 진화과정이 일어났다. 2020년대 넘어서면서 1990년대와 유사한 상황이 재현되었고, 이를 해결하고 새로운 전환점을 만들기 위해 OpenEO가 제시되며 사용자와 데이터 센터 사이에서 새로운 표준으로 자리잡게 되면서 R, 파이썬 등 데이터 과학 프로그래밍 언어를 통한 공간정보 가치 창출이 용이해졌다. (Pebesma 기타 2016)",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#도구-패키지",
    "href": "map.html#도구-패키지",
    "title": "\n22  지리정보시스템과 프롭테크: 도구의 진화와 미래 방향\n",
    "section": "\n22.2 도구 패키지",
    "text": "22.2 도구 패키지\n인간은 오랜 역사 동안 다양한 도구를 개발하고 사용해 왔으며, GIS에서 도구는 단순한 계산 작업부터 복잡한 데이터 분석, 시각화를 통한 의사결정지원, 최근 거대언어모형(LLM) 인공지능까지 다양한 형태로 업무를 지원하고 있다.\n앞서 공간 데이터를 분석하는데 사용되는 도구의 진화과정을 살펴봤다. 현재 직면한 문제를 해결하는데 과거 도구를 사용하는 것은 철기시대에 석기시대 도구를 사용하는 것과 다름이 없다. 따라서, 데이터를 통해 가치를 만드는 데이터 과학자 입장에서 도구의 선택은 매우 중요하다.\n\n\n그림 22.1: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\n\n현재 R 공간정보 생태계는 앞선 공간정보 도구의 진화과정을 몇차례 경험한 후에 그림 22.1 처럼 자리를 잡았다. 결론부터 들어가면 sf 패키지가 생태계의 중심을 잡았으며 사용자와 개발자는 sf 패키지를 통해 상당수 공간정보 데이터 문제를 해결할 수 있다. 하지만, sf는 C/C++ 라이브러리에 크게 의존성하기 때문에 각 라이브러리를 살펴보는 것은 향후 GIS 개발자와 분석가로 현업에서 활약하는데 큰 도움이 될 것으로 보인다.\nGDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2는 C/C++ 라이브러리를 개발, 유지보수, 지원하는 커뮤니티가 존재하는 반면 R, 파이썬, 줄리아(Julia), 자바스크립트는 대화형 인터페이스를 통해 라이브러리를 활용하는 커뮤니티도 존재한다. (Pebesma 와/과 Bivand 2023)\nGDAL (Geospatial Data Abstraction Library) 라이브러리는 공간 데이터 처리에 있어 멀티툴의 대명사인 스위스군 칼(SAK, Swiss Army Knife)이라는 별명을 갖고 있고 100개가 넘는 다른 라이브러리와 연결되어 다양한 공간 데이터를 불러오고, 처리하고, 내보내는 기능을 제공한다.\nPROJ는 지도 투영 및 데이터 변환을 위한 라이브러리로, 하나의 좌표계에서 다른 좌표계로 좌표를 변환할 때 유용하다.\nGEOS (Geometry Engine Open Source)와 s2geometry 라이브러리는 기하학 연산에 사용하며, 길이, 면적, 거리를 측정하거나 연산작업에 사용되며 \\(R^2\\)로 표기되며 GEOS는 평평한 2차원 공간에, \\(S^2\\)로 표기되며 s2geometry는 구형 공간에 사용한다.\nNetCDF는 파일 형식이며 C 라이브러리로, 어떤 차원 배열도 정의할 수 있으며 특히 (기후) 모형개발 커뮤니티에서 널리 사용된다. Udunits2는 측정 단위의 데이터베이스 및 소프트웨어 라이브러리로, 단위의 변환과 파생 단위, 사용자 정의 단위를 처리한다. liblwgeom은 PostGIS 구성 요소로, GDAL, GEOS에서 누락된 몇 가지 루틴을 포함한다. (Pebesma 기타 2016)"
  },
  {
    "objectID": "map.html#사례",
    "href": "map.html#사례",
    "title": "\n22  지도 공간정보\n",
    "section": "\n22.3 사례",
    "text": "22.3 사례\nsf 패키지가 추상화된 함수를 제공하기 때문에 R 공간정보 생태계에서 GDAL 라이브러리를 GIS 분석가가 직접적으로 다룰 일은 없다. 하지만, GIS 도구를 활용하여 2023년 7월 기준 시도별 인구수를 대한민국 지도위에 도식화하기 위해서는 R 공간정보 생태계를 구성하는 다양한 도구가 꼭 필요하다. 먼저, 지도 데이터를 다루기 위해 sf 패키지를 사용하고 통계청(KOSIS) 데이터를 처리하기 위해 tidyverse 패키지 도구를 활용하고 오픈지도 개발자가 공개한 geojson 파일을 결합하여 인구수를 시도 수준에서 시도별 인구수 색상을 달리하여 지도위에 시각화한다. 작성된 코드는 R로 작성되었지만 파이썬 진영에도 공간정보 생태계도 유사한 도구가 준비되어 있어 각자 사용하기 좋은 도구를 가지고 의미있는 결과물을 만들어내고 있다.\n\nlibrary(sf)\nlibrary(tidyverse)\nsf_use_s2(FALSE)\n\n## 지도\nkorea_map &lt;- read_sf(\"data/HangJeongDong_ver20230401.geojson\")\n\nsido_map &lt;- korea_map |&gt; \n  group_by(sidonm) |&gt; \n  summarise(geometry = sf::st_union(geometry))\n\n## 23년 7월 인구수(KOSIS) 행정구역별, 성별 인구수\npop_tbl &lt;- read_csv(\"data/행정구역_시군구_별__성별_인구수_20230831223248.csv\",\n         locale=locale(encoding=\"euc-kr\"), skip = 1) |&gt; \n  set_names(c(\"sidonm\", \"인구수\")) |&gt; \n  mutate(sidonm = if_else(sidonm == \"강원특별자치도\", \"강원도\", sidonm))\n\nsigo_gg &lt;- sido_map |&gt; \n  left_join(pop_tbl) |&gt; \n  ggplot() +\n    geom_sf(aes(geometry = geometry, fill = cut(인구수, 10)), show.legend = FALSE) +\n    ggrepel::geom_label_repel(aes(label = sidonm, geometry = geometry), \n                              size = 3, stat = \"sf_coordinates\") +\n    theme_void() +\n    scale_fill_brewer(palette = \"OrRd\")\n\nragg::agg_jpeg(\"images/GIS_tools.jpeg\",\n               width = 10, height = 7, units = \"in\", res = 600)\nsigo_gg\ndev.off()\n\n\n\nGIS 도구 활용 대한민국 시도별 인구수",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#도구-진화과정",
    "href": "map.html#도구-진화과정",
    "title": "\n22  지리정보시스템과 프롭테크: 도구의 진화와 미래 방향\n",
    "section": "\n22.1 도구 진화과정",
    "text": "22.1 도구 진화과정\n초기 PC가 보급되면서 1980년대 GIS 시스템은 사일로 형태 고가 시스템이었으며, 1990년대에는 1980년대 개발된 GIS 시스템간 연결이 시작되면서 스파게티 현상이 심해지면서 새로운 애플리케이션과 파일 형식이 도입되는데 필요 이상의 낭비가 심해지는 구조가 되었다.\n2000년대 GDAL(Geospatial Data Abstraction Layer)이 등장하여 개발자가 각 파일 형식에 대한 드라이버를 작성하는 대신 GDAL 클라이언트 드라이버만 개발하면 되기 때문에 일대 혁신이 일어났다. 2010년대는 인공위성을 통한 대량의 데이터가 넘쳐나며 이를 저장하기 위해서 클라우드 서비스가 우후죽순처럼 생겨났고 각기 다른 진화과정을 거쳤다. 2 020년대 넘어서면서 1990년대와 유사한 상황이 되었고 이를 해결하고 새로운 전환점을 만들기 위해 OpenEO가 사용자와 데이터 센터 사이에서 새로운 표준으로 자리잡게 되면서 R, 파이썬 등 데이터 과학 프로그래밍 언어를 통해 공간정보를 통한 가치 창출이 용이해졌다.\n\n\nGIS 도구 진화과정"
  },
  {
    "objectID": "map.html#도구의-진화",
    "href": "map.html#도구의-진화",
    "title": "\n22  지도 공간정보\n",
    "section": "",
    "text": "GIS 도구 진화과정",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#gis-생태계",
    "href": "map.html#gis-생태계",
    "title": "\n22  지도 공간정보\n",
    "section": "\n22.2 GIS 생태계",
    "text": "22.2 GIS 생태계\n인간은 오랜 역사 동안 다양한 도구를 개발하고 사용해 왔으며, GIS에서도 도구는 단순한 계산부터 복잡한 데이터 분석, 시각화를 통한 의사결정지원, 최근 거대언어모형(LLM) 인공지능까지 다양한 형태로 기여하고 있다.\n현재 직면한 문제를 해결하는데 과거 도구를 사용하는 것은 마치 철기시대에 석기시대 도구를 사용하는 것과 다름이 없다. 따라서, 데이터를 통해 가치를 만드는 데이터 과학자 입장에서 도구의 선택은 매우 중요할 수 밖에 없다.\n\n\n\n\n\n그림 22.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\n\n\n현재 R 공간정보 생태계는 공간정보 도구 진화과정을 몇차례 경험한 후에 그림 22.2 처럼 자리를 잡았다. 결론부터 들어가면 sf 패키지가 생태계의 중심으로 자리 잡았으며, 사용자와 개발자는 sf 패키지를 통해 상당수 공간정보 데이터 문제를 해결할 수 있게 되었다. 하지만, sf 패키지는 C/C++ 라이브러리에 크게 의존성하기 때문에 각 라이브러리를 살펴보는 것은 향후 GIS 개발자와 분석가로 현업에서 활약하는데 큰 도움이 될 것으로 보인다.\nGDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2는 C/C++ 라이브러리를 개발, 유지보수, 지원하는 커뮤니티가 존재하는 반면 R, 파이썬, 줄리아(Julia), 자바스크립트는 대화형 인터페이스를 통해 라이브러리를 활용하는 커뮤니티도 존재한다. (Pebesma 와/과 Bivand 2023)\n\n\nGDAL (Geospatial Data Abstraction Library) 라이브러리는 공간 데이터 처리에 있어 멀티툴의 대명사인 스위스군 칼(SAK, Swiss Army Knife)이라는 별명을 갖고 있고 100개가 넘는 다른 라이브러리와 연결되어 다양한 공간 데이터를 불러오고, 처리하고, 내보내는 기능을 제공한다.\n\nPROJ는 지도 투영 및 데이터 변환을 위한 라이브러리로, 하나의 좌표계에서 다른 좌표계로 좌표를 변환할 때 유용하다.\n\nGEOS (Geometry Engine Open Source)와 s2geometry 라이브러리는 기하학 연산에 사용하며, 길이, 면적, 거리를 측정하거나 연산작업에 사용되며 \\(R^2\\)로 표기되며 GEOS는 평평한 2차원 공간에, \\(S^2\\)로 표기되며 s2geometry는 구형 공간에 사용한다.\n\nNetCDF는 파일 형식이며 C 라이브러리로, 어떤 차원 배열도 정의할 수 있으며 특히 (기후) 모형개발 커뮤니티에서 널리 사용된다. Udunits2는 측정 단위의 데이터베이스 및 소프트웨어 라이브러리로, 단위의 변환과 파생 단위, 사용자 정의 단위를 처리한다. liblwgeom은 PostGIS 구성 요소로, GDAL, GEOS에서 누락된 몇 가지 루틴을 포함한다. (Pebesma 기타 2016)",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "map.html#마무리",
    "href": "map.html#마무리",
    "title": "\n22  지도 공간정보\n",
    "section": "\n22.4 마무리",
    "text": "22.4 마무리\n공간정보 개발자와 사용자가 개발하는 코드는 기계보다 사람 친화적으로 바뀌었으며, 효과적인 디버그와 신속한 개발을 위한 파이프 철학도 도입되어 생산성 향상이 비약적으로 높아졌고 도구의 추상화 수준도 대폭 향상되었다. IoT와 인공위성을 통해 엄청난 공간정보 데이터가 축적되고 있지만, GIS 도구가 꾸준히 발전하면서 이제 누구나 이러한 도구를 활용하여 공간정보 데이터를 통해 의미있는 산출물을 제작하고 도구도 개발할 수 있게 되었고, 다른 한편으로는 도구 없이 도구에 대한 이해없이 프롭테크를 논하는 것조차 의미없는 시대로 접어들고 있다.\n\n\n\n그림 22.1: GIS 벡터와 래스터 계층, 출처: http://gis.sbcounty.gov/\nGIS 도구 진화과정\n그림 22.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\nGIS 도구 활용 대한민국 시도별 인구수\n\n\n\nPebesma, Edzer, 와/과 Roger Bivand. 2023. Spatial Data Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, 와/과 Markus Neteler. 2016. “OpenEO: a GDAL for Earth Observation Analytics”. 2016년. https://r-spatial.org/2016/11/29/openeo.html.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법”. 프롭빅스(PROPBIX), 호 13 (9월). http://www.kahps.org/.",
    "crumbs": [
      "커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "langchain.html#허깅페이스",
    "href": "langchain.html#허깅페이스",
    "title": "\n24  랭체인\n",
    "section": "",
    "text": "pip install langchain_community, pip install dotenv, pip install huggingface_hub: 이 세 명령어는 파이썬 환경에서 필요한 패키지들을 설치한다. langchain_community는 언어 체인 커뮤니티 라이브러리, dotenv는 환경 변수를 관리하는 라이브러리, huggingface_hub는 Hugging Face Hub와 연동하는 데 사용되는 라이브러리다.\nR 코드 부분에서 library(reticulate)를 사용해 파이썬과 R 사이의 상호작용을 가능하게 하는 reticulate 라이브러리를 로드한다. use_condaenv(\"langchain\", required = TRUE)는 langchain이라는 이름의 Conda 환경을 사용하도록 지시한다. 이는 파이썬 코드를 R 환경에서 실행하기 위한 준비 단계다.\n파이썬 코드에서는 먼저 langchain_community.llms에서 HuggingFaceHub 클래스를, dotenv에서 load_dotenv 함수를 가져온다. 이후 os 모듈을 임포트한다. load_dotenv()를 호출하여 환경 변수를 로드한다. 이는 .env 파일에 저장된 환경 변수를 사용할 수 있게 한다.\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')는 환경 변수에서 ’HF_TOKEN’을 찾아 해당 토큰을 변수에 저장한다. 이 토큰은 Hugging Face Hub에 접근할 때 인증을 위해 사용된다.\nHuggingFaceHub 클래스의 인스턴스를 생성한다. 이 때 repo_id에는 사용할 Hugging Face 모델의 저장소 ID를, huggingfacehub_api_token에는 위에서 얻은 API 토큰을 넣는다.\n대형 언어 모델에 질문을 하기 위해 question 변수에 질문을 저장하고, llm.invoke(question)을 호출하여 모델에 질문을 전달하고 결과를 받는다.\n마지막으로 print(output)을 통해 얻은 결과를 출력한다. 이 코드는 Hugging Face Hub의 특정 모델을 사용하여 질문에 대한 답변을 얻는 과정을 보여준다.\n\n\n\npip install langchain_community\npip install dotenv\npip install huggingface_hub\n\n\nfrom langchain_community.llms import HuggingFaceHub\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')\n\nllm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', \n                     huggingfacehub_api_token = huggingfacehub_api_token)\n\nquestion = 'what is an Large Language Model in artificial intelligence?'\noutput = llm.invoke(question)\n\nprint(output)",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>랭체인</span>"
    ]
  },
  {
    "objectID": "code_interpreter.html#프롬프트",
    "href": "code_interpreter.html#프롬프트",
    "title": "25  데이터 사이언스",
    "section": "25.1 프롬프트",
    "text": "25.1 프롬프트\n\n데이터 전처리EDA통계모형기계학습성능 최적화시각화분석도구대쉬보드파이프라인모듈 개발\n\n\n\nPrompt: What are the best practices for preprocessing {topic} data using {programming_language_or_framework}?\n\n\nData Cleaning: Identify and address missing values, outliers, and duplicate records. Cleaning your data ensures that you’re working with accurate and reliable information.\nData Transformation: Normalize or standardize numerical features, encode categorical variables, and create new features if necessary. Data transformation enhances the quality of your dataset.\nFeature Engineering: Extract meaningful information from your data to improve the performance of machine learning models. Feature engineering involves creating new features or modifying existing ones to make them more informative.\nData Validation: Ensure data consistency and integrity by performing validation checks. Validate that your data adheres to predefined rules and constraints.\n\n\n\n\nPrompt: How can I perform exploratory data analysis on {topic} data using {programming_language_or_framework}?\n\n\nData Visualization: Create informative plots, charts, histograms, and scatterplots to visualize data distributions, relationships, and trends. Visualization helps in gaining initial insights into the data.\nStatistical Analysis: Compute summary statistics, such as mean, median, and standard deviation, to describe the central tendencies and variability of your data. Statistical tests can reveal relationships and dependencies.\nHypothesis Testing: Formulate hypotheses about your data and conduct statistical tests to validate or reject these hypotheses. Hypothesis testing is useful for making data-driven decisions.\nInteractive Exploration: Leverage libraries and tools available in {programming_language_or_framework} to perform interactive exploration. Interactive visualization and widgets allow for dynamic exploration of the data.\n\n\n\n\nPrompt: What are the most common statistical techniques to analyze {topic} data in {programming_language_or_framework}?\n\n\nRegression Analysis: Use regression techniques to model relationships between variables and make predictions. Linear regression, logistic regression, and polynomial regression are common types.\nClustering: Apply clustering algorithms, such as K-means or hierarchical clustering, to group similar data points together. Clustering helps in segmentation and pattern recognition.\nClassification: Perform classification tasks to categorize data into predefined classes or labels. Decision trees, support vector machines, and neural networks are frequently used for classification.\nTime Series Analysis: Analyze data with temporal components using time series analysis. This technique is essential for understanding trends and patterns over time.\n\n\n\n\nPrompt: Provide a step-by-step guide for implementing a machine learning model for {specific_task} using {programming_language_or_framework}.\n\n\nData Preparation: Begin by preprocessing and cleaning your data, ensuring that it’s in the right format for modeling.\nFeature Selection: Identify and select the most relevant features or variables for your model. Feature selection helps improve model performance and reduce complexity.\nModel Selection: Choose an appropriate machine learning algorithm or model for your task. Consider factors like data size, complexity, and interpretability.\nTraining and Evaluation: Train your chosen model on a portion of your data and evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\nDeployment: If the model performs satisfactorily, deploy it in your application or workflow for making predictions.\n\n\n\n\nPrompt: Explain how to optimize {topic} data analysis performance in {programming_language_or_framework} using best coding practices.\n\n\nVectorization: Take advantage of vectorized operations to perform calculations on entire arrays or datasets, which can significantly speed up computations.\nMemory Management: Efficiently manage memory resources, such as by releasing unnecessary objects or using data structures that minimize memory usage.\nParallel Processing: Utilize parallel computing techniques to distribute tasks across multiple cores or processors, thereby accelerating data processing.\nProfiling and Testing: Regularly profile your code to identify performance bottlenecks and optimize critical sections. Thoroughly test your code to ensure correctness and reliability.\n\n\n\n\nPrompt: Discuss the pros and cons of different data visualization techniques for {topic} data analysis in {programming_language_or_framework}.\n\n\nBar Charts and Histograms: These are effective for showing data distributions and comparing categories. They are excellent for visualizing frequency and count data.\nScatterplots: Ideal for displaying relationships between two continuous variables. Scatterplots help identify correlations and trends in data.\nHeatmaps: Useful for visualizing large datasets and identifying patterns in multidimensional data. They are especially valuable for displaying correlation matrices.\nInteractive Dashboards: Create user-friendly interactive dashboards that allow users to explore and interact with data. Dashboards can provide real-time insights and support decision-making.\n\n\n\n\nPrompt: Describe the process of building a custom data analysis tool for {topic} using {programming_language_or_framework}, including the necessary features and functionalities.\n\n\nData Import: Allow users to import data from various sources, such as CSV files, databases, or APIs.\nData Processing: Include preprocessing and transformation capabilities, enabling users to clean, filter, and manipulate data easily.\nVisualization: Incorporate interactive visualization components that help users explore and understand the data visually.\nExport and Reporting: Provide options for exporting analysis results, generating reports, and sharing findings with stakeholders.\n\n\n\n\nPrompt: Explain how to develop a user-friendly dashboard for visualizing and interacting with {topic} data analysis results using {programming_language_or_framework}.\n\n\nIntuitive Design: Create a visually appealing and intuitive design that makes it easy for users to navigate and understand the dashboard.\nInteractive Elements: Incorporate interactive elements, such as filters, sliders, and dropdowns, that allow users to customize their data views.\nReal-Time Updates: Enable real-time updates of data and visualizations to provide users with the latest information.\nAccessibility: Ensure that the dashboard is accessible to all users, including those with disabilities, by following accessibility guidelines.\n\n\n\n\nPrompt: Provide a step-by-step guide for creating a reusable data analysis pipeline for {topic} using {programming_language_or_framework}, covering data preprocessing, analysis, and visualization.\n\n\nData Ingestion: Develop a module for loading data from various sources, including files, databases, and APIs.\nPreprocessing: Create a preprocessing module that encompasses data cleaning, transformation, and feature engineering steps.\nAnalysis: Develop analysis modules that encapsulate statistical analyses, machine learning models, and hypothesis tests.\nVisualization: Implement visualization modules that generate informative charts and graphs for insights.\n\n\n\n\nPrompt: Discuss the key considerations when designing a scalable and modular data analysis tool for {topic} in {programming_language_or_framework}, including performance optimization and extensibility.\n\n\nPerformance Optimization: Optimize your code and algorithms for scalability to ensure that the tool can handle large datasets efficiently.\nModular Architecture: Design the tool with a modular architecture, making it easier to add new features, update existing ones, and maintain the codebase.\nExtensibility: Allow for the easy integration of additional data sources, analysis methods, and visualization techniques to accommodate changing needs.\nUser Collaboration: Implement features that enable collaboration among multiple users, such as data sharing, version control, and user permissions.",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>데이터 사이언스</span>"
    ]
  },
  {
    "objectID": "code_interpreter.html#llm-데이터-분석",
    "href": "code_interpreter.html#llm-데이터-분석",
    "title": "25  데이터 사이언스",
    "section": "25.2 LLM 데이터 분석",
    "text": "25.2 LLM 데이터 분석\n대형 언어 모델(LLMs)과 이미지 생성 모델(IGMs)을 기반으로 파이프라인을 사용하여 데이터 시각화 산출물 생성을 제시한 연구(Dibia 2023) 로 LIDA라는 새로운 도구를 통해 문법에 구애받지 않고 자연어를 통해 시각화 및 인포그래픽을 생성한다. LIDA는 데이터를 자연어 요약으로 변환하는 SUMMARIZER, 데이터를 기반으로 시각화 목표를 나열하는 GOAL EXPLORER, 시각화 코드를 생성하고 정제하며 실행하고 필터링하는 VISGENERATOR, IGM을 사용해 데이터 충실한 스타일화된 그래픽을 생성하는 INFOGRAPHER로 구성된다.\n\n\n\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models”. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 편집자： Danushka Bollegala, Ruihong Huang, 와/과 Alan Ritter, 113–26. Toronto, Canada: Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10 ChatGPT Prompts You Should Start Using Today”. medium.com, 12월. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>데이터 사이언스</span>"
    ]
  },
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n26  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n27 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "langchain.html",
    "href": "langchain.html",
    "title": "\n24  랭체인\n",
    "section": "",
    "text": "24.1 허깅페이스\n파이썬과 R을 사용해 Hugging Face Hub의 대형 언어 모델(Large Language Model, LLM)을 활용한다. 파이썬에서는 필요한 라이브러리를 설치하고, R에서는 reticulate 라이브러리를 통해 파이썬 환경을 사용한다. 파이썬 코드에서 Hugging Face Hub에 접근하기 위한 API 토큰을 로드하고, HuggingFaceHub 클래스를 사용하여 특정 모델(‘tiiuae/falcon-7b-instruct’)에 질문을 하고, 모델의 답변을 출력한다.\nlibrary(reticulate)\n\nuse_condaenv(\"langchain\", required = TRUE)",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>랭체인</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "27  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "챗GPT",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  }
]